2021-01-09 18:59:49,432 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 18:59:49,432 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 18:59:50,884 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 18:59:52,319 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 18:59:53,741 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 18:59:55,158 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 18:59:56,584 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 18:59:58,126 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 18:59:59,655 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:00:01,167 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:00:02,561 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:00:04,790 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:00:07,031 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:00:09,474 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:00:09,476 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:00:09,476 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:00:44,744 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:00:44,745 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:00:46,234 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:00:47,695 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:00:49,168 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:00:50,627 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:00:52,084 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:00:53,673 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:00:55,274 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:00:56,858 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:00:58,316 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:01:00,638 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:01:02,957 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:04:26,882 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:04:26,882 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:04:28,372 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:04:29,794 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:04:31,207 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:04:32,631 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:04:34,045 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:04:35,598 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:04:37,119 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:04:38,625 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:04:39,998 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:04:42,229 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:04:44,475 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:13:16,343 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:13:16,343 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:13:17,763 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:13:19,168 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:13:20,578 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:13:21,995 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:13:23,406 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:13:24,955 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:13:26,477 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:13:28,014 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:13:29,393 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:13:31,612 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:13:33,879 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:13:36,309 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:13:36,309 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:13:36,352 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:17:08,813 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:17:08,813 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:17:10,681 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:17:12,455 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:17:14,061 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:17:15,487 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:17:16,929 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:17:18,564 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:17:20,128 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:17:21,685 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:17:23,073 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:17:25,289 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:17:27,524 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:17:29,990 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:20:44,178 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:20:44,178 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:20:47,247 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:20:50,310 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:20:53,373 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:20:56,427 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:20:59,482 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:21:02,668 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:21:05,891 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:21:09,045 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:21:12,069 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:21:16,543 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:21:21,040 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:21:25,725 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:21:25,725 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:21:25,726 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:21:25,726 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:21:25,727 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:21:25,769 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:25:01,891 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:25:01,891 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:25:04,963 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:25:08,023 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:25:11,084 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:25:14,144 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:25:17,216 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:25:20,418 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:25:23,637 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:25:26,797 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:25:29,826 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:25:34,314 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:25:38,815 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:25:43,508 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:25:43,510 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:25:43,551 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:28:25,668 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:28:25,668 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:28:28,752 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:28:31,816 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:28:34,889 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:28:37,947 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:28:41,009 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:28:44,200 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:28:47,425 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:28:50,595 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:28:53,645 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:28:58,140 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:29:02,636 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:29:07,333 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:29:07,376 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:30:55,068 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:30:55,068 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:30:56,495 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:30:57,899 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:30:59,301 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:31:00,713 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:31:02,115 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:31:03,661 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:31:05,165 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:31:06,707 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:31:08,088 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:31:10,299 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:31:12,519 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:31:14,972 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:32:06,611 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:32:06,611 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:32:09,678 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:32:12,718 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:32:15,750 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:32:18,783 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:32:21,832 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:32:25,009 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:32:28,211 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:32:31,346 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:32:34,352 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:32:38,805 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:32:43,281 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:32:47,939 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:32:47,941 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:32:47,986 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:37:21,610 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:37:21,610 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:37:23,040 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:37:24,455 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:37:25,872 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:37:27,289 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:37:28,708 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:37:30,268 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:37:31,797 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:37:33,364 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:37:34,745 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:37:36,998 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:37:39,241 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:37:41,663 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:39:53,558 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:39:53,558 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:39:55,029 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:39:56,487 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:39:57,935 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:39:59,390 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:40:00,842 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:40:02,421 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:40:03,977 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:40:05,573 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:40:06,992 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:40:09,276 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:40:11,576 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:40:14,107 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:40:14,175 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:40:14,176 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,404 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:40:14,404 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:40:14,460 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,460 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:40:14,552 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:40:14,552 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:40:14,608 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:40:14,609 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:40:14,686 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:40:14,743 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,743 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:40:14,806 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:40:14,862 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,863 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:14,937 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:40:14,937 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:40:14,993 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:14,993 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,057 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:40:15,058 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:40:15,113 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,113 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,174 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:40:15,175 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:40:15,233 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,233 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,294 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:40:15,294 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:40:15,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,348 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,410 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:40:15,410 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:40:15,464 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,464 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:40:15,528 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:40:15,529 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:40:15,584 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:40:15,584 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:40:15,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:40:15,650 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:40:15,705 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:40:15,705 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:40:15,771 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:40:15,771 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:40:15,826 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:40:15,826 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:40:15,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:40:15,896 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:40:15,950 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:40:15,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:40:16,016 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:40:16,017 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:40:16,071 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:40:16,071 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:40:16,132 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:40:16,133 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:40:16,188 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:40:16,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:40:16,252 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:40:16,252 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:40:16,306 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:40:16,306 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:40:16,368 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:40:16,369 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:40:16,425 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:40:16,425 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,488 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:40:16,488 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:40:16,542 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,542 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,605 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:40:16,605 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:40:16,658 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,659 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:40:16,722 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:40:16,723 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:40:16,776 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:40:16,776 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:40:16,844 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:40:16,845 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:40:16,901 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:40:16,901 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:40:16,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:40:16,962 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:40:17,016 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:40:17,017 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:40:17,079 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:40:17,080 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:40:17,136 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:40:17,136 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:40:17,201 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:40:17,202 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:40:17,256 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:40:17,256 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:40:17,320 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:40:17,321 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:40:17,377 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:40:17,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:40:17,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:40:17,440 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:40:17,494 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:40:17,494 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,556 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:40:17,557 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:40:17,613 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,613 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,674 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:40:17,675 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:40:17,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,728 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:40:17,791 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:40:17,791 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:40:17,847 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:40:17,847 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:17,919 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:40:17,920 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:40:17,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:17,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,040 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:40:18,040 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:40:18,097 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,097 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,159 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:40:18,160 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:40:18,214 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,214 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:40:18,278 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:40:18,278 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:40:18,334 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:40:18,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,397 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:40:18,397 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:40:18,450 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,450 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,513 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:40:18,513 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:40:18,567 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,567 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:40:18,633 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:40:18,633 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:40:18,687 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:40:18,687 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:40:18,750 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:40:18,750 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:40:39,417 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:40:39,417 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:40:40,840 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:40:42,253 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:40:43,662 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:40:45,077 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:40:46,495 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:40:48,037 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:40:49,559 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:40:51,122 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:40:52,499 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:40:54,732 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:40:56,971 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:40:59,439 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:40:59,509 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:40:59,510 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,619 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:40:59,619 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:40:59,673 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,673 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:40:59,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:40:59,739 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:40:59,794 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:40:59,795 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,862 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:40:59,862 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:40:59,915 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,915 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,974 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:40:59,975 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:41:00,033 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:41:00,033 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,107 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:41:00,108 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:41:00,169 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,169 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:41:00,232 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:41:00,286 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,286 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,351 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:41:00,352 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:41:00,410 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,411 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,473 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:41:00,473 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:41:00,528 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,528 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,596 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:41:00,596 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:41:00,654 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,654 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:41:00,714 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:41:00,715 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:41:00,769 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:41:00,769 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:41:00,835 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:41:00,836 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:41:00,893 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:41:00,893 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:41:00,956 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:41:00,956 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:41:01,009 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:41:01,009 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:41:01,070 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:41:01,071 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:41:01,127 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:41:01,127 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:41:01,189 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:41:01,189 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:41:01,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:41:01,243 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:41:01,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:41:01,305 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:41:01,359 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:41:01,359 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:41:01,420 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:41:01,421 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:41:01,474 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:41:01,474 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:41:01,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:41:01,538 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:41:01,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:41:01,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,657 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:41:01,657 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:41:01,710 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,711 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,773 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:41:01,774 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:41:01,828 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,828 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:41:01,897 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:41:01,898 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:41:01,950 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:41:01,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:41:02,016 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:41:02,016 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:41:02,074 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:41:02,074 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:41:02,138 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:41:02,138 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:41:02,193 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:41:02,193 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:41:02,256 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:41:02,257 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:41:02,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:41:02,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:41:02,372 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:41:02,372 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:41:02,424 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:41:02,424 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:41:02,486 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:41:02,486 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:41:02,543 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:41:02,544 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:41:02,608 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:41:02,608 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:41:02,664 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:41:02,664 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:41:02,729 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:41:02,786 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,786 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,857 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:41:02,858 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:41:02,912 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,913 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:41:02,982 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:41:02,983 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:41:03,038 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:41:03,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,099 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:41:03,100 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:41:03,154 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,154 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,218 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:41:03,219 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:41:03,276 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,276 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,348 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:41:03,349 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:41:03,402 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,402 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:41:03,473 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:41:03,474 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:41:03,528 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:41:03,528 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,599 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:41:03,600 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:41:03,658 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,658 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,732 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:41:03,733 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:41:03,790 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,790 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:41:03,861 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:41:03,862 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:41:03,915 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:41:03,916 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:41:03,992 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:41:03,993 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:42:16,860 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:42:16,860 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:42:18,288 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:42:19,699 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:42:21,114 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:42:22,534 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:42:24,002 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:42:25,542 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:42:27,060 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:42:28,612 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:42:29,984 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:42:32,218 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:42:34,440 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:42:36,866 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:42:36,900 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:42:36,969 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:42:36,969 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,071 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:42:37,072 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:42:37,126 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,126 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:42:37,192 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:42:37,193 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:42:37,246 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:42:37,246 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,308 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:42:37,308 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:42:37,368 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,368 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,440 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:42:37,441 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:42:37,496 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,497 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,560 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:42:37,561 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:42:37,614 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,614 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,674 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:42:37,674 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:42:37,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,729 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,788 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:42:37,789 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:42:37,848 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,848 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:42:37,915 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:42:37,916 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:42:37,969 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:42:37,969 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:42:38,034 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:42:38,034 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:42:38,091 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:42:38,091 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:42:38,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:42:38,151 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:42:38,203 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:42:38,204 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:42:38,265 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:42:38,265 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:42:38,322 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:42:38,322 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:42:38,384 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:42:38,385 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:42:38,440 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:42:38,440 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:42:38,505 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:42:38,505 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:42:38,561 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:42:38,561 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:42:38,624 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:42:38,625 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:42:38,677 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:42:38,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:42:38,739 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:42:38,739 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:42:38,798 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:42:38,798 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:42:38,866 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:42:38,867 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:42:38,921 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:42:38,921 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:42:38,983 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:42:38,984 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:42:39,037 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:42:39,037 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,096 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:42:39,097 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:42:39,150 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,150 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,212 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:42:39,212 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:42:39,269 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,269 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:42:39,331 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:42:39,331 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:42:39,385 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:42:39,385 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:42:39,449 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:42:39,450 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:42:39,507 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:42:39,507 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:42:39,568 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:42:39,569 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:42:39,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:42:39,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:42:39,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:42:39,683 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:42:39,735 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:42:39,735 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:42:39,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:42:39,798 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:42:39,852 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:42:39,853 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:42:39,922 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:42:39,922 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:42:39,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:42:39,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:42:40,036 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:42:40,036 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:42:40,088 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:42:40,088 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:42:40,150 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:42:40,208 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,208 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,270 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:42:40,271 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:42:40,325 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,325 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:42:40,386 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:42:40,387 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:42:40,439 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:42:40,439 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,501 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:42:40,502 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:42:40,557 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,557 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,621 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:42:40,621 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:42:40,673 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,674 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,740 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:42:40,741 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:42:40,792 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,793 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:42:40,860 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:42:40,860 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:42:40,912 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:42:40,912 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:42:40,972 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:42:40,973 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:42:41,025 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,026 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,085 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:42:41,086 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:42:41,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,142 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:42:41,207 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:42:41,208 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:42:41,261 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:42:41,261 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:42:41,324 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:42:41,324 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:43:09,064 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:43:09,064 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:43:10,722 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:43:12,379 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:43:14,026 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:43:15,680 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:43:17,331 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:43:19,106 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:43:20,867 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:43:22,663 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:43:24,278 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:43:26,822 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:43:29,385 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:43:32,153 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:43:32,153 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:43:32,185 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:43:32,263 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:43:32,263 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,367 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:43:32,367 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:43:32,427 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,427 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:43:32,492 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:43:32,492 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:43:32,551 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:43:32,552 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,621 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:43:32,622 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:43:32,683 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,683 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,744 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:43:32,745 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:43:32,808 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,808 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:32,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:43:32,880 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:43:32,942 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:32,942 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,003 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:43:33,004 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:43:33,066 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,066 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,133 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:43:33,133 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:43:33,197 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,198 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,258 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:43:33,258 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:43:33,318 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,319 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,384 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:43:33,384 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:43:33,447 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,448 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:43:33,507 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:43:33,507 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:43:33,566 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:43:33,566 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:43:33,633 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:43:33,633 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:43:33,697 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:43:33,697 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:43:33,758 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:43:33,759 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:43:33,822 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:43:33,822 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:43:33,893 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:43:33,894 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:43:33,957 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:43:33,957 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:43:34,020 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:43:34,021 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:43:34,079 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:43:34,079 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:43:34,145 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:43:34,145 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:43:34,210 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:43:34,210 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:43:34,270 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:43:34,270 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:43:34,331 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:43:34,332 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:43:34,399 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:43:34,399 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:43:34,462 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:43:34,462 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,523 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:43:34,524 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:43:34,584 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,585 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,651 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:43:34,651 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:43:34,714 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,714 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:43:34,774 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:43:34,775 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:43:34,838 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:43:34,838 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:43:34,909 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:43:34,910 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:43:34,972 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:43:34,973 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:43:35,039 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:43:35,039 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:43:35,100 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:43:35,100 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:43:35,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:43:35,167 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:43:35,232 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:43:35,232 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:43:35,293 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:43:35,293 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:43:35,354 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:43:35,354 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:43:35,421 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:43:35,421 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:43:35,485 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:43:35,485 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:43:35,544 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:43:35,544 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:43:35,605 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:43:35,606 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,670 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:43:35,670 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:43:35,733 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,733 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,795 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:43:35,795 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:43:35,858 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,859 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:43:35,930 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:43:35,931 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:43:35,993 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:43:35,993 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,057 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:43:36,058 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:43:36,118 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,118 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,184 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:43:36,184 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:43:36,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,244 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,303 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:43:36,304 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:43:36,363 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,363 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:43:36,427 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:43:36,427 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:43:36,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:43:36,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,551 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:43:36,552 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:43:36,612 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,617 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:43:36,683 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:43:36,741 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:43:36,800 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:43:36,801 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:43:36,863 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:43:36,863 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:43:36,931 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:43:36,931 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:43:54,373 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:43:54,397 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:43:56,053 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:43:57,697 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:43:59,348 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:44:00,999 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:44:02,661 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:44:04,446 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:44:06,206 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:44:07,946 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:44:09,628 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:44:12,171 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:44:14,727 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:44:17,474 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:44:17,475 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:44:17,489 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:44:17,490 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:44:17,490 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:44:17,492 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:44:17,528 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:44:17,607 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:44:17,607 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,678 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:44:17,679 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:44:17,743 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,743 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:44:17,809 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:44:17,810 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:44:17,868 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:44:17,868 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,938 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:44:17,939 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:44:18,001 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,001 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,062 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:44:18,062 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:44:18,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,121 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:44:18,188 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:44:18,249 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,249 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,311 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:44:18,311 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:44:18,369 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,369 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,433 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:44:18,434 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:44:18,493 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,493 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,553 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:44:18,553 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:44:18,614 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,614 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,681 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:44:18,681 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:44:18,745 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,745 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:44:18,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:44:18,805 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:44:18,867 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:44:18,868 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:44:18,938 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:44:18,939 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:44:19,001 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:44:19,001 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:44:19,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:44:19,063 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:44:19,123 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:44:19,124 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:44:19,188 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:44:19,189 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:44:19,247 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:44:19,247 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:44:19,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:44:19,305 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:44:19,363 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:44:19,364 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:44:19,428 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:44:19,428 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:44:19,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:44:19,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:44:19,551 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:44:19,552 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:44:19,612 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:44:19,612 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:44:19,676 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:44:19,676 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:44:19,740 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:44:19,740 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,801 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:44:19,802 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:44:19,865 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,865 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,936 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:44:19,936 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:44:19,999 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,999 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:44:20,061 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:44:20,061 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:44:20,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:44:20,121 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:44:20,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:44:20,187 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:44:20,251 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:44:20,251 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:44:20,311 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:44:20,312 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:44:20,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:44:20,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:44:20,437 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:44:20,438 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:44:20,498 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:44:20,498 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:44:20,558 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:44:20,559 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:44:20,619 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:44:20,619 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:44:20,684 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:44:20,684 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:44:20,747 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:44:20,747 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:44:20,808 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:44:20,808 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:44:20,871 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:44:20,871 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:44:20,944 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:44:20,944 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:44:21,008 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,008 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:44:21,069 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:44:21,131 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,131 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:44:21,196 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:44:21,197 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:44:21,255 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:44:21,255 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,314 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:44:21,314 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:44:21,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,436 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:44:21,437 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:44:21,500 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,500 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:44:21,563 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:44:21,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:44:21,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:44:21,686 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:44:21,746 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:44:21,746 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:44:21,806 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:44:21,869 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,869 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,939 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:44:21,940 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:44:21,999 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,999 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:44:22,061 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:44:22,062 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:44:22,120 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:44:22,120 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:44:22,185 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:44:22,185 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:44:44,037 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:44:44,037 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:44:45,476 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:44:46,901 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:44:48,320 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:44:49,745 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:44:51,161 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:44:52,725 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:44:54,255 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:44:55,818 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:44:57,212 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:44:59,444 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:45:01,690 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:45:04,160 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:45:04,229 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:45:04,229 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,334 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:45:04,334 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:45:04,390 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,390 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:45:04,462 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:45:04,463 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:45:04,518 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:45:04,519 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,584 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:45:04,585 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:45:04,643 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,643 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:45:04,703 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:45:04,758 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,758 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,822 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:45:04,822 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:45:04,877 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,877 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,946 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:45:04,947 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:45:05,002 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,002 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,067 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:45:05,067 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:45:05,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,122 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,184 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:45:05,184 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:45:05,238 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,238 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,301 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:45:05,301 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:45:05,358 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,358 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:45:05,419 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:45:05,419 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:45:05,474 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:45:05,474 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:45:05,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:45:05,538 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:45:05,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:45:05,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:45:05,654 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:45:05,654 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:45:05,709 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:45:05,709 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:45:05,773 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:45:05,774 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:45:05,827 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:45:05,827 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:45:05,897 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:45:05,898 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:45:05,953 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:45:05,954 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:45:06,019 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:45:06,019 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:45:06,076 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:45:06,076 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:45:06,137 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:45:06,138 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:45:06,193 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:45:06,193 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:45:06,255 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:45:06,256 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:45:06,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:45:06,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,371 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:45:06,372 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:45:06,426 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,426 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,489 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:45:06,489 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:45:06,544 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,544 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:45:06,612 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:45:06,612 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:45:06,666 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:45:06,666 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:45:06,729 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:45:06,730 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:45:06,785 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:45:06,786 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:45:06,846 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:45:06,846 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:45:06,901 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:45:06,902 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:45:06,972 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:45:06,972 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:45:07,030 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:45:07,031 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:45:07,090 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:45:07,091 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:45:07,144 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:45:07,144 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:45:07,207 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:45:07,208 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:45:07,261 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:45:07,261 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:45:07,324 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:45:07,324 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:45:07,377 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:45:07,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:45:07,440 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:45:07,494 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,494 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,557 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:45:07,557 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:45:07,610 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,611 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:45:07,671 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:45:07,672 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:45:07,727 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:45:07,727 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,790 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:45:07,790 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:45:07,844 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,844 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,914 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:45:07,914 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:45:07,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:08,042 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:45:08,042 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:45:08,096 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:08,096 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:45:08,160 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:45:08,161 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:45:08,218 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:45:08,218 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,281 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:45:08,281 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:45:08,335 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,396 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:45:08,397 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:45:08,453 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,453 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:45:08,517 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:45:08,517 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:45:08,570 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:45:08,570 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:45:08,634 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:45:08,635 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:59:03,092 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:59:03,093 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:59:04,535 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:59:06,185 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:59:07,615 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:59:09,045 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:59:10,764 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:59:12,457 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:59:14,240 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:59:15,752 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:59:17,145 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:59:19,382 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:59:21,632 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:59:24,061 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:59:24,062 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:59:24,063 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:59:24,064 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:59:24,065 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:59:24,067 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:59:24,069 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:59:24,070 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:59:24,070 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:59:24,071 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:59:24,104 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:59:24,188 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:59:24,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,287 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:59:24,289 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:59:24,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,348 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:59:24,407 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:59:24,409 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:59:24,467 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:59:24,467 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,525 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:59:24,526 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:59:24,585 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,585 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,646 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:59:24,648 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:59:24,705 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,705 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,764 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:59:24,765 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:59:24,823 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,823 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,883 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:59:24,884 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:59:24,942 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,942 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:25,002 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:59:25,004 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:59:25,063 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:25,064 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,129 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:59:25,131 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:59:25,190 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,190 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,250 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:59:25,251 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:59:25,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:59:25,369 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:59:25,371 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:59:25,428 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:59:25,428 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:59:25,488 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:59:25,490 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:59:25,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:59:25,547 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:59:25,610 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:59:25,612 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:59:25,669 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:59:25,670 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:59:25,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:59:25,730 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:59:25,788 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:59:25,788 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:59:25,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:59:25,848 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:59:25,906 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:59:25,907 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:59:25,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:59:25,966 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:59:26,025 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:59:26,025 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:59:26,092 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:59:26,093 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:59:26,152 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:59:26,152 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:59:26,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:59:26,216 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:59:26,272 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:59:26,273 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,330 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:59:26,331 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:59:26,389 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,389 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,448 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:59:26,449 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:59:26,506 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,507 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:59:26,573 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:59:26,575 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:59:26,637 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:59:26,637 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:59:26,696 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:59:26,697 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:59:26,754 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:59:26,755 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:59:26,822 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:59:26,823 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:59:26,881 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:59:26,881 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:59:26,946 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:59:26,947 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:59:27,006 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:59:27,006 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:59:27,074 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:59:27,076 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:59:27,134 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:59:27,134 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:59:27,198 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:59:27,200 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:59:27,257 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:59:27,258 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:59:27,317 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:59:27,318 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:59:27,375 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:59:27,375 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,433 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:59:27,434 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:59:27,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,552 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:59:27,554 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:59:27,613 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,613 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:59:27,671 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:59:27,673 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:59:27,730 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:59:27,730 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,790 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:59:27,791 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:59:27,849 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,849 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,909 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:59:27,910 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:59:27,967 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,967 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:28,031 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:59:28,033 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:59:28,091 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:28,091 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:59:28,153 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:59:28,154 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:59:28,214 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:59:28,214 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,274 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:59:28,275 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:59:28,333 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,333 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,392 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:59:28,393 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:59:28,451 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,451 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:59:28,510 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:59:28,511 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:59:28,568 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:59:28,568 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:59:28,632 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:59:28,633 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:00:45,607 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:00:45,608 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:00:47,142 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:00:48,588 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:00:50,036 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:00:51,479 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:00:52,941 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 20:00:54,517 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:00:56,068 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:00:57,618 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:00:59,028 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:01:01,288 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:01:03,553 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:01:06,021 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:01:06,022 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:01:06,023 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:01:06,025 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:01:06,026 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:01:06,028 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:01:06,029 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:01:06,032 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:01:06,066 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:01:06,149 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 20:01:06,149 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,254 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:01:06,256 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:01:06,315 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,315 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 20:01:06,374 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:01:06,375 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:01:06,436 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 20:01:06,436 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,495 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:01:06,497 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:01:06,556 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,556 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,617 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:01:06,619 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:01:06,678 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:01:06,739 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:01:06,797 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,797 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,858 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:01:06,860 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:01:06,919 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,920 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,980 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:01:06,982 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:01:07,042 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:07,042 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,111 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:01:07,113 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:01:07,171 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,171 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,239 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:01:07,241 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:01:07,298 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,298 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 20:01:07,357 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:01:07,359 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:01:07,419 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 20:01:07,420 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 20:01:07,480 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:01:07,482 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:01:07,540 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 20:01:07,540 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 20:01:07,602 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:01:07,603 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:01:07,661 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 20:01:07,661 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 20:01:07,720 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:01:07,722 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:01:07,779 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 20:01:07,779 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 20:01:07,838 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:01:07,839 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:01:07,897 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 20:01:07,898 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 20:01:07,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:01:07,963 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:01:08,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 20:01:08,023 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 20:01:08,091 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:01:08,093 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:01:08,151 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 20:01:08,151 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 20:01:08,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:01:08,217 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:01:08,275 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 20:01:08,275 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,341 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:01:08,342 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:01:08,401 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,401 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,459 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:01:08,460 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:01:08,519 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,520 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 20:01:08,580 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:01:08,581 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:01:08,640 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 20:01:08,640 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 20:01:08,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:01:08,705 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:01:08,763 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 20:01:08,763 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 20:01:08,824 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:01:08,825 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:01:08,882 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 20:01:08,882 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 20:01:08,939 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:01:08,941 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:01:08,998 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 20:01:08,998 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 20:01:09,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:01:09,071 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:01:09,129 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 20:01:09,129 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 20:01:09,202 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:01:09,203 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:01:09,260 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 20:01:09,260 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 20:01:09,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:01:09,322 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:01:09,379 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 20:01:09,379 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:01:09,441 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:01:09,498 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,499 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:01:09,563 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:01:09,622 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,622 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 20:01:09,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:01:09,684 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:01:09,741 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 20:01:09,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,799 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:01:09,801 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:01:09,859 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,860 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:01:09,925 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:01:09,982 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,982 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:10,049 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:01:10,051 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:01:10,108 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:10,108 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 20:01:10,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:01:10,169 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:01:10,228 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 20:01:10,228 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,287 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:01:10,288 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:01:10,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,349 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,412 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:01:10,413 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:01:10,471 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,471 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 20:01:10,531 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:01:10,532 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:01:10,588 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 20:01:10,589 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 20:01:10,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:01:10,651 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:51:54,432 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:51:54,432 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:51:55,966 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:51:57,457 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:51:58,997 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:52:00,491 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:52:02,094 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:52:03,684 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:52:05,264 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:52:06,705 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:52:09,010 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:52:11,328 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:52:13,837 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:52:13,837 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:52:13,837 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:52:13,837 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:52:13,837 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:52:13,838 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:52:13,838 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:52:13,838 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:52:13,838 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:52:13,839 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:52:13,871 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:52:13,940 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 20:52:13,940 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,047 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:52:14,047 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:52:14,106 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,106 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 20:52:14,171 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:52:14,172 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:52:14,227 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 20:52:14,227 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,289 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:52:14,290 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:52:14,343 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,343 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,404 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:52:14,404 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:52:14,459 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:52:14,459 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,519 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:52:14,519 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:52:14,575 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,575 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,648 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:52:14,648 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:52:14,702 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,702 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,769 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:52:14,769 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:52:14,824 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:52:14,825 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:52:14,885 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:52:14,886 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:52:14,941 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:52:14,941 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:52:15,002 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:52:15,002 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:52:15,055 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:52:15,055 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 20:52:15,117 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:52:15,118 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:52:15,171 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 20:52:15,171 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 20:52:15,234 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:52:15,234 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:52:15,291 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 20:52:15,291 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 20:52:15,355 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:52:15,355 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:52:15,410 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 20:52:15,410 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 20:52:15,473 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:52:15,474 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:52:15,531 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 20:52:15,531 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 20:52:15,603 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:52:15,604 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:52:15,658 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 20:52:15,658 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 20:52:15,723 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:52:15,724 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:52:15,782 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 20:52:15,782 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 20:52:15,842 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:52:15,843 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:52:15,898 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 20:52:15,898 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 20:52:15,962 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:52:15,963 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:52:16,021 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 20:52:16,021 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:52:16,082 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:52:16,083 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:52:16,136 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:52:16,137 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:52:16,199 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:52:16,199 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:52:16,251 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:52:16,252 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 20:52:16,312 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:52:16,313 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:52:16,365 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 20:52:16,365 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 20:52:16,426 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:52:16,427 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:52:16,483 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 20:52:16,483 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 20:52:16,549 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:52:16,550 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:52:16,605 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 20:52:16,605 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 20:52:16,669 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:52:16,669 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:52:16,722 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 20:52:16,722 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 20:52:16,782 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:52:16,782 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:52:16,834 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 20:52:16,834 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 20:52:16,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:52:16,895 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:52:16,948 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 20:52:16,948 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 20:52:17,012 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:52:17,013 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:52:17,065 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 20:52:17,065 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:52:17,127 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:52:17,127 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:52:17,183 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:52:17,183 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:52:17,246 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:52:17,247 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:52:17,301 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:52:17,301 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 20:52:17,363 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:52:17,363 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:52:17,415 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 20:52:17,415 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,477 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:52:17,477 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:52:17,532 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,532 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,603 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:52:17,603 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:52:17,664 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,664 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:52:17,728 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:52:17,782 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:52:17,782 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 20:52:17,845 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:52:17,846 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:52:17,903 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 20:52:17,903 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:52:17,964 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:52:17,964 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:52:18,019 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:52:18,019 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:52:18,080 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:52:18,081 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:52:18,133 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:52:18,134 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 20:52:18,199 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:52:18,199 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:52:18,250 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 20:52:18,251 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 20:52:18,314 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:52:18,314 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:53:00,704 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:53:00,704 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:53:02,209 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:53:03,657 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:53:05,162 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:53:06,613 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:53:22,840 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:53:22,840 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:53:24,345 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:53:25,796 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:53:27,296 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:53:28,742 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:53:30,315 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:53:31,869 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:53:33,404 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:53:34,818 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:53:37,080 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:53:39,345 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:53:41,791 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:53:41,791 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:53:41,792 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:53:41,792 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:53:41,792 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:53:41,792 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:53:41,793 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:53:41,793 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:53:41,793 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:53:41,793 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:53:41,825 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:53:41,895 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 20:53:41,895 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:53:41,996 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:53:41,997 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:53:42,053 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:53:42,053 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 20:53:42,119 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:53:42,120 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:53:42,175 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 20:53:42,175 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:53:42,241 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:53:42,242 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:53:42,300 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:53:42,300 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:53:42,358 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:53:42,359 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:53:42,414 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:53:42,414 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,476 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:53:42,477 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:53:42,534 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,534 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,602 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:53:42,602 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:53:42,656 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,656 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,719 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:53:42,719 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:53:42,775 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:53:42,775 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:53:42,835 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:53:42,836 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:53:42,890 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:53:42,890 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:53:42,950 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:53:42,950 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:53:43,004 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:53:43,004 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 20:53:43,070 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:53:43,070 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:53:43,123 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 20:53:43,123 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 20:53:43,189 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:53:43,190 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:53:43,248 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 20:53:43,248 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 20:53:43,309 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:53:43,309 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:53:43,362 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 20:53:43,362 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 20:53:43,426 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:53:43,426 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:53:43,479 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 20:53:43,479 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 20:53:43,539 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:53:43,540 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:53:43,593 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 20:53:43,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 20:53:43,661 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:53:43,661 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:53:43,719 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 20:53:43,719 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 20:53:43,780 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:53:43,780 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:53:43,835 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 20:53:43,835 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 20:53:43,898 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:53:43,898 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:53:43,953 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 20:53:43,953 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:53:44,015 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:53:44,015 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:53:44,070 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:53:44,070 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:53:44,130 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:53:44,131 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:53:44,189 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:53:44,189 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 20:53:44,251 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:53:44,251 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:53:44,305 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 20:53:44,305 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 20:53:44,368 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:53:44,369 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:53:44,421 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 20:53:44,421 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 20:53:44,483 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:53:44,483 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:53:44,539 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 20:53:44,539 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 20:53:44,609 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:53:44,609 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:53:44,666 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 20:53:44,666 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 20:53:44,726 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:53:44,727 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:53:44,778 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 20:53:44,778 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 20:53:44,838 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:53:44,838 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:53:44,895 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 20:53:44,895 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 20:53:44,958 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:53:44,958 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:53:45,011 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 20:53:45,011 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:53:45,073 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:53:45,074 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:53:45,126 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:53:45,126 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:53:45,189 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:53:45,189 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:53:45,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:53:45,243 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 20:53:45,303 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:53:45,303 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:53:45,356 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 20:53:45,357 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,420 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:53:45,421 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:53:45,472 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,473 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,534 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:53:45,534 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:53:45,588 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,588 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,661 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:53:45,662 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:53:45,715 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:53:45,715 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 20:53:45,779 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:53:45,780 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:53:45,836 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 20:53:45,836 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:53:45,899 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:53:45,899 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:53:45,952 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:53:45,952 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:53:46,020 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:53:46,021 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:53:46,081 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:53:46,082 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 20:53:46,156 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:53:46,157 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:53:46,209 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 20:53:46,209 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 20:53:46,290 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:53:46,290 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:54:40,035 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:54:40,035 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:54:41,536 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:54:43,032 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:54:44,576 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:54:46,082 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:54:47,698 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:54:49,290 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:54:50,880 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:54:52,337 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:54:54,667 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:54:56,999 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:54:59,586 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:54:59,586 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:54:59,586 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:54:59,587 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:54:59,587 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:54:59,587 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:54:59,587 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:54:59,588 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:54:59,588 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:54:59,588 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:54:59,621 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:54:59,691 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 20:54:59,691 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:54:59,800 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:54:59,800 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:54:59,857 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:54:59,857 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 20:54:59,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:54:59,925 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:54:59,980 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 20:54:59,981 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:55:00,054 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:55:00,054 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:55:00,110 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:55:00,110 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:55:00,177 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:55:00,177 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:55:00,230 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:55:00,230 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,295 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:55:00,296 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:55:00,352 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,352 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,419 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:55:00,420 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:55:00,474 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,475 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,536 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:55:00,537 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:55:00,592 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:00,592 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:55:00,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:55:00,663 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:55:00,721 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:55:00,721 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:55:00,782 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:55:00,782 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:55:00,835 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:55:00,836 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 20:55:00,897 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:55:00,897 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:55:00,955 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 20:55:00,955 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 20:55:01,020 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:55:01,021 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:55:01,075 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 20:55:01,075 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 20:55:01,145 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:55:01,146 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:55:01,200 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 20:55:01,200 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 20:55:01,260 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:55:01,260 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:55:01,314 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 20:55:01,314 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 20:55:01,377 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:55:01,378 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:55:01,430 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 20:55:01,430 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 20:55:01,494 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:55:01,494 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:55:01,551 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 20:55:01,551 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 20:55:01,620 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:55:01,620 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:55:01,673 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 20:55:01,673 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 20:55:01,734 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:55:01,734 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:55:01,787 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 20:55:01,787 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:55:01,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:55:01,847 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:55:01,904 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:55:01,905 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:55:01,969 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:55:01,969 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:55:02,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:55:02,023 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 20:55:02,086 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:55:02,087 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:55:02,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 20:55:02,142 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 20:55:02,205 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:55:02,205 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:55:02,259 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 20:55:02,259 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 20:55:02,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:55:02,321 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:55:02,373 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 20:55:02,374 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 20:55:02,435 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:55:02,435 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:55:02,490 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 20:55:02,490 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 20:55:02,557 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:55:02,557 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:55:02,617 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 20:55:02,617 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 20:55:02,687 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:55:02,687 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:55:02,740 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 20:55:02,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 20:55:02,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:55:02,806 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:55:02,863 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 20:55:02,863 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:55:02,925 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:55:02,925 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:55:02,979 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:55:02,979 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:55:03,048 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:55:03,048 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:55:03,105 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:55:03,105 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 20:55:03,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:55:03,167 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:55:03,219 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 20:55:03,219 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,278 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:55:03,279 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:55:03,332 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,332 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,419 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:55:03,419 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:55:03,475 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,475 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,543 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:55:03,544 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:55:03,599 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:03,600 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 20:55:03,665 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:55:03,665 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:55:03,719 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 20:55:03,720 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:55:03,782 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:55:03,782 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:55:03,834 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:55:03,834 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:55:03,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:55:03,896 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:55:03,949 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:55:03,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 20:55:04,013 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:55:04,013 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:55:04,066 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 20:55:04,066 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 20:55:04,129 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:55:04,129 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:55:25,491 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:55:25,491 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:55:26,987 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:55:28,429 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:55:29,921 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:55:31,361 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:55:32,930 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:55:34,486 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:55:36,017 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:55:37,424 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:55:39,665 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:55:41,912 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:55:44,352 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:55:44,352 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:55:44,352 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:55:44,353 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:55:44,353 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:55:44,353 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:55:44,353 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:55:44,354 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:55:44,354 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:55:44,354 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:55:44,386 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:55:44,454 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 20:55:44,454 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,561 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:55:44,562 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:55:44,619 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,619 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 20:55:44,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:55:44,685 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:55:44,741 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 20:55:44,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:55:44,806 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:55:44,860 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,860 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,922 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:55:44,923 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:55:44,977 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:55:44,977 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,043 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:55:45,043 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:55:45,099 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,100 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,162 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:55:45,162 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:55:45,217 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,217 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,281 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:55:45,282 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:55:45,339 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:55:45,339 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:55:45,401 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:55:45,401 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:55:45,455 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:55:45,455 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:55:45,518 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:55:45,518 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:55:45,573 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:55:45,573 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 20:55:45,642 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:55:45,642 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:55:45,696 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 20:55:45,696 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 20:55:45,760 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:55:45,761 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:55:45,818 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 20:55:45,818 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 20:55:45,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:55:45,880 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:55:45,934 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 20:55:45,934 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 20:55:45,998 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:55:45,999 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:55:46,054 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 20:55:46,054 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 20:55:46,116 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:55:46,116 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:55:46,170 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 20:55:46,171 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 20:55:46,233 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:55:46,234 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:55:46,292 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 20:55:46,292 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 20:55:46,354 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:55:46,354 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:55:46,407 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 20:55:46,407 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 20:55:46,469 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:55:46,470 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:55:46,524 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 20:55:46,524 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:55:46,594 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:55:46,594 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:55:46,648 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:55:46,648 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:55:46,711 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:55:46,711 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:55:46,769 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:55:46,769 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 20:55:46,838 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:55:46,839 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:55:46,892 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 20:55:46,892 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 20:55:46,960 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:55:46,960 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:55:47,016 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 20:55:47,016 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 20:55:47,076 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:55:47,077 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:55:47,131 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 20:55:47,132 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 20:55:47,192 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:55:47,193 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:55:47,251 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 20:55:47,251 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 20:55:47,319 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:55:47,319 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:55:47,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 20:55:47,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 20:55:47,435 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:55:47,436 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:55:47,492 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 20:55:47,492 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 20:55:47,556 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:55:47,556 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:55:47,610 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 20:55:47,610 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:55:47,681 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:55:47,682 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:55:47,735 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:55:47,735 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:55:47,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:55:47,797 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:55:47,850 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:55:47,851 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 20:55:47,911 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:55:47,911 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:55:47,965 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 20:55:47,965 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,032 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:55:48,032 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:55:48,087 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,087 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:55:48,151 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:55:48,205 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,206 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,268 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:55:48,268 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:55:48,321 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:55:48,321 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 20:55:48,382 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:55:48,383 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:55:48,442 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 20:55:48,442 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:55:48,504 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:55:48,505 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:55:48,558 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:55:48,558 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:55:48,630 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:55:48,630 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:55:48,684 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:55:48,684 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 20:55:48,744 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:55:48,745 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:55:48,798 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 20:55:48,798 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 20:55:48,859 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:55:48,860 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:57:05,580 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:57:05,580 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:57:08,669 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:57:11,795 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:57:14,866 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:57:17,933 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:57:21,130 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:57:24,309 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:57:27,476 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:57:30,510 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:57:34,987 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:57:39,471 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:57:44,154 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:57:44,154 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:57:44,155 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:57:44,155 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:57:44,156 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:57:44,156 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:57:44,157 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:57:44,158 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:57:44,158 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:57:44,158 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:57:44,194 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:58:58,003 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 20:58:58,004 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,128 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:58:58,129 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:58:58,235 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,235 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 20:58:58,317 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:58:58,318 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:58:58,417 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 20:58:58,417 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,490 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:58:58,490 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:58:58,591 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,591 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,665 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:58:58,666 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:58:58,772 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 20:58:58,773 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:58:58,845 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:58:58,846 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:58:58,946 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:58:58,946 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:58:59,017 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:58:59,018 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:58:59,117 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:58:59,117 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 20:58:59,196 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:58:59,197 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:58:59,300 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 20:58:59,300 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:58:59,369 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:58:59,369 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:58:59,471 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:58:59,471 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 20:58:59,548 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:58:59,548 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:58:59,654 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 20:58:59,654 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 20:58:59,727 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:58:59,728 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:58:59,831 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 20:58:59,831 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 20:58:59,906 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:58:59,907 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:59:00,009 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 20:59:00,009 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 20:59:00,094 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:59:00,095 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:59:00,195 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 20:59:00,195 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 20:59:00,270 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:59:00,270 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:59:00,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 20:59:00,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 20:59:00,442 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:59:00,443 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:59:00,545 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 20:59:00,545 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 20:59:00,626 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:59:00,628 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:59:00,731 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 20:59:00,731 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 20:59:00,802 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:59:00,803 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:59:00,902 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 20:59:00,902 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 20:59:00,978 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:59:00,978 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:59:01,082 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 20:59:01,082 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:59:01,158 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:59:01,158 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:59:01,262 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:59:01,263 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 20:59:01,337 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:59:01,337 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:59:01,439 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 20:59:01,439 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 20:59:01,510 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:59:01,510 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:59:01,617 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 20:59:01,617 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 20:59:01,698 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:59:01,698 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:59:01,800 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 20:59:01,800 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 20:59:01,872 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:59:01,872 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:59:01,975 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 20:59:01,975 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 20:59:02,049 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:59:02,050 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:59:02,148 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 20:59:02,148 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 20:59:02,220 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:59:02,221 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:59:02,322 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 20:59:02,322 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 20:59:02,396 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:59:02,397 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:59:02,497 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 20:59:02,497 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 20:59:02,573 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:59:02,573 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:59:02,675 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 20:59:02,675 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:59:02,752 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:59:02,752 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:59:02,852 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:59:02,852 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 20:59:02,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:59:02,924 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:59:03,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 20:59:03,024 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 20:59:03,101 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:59:03,101 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:59:03,200 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 20:59:03,200 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,271 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:59:03,272 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:59:03,378 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,378 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,452 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:59:03,453 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:59:03,553 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,553 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,634 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:59:03,635 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:59:03,739 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 20:59:03,739 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 20:59:03,833 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:59:03,833 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:59:03,937 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 20:59:03,937 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:59:04,010 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:59:04,011 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:59:04,111 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:59:04,111 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 20:59:04,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:59:04,188 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:59:04,288 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 20:59:04,288 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 20:59:04,358 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:59:04,358 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:59:04,461 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 20:59:04,461 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 20:59:04,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:59:04,538 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:59:35,415 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:59:35,415 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:59:38,510 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:59:41,633 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:59:44,707 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:59:47,784 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:59:50,995 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:59:54,186 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:59:57,413 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:00:00,475 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:00:05,109 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:00:09,626 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:00:14,355 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:00:14,356 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:00:14,356 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:00:14,356 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:00:14,357 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:00:14,357 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:00:14,359 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:00:14,359 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:00:14,359 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:00:14,359 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:00:14,392 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:00:14,530 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:00:14,531 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:00:14,659 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:00:14,660 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:00:14,762 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:00:14,762 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:00:14,835 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:00:14,836 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:00:14,935 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:00:14,935 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:00:15,011 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:00:15,012 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:00:15,112 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:00:15,113 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:00:15,185 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:00:15,186 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:00:15,286 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:00:15,287 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,363 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:00:15,364 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:00:15,465 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,465 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,536 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:00:15,537 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:00:15,637 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,637 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,713 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:00:15,713 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:00:15,813 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:00:15,813 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:00:15,884 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:00:15,885 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:00:15,985 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:00:15,986 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:00:16,064 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:00:16,065 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:00:16,163 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:00:16,163 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:00:16,233 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:00:16,234 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:00:16,335 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:00:16,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:00:16,413 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:00:16,414 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:00:16,516 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:00:16,516 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:00:16,592 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:00:16,593 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:00:16,694 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:00:16,694 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:00:16,770 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:00:16,770 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:00:16,870 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:00:16,870 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:00:16,940 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:00:16,940 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:00:17,040 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:00:17,040 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:00:17,117 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:00:17,118 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:00:17,221 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:00:17,221 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:00:17,292 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:00:17,292 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:00:17,395 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:00:17,396 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:00:17,470 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:00:17,471 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:00:17,573 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:00:17,573 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:00:17,654 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:00:17,655 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:00:17,758 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:00:17,758 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:00:17,832 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:00:17,833 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:00:17,931 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:00:17,931 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:00:18,002 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:00:18,002 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:00:18,103 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:00:18,103 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:00:18,178 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:00:18,178 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:00:18,277 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:00:18,277 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:00:18,350 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:00:18,351 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:00:18,453 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:00:18,453 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:00:18,533 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:00:18,533 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:00:18,631 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:00:18,631 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:00:18,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:00:18,703 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:00:18,804 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:00:18,804 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:00:18,886 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:00:18,887 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:00:18,988 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:00:18,989 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:00:19,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:00:19,063 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:00:19,165 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:00:19,166 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:00:19,242 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:00:19,243 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:00:19,344 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:00:19,344 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:00:19,414 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:00:19,414 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:00:19,517 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:00:19,517 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:00:19,597 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:00:19,597 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:00:19,697 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:00:19,697 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:00:19,768 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:00:19,769 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:00:19,869 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:00:19,869 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:00:19,946 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:00:19,947 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:00:20,048 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:00:20,048 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:00:20,118 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:00:20,119 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:00:20,219 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:00:20,220 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:00:20,297 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:00:20,298 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:00:20,396 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:00:20,397 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:00:20,467 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:00:20,468 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:00:20,569 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:00:20,569 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:00:20,650 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:00:20,650 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:00:20,749 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:00:20,751 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:00:20,823 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:00:20,824 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:00:20,925 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:00:20,925 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:00:21,001 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:00:21,001 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:11:17,080 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:11:17,081 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:11:18,570 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:11:20,003 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:11:21,495 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:11:22,933 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:11:24,497 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:11:26,048 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:11:27,581 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:11:28,981 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:11:31,226 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:11:33,475 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:11:35,926 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:11:35,926 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:11:35,926 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:11:35,927 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:11:35,927 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:11:35,927 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:11:35,928 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:11:35,928 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:11:35,928 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:11:35,928 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:11:35,959 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:11:36,029 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:11:36,029 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,134 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:11:36,135 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:11:36,192 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,192 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:11:36,259 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:11:36,260 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:11:36,314 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:11:36,314 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,377 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:11:36,377 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:11:36,431 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,431 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,494 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:11:36,495 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:11:36,549 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:11:36,549 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,618 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:11:36,618 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:11:36,672 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,672 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,733 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:11:36,734 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:11:36,787 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,787 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,850 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:11:36,851 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:11:36,909 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:11:36,910 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:11:36,971 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:11:36,971 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:11:37,025 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:11:37,025 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:11:37,092 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:11:37,092 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:11:37,150 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:11:37,150 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:11:37,211 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:11:37,211 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:11:37,264 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:11:37,264 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:11:37,326 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:11:37,327 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:11:37,380 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:11:37,380 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:11:37,443 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:11:37,443 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:11:37,496 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:11:37,496 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:11:37,563 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:11:37,564 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:11:37,617 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:11:37,617 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:11:37,677 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:11:37,677 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:11:37,731 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:11:37,731 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:11:37,792 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:11:37,793 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:11:37,847 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:11:37,847 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:11:37,909 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:11:37,909 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:11:37,963 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:11:37,963 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:11:38,024 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:11:38,025 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:11:38,078 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:11:38,079 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:11:38,141 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:11:38,142 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:11:38,196 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:11:38,196 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:11:38,259 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:11:38,259 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:11:38,315 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:11:38,316 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:11:38,376 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:11:38,377 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:11:38,430 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:11:38,430 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:11:38,491 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:11:38,491 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:11:38,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:11:38,548 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:11:38,618 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:11:38,619 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:11:38,672 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:11:38,673 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:11:38,735 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:11:38,736 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:11:38,790 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:11:38,790 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:11:38,851 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:11:38,852 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:11:38,904 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:11:38,904 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:11:38,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:11:38,966 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:11:39,021 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:11:39,021 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:11:39,084 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:11:39,085 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:11:39,137 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:11:39,137 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:11:39,199 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:11:39,200 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:11:39,255 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:11:39,255 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:11:39,318 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:11:39,318 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:11:39,371 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:11:39,371 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:11:39,435 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:11:39,435 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:11:39,488 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:11:39,488 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,552 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:11:39,553 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:11:39,605 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,606 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,678 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:11:39,678 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:11:39,732 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,732 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,792 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:11:39,793 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:11:39,845 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:11:39,845 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:11:39,906 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:11:39,906 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:11:39,960 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:11:39,961 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:11:40,029 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:11:40,029 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:11:40,082 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:11:40,082 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:11:40,147 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:11:40,147 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:11:40,204 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:11:40,204 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:11:40,264 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:11:40,265 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:11:40,318 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:11:40,319 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:11:40,381 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:11:40,381 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:13:26,610 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:13:26,610 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:13:28,098 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:13:29,541 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:13:31,030 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:13:32,461 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:13:34,018 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:13:35,637 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:13:37,216 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:13:38,658 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:13:41,021 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:13:43,278 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:13:45,724 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:13:45,724 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:13:45,724 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:13:45,724 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:13:45,725 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:13:45,725 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:13:45,725 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:13:45,725 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:13:45,725 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:13:45,726 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:13:45,759 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:13:45,830 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:13:45,831 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:13:45,942 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:13:45,943 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:13:45,998 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:13:45,998 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:13:46,075 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:13:46,075 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:13:46,132 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:13:46,132 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:13:46,203 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:13:46,203 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:13:46,257 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:13:46,258 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:13:46,318 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:13:46,318 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:13:46,373 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:13:46,373 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,435 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:13:46,436 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:13:46,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,492 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,561 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:13:46,562 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:13:46,619 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,619 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:13:46,685 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:13:46,744 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:13:46,744 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:13:46,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:13:46,806 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:13:46,860 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:13:46,860 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:13:46,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:13:46,924 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:13:46,978 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:13:46,978 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:13:47,041 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:13:47,042 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:13:47,096 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:13:47,096 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:13:47,160 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:13:47,160 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:13:47,218 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:13:47,219 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:13:47,277 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:13:47,278 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:13:47,331 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:13:47,332 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:13:47,391 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:13:47,392 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:13:47,446 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:13:47,446 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:13:47,508 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:13:47,508 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:13:47,562 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:13:47,562 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:13:47,632 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:13:47,633 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:13:47,687 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:13:47,687 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:13:47,748 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:13:47,748 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:13:47,801 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:13:47,801 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:13:47,863 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:13:47,864 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:13:47,918 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:13:47,918 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:13:47,979 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:13:47,980 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:13:48,035 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:13:48,036 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:13:48,096 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:13:48,097 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:13:48,150 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:13:48,150 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:13:48,217 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:13:48,218 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:13:48,271 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:13:48,271 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:13:48,334 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:13:48,335 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:13:48,392 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:13:48,393 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:13:48,453 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:13:48,454 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:13:48,508 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:13:48,508 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:13:48,574 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:13:48,575 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:13:48,633 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:13:48,633 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:13:48,694 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:13:48,694 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:13:48,746 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:13:48,747 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:13:48,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:13:48,807 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:13:48,860 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:13:48,861 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:13:48,923 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:13:48,924 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:13:48,976 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:13:48,976 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:13:49,043 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:13:49,044 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:13:49,096 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:13:49,097 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:13:49,157 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:13:49,158 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:13:49,212 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:13:49,213 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:13:49,273 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:13:49,273 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:13:49,330 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:13:49,330 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,400 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:13:49,401 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:13:49,454 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,455 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,525 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:13:49,525 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:13:49,579 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,579 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:13:49,650 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:13:49,703 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:13:49,703 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:13:49,766 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:13:49,767 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:13:49,820 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:13:49,820 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:13:49,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:13:49,880 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:13:49,934 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:13:49,934 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:13:49,995 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:13:49,996 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:13:50,052 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:13:50,052 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:13:50,122 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:13:50,123 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:13:50,178 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:13:50,178 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:13:50,250 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:13:50,250 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:14:56,207 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:14:56,208 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:14:57,694 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:14:59,131 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:15:00,624 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:15:02,061 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:15:03,631 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:15:05,180 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:15:06,726 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:15:08,132 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:15:10,388 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:15:12,761 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:15:15,293 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:15:15,293 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:15:15,293 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:15:15,293 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:15:15,294 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:15:15,294 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:15:15,294 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:15:15,294 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:15:15,295 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:15:15,295 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:15:15,326 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:15:15,396 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:15:15,396 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,493 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:15:15,494 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:15:15,549 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,549 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:15:15,620 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:15:15,621 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:15:15,675 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:15:15,675 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,736 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:15:15,737 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:15:15,791 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,791 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,860 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:15:15,861 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:15:15,916 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:15:15,916 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:15:15,985 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:15:15,985 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:15:16,041 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:15:16,041 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:15:16,109 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:15:16,109 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:15:16,164 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:15:16,165 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:15:16,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:15:16,232 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:15:16,286 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:15:16,286 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:15:16,349 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:15:16,349 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:15:16,403 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:15:16,403 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:15:16,464 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:15:16,465 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:15:16,518 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:15:16,519 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:15:16,587 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:15:16,587 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:15:16,641 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:15:16,641 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:15:16,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:15:16,703 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:15:16,757 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:15:16,757 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:15:16,818 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:15:16,818 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:15:16,872 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:15:16,872 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:15:16,938 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:15:16,938 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:15:16,992 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:15:16,992 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:15:17,060 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:15:17,061 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:15:17,116 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:15:17,117 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:15:17,182 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:15:17,183 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:15:17,239 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:15:17,239 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:15:17,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:15:17,306 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:15:17,360 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:15:17,360 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:15:17,425 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:15:17,425 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:15:17,478 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:15:17,479 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:15:17,543 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:15:17,543 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:15:17,600 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:15:17,600 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:15:17,669 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:15:17,669 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:15:17,723 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:15:17,723 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:15:17,787 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:15:17,788 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:15:17,844 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:15:17,844 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:15:17,905 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:15:17,905 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:15:17,959 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:15:17,959 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:15:18,023 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:15:18,023 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:15:18,080 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:15:18,080 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:15:18,141 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:15:18,141 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:15:18,196 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:15:18,196 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:15:18,260 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:15:18,260 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:15:18,314 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:15:18,314 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:15:18,377 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:15:18,377 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:15:18,430 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:15:18,430 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:15:18,489 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:15:18,490 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:15:18,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:15:18,547 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:15:18,615 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:15:18,616 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:15:18,670 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:15:18,670 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:15:18,737 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:15:18,737 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:15:18,795 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:15:18,795 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:15:18,856 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:15:18,857 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:15:18,909 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:15:18,910 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:15:18,973 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:15:18,974 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:15:19,030 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:15:19,030 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:15:19,092 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:15:19,092 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:15:19,145 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:15:19,145 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:15:19,209 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:15:19,209 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:15:19,263 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:15:19,263 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:15:19,325 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:15:19,325 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:15:19,379 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:15:19,380 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:15:19,442 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:15:19,443 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:15:19,498 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:15:19,498 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:15:19,567 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:15:19,567 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:15:19,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:15:19,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:15:19,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:15:19,685 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:15:19,743 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:15:19,743 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:15:19,807 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:15:19,807 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:15:58,559 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:15:58,559 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:16:01,694 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:16:04,869 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:16:08,002 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:16:11,130 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:16:14,377 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:16:17,615 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:16:20,831 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:16:23,907 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:16:28,454 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:16:33,001 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:16:37,839 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:16:37,840 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:16:37,840 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:16:37,840 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:16:37,841 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:16:37,842 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:16:37,842 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:16:37,843 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:16:37,843 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:16:37,843 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:16:37,876 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:16:38,014 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:16:38,014 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,141 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:16:38,142 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:16:38,245 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,245 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:16:38,315 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:16:38,315 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:16:38,418 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:16:38,418 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,492 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:16:38,493 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:16:38,595 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,595 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,677 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:16:38,677 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:16:38,777 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:16:38,777 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:16:38,860 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:16:38,860 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:16:38,964 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:16:38,964 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:16:39,046 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:16:39,047 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:16:39,151 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:16:39,151 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:16:39,235 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:16:39,236 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:16:39,337 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:16:39,337 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:16:39,406 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:16:39,406 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:16:39,511 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:16:39,511 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:16:39,596 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:16:39,597 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:16:39,699 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:16:39,699 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:16:39,785 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:16:39,786 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:16:39,891 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:16:39,891 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:16:39,971 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:16:39,972 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:16:40,074 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:16:40,074 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:16:40,143 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:16:40,143 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:16:40,246 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:16:40,247 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:16:40,319 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:16:40,320 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:16:40,417 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:16:40,417 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:16:40,486 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:16:40,486 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:16:40,591 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:16:40,591 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:16:40,668 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:16:40,669 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:16:40,769 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:16:40,770 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:16:40,837 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:16:40,838 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:16:40,944 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:16:40,944 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:16:41,019 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:16:41,020 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:16:41,119 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:16:41,119 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:16:41,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:16:41,188 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:16:41,292 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:16:41,292 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:16:41,368 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:16:41,369 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:16:41,470 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:16:41,470 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:16:41,539 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:16:41,540 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:16:41,638 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:16:41,639 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:16:41,716 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:16:41,717 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:16:41,817 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:16:41,818 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:16:41,886 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:16:41,887 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:16:41,990 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:16:41,990 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:16:42,066 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:16:42,066 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:16:42,165 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:16:42,166 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:16:42,234 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:16:42,235 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:16:42,332 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:16:42,333 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:16:42,408 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:16:42,408 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:16:42,508 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:16:42,508 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:16:42,583 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:16:42,583 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:16:42,679 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:16:42,679 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:16:42,754 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:16:42,755 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:16:42,853 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:16:42,853 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:16:42,921 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:16:42,922 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:16:43,021 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:16:43,021 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:16:43,097 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:16:43,098 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:16:43,197 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:16:43,197 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,266 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:16:43,267 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:16:43,368 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,368 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,452 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:16:43,453 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:16:43,554 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,554 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,631 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:16:43,631 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:16:43,734 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:16:43,734 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:16:43,808 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:16:43,808 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:16:43,908 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:16:43,908 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:16:43,977 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:16:43,977 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:16:44,081 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:16:44,081 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:16:44,156 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:16:44,157 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:16:44,257 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:16:44,258 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:16:44,325 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:16:44,326 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:16:44,427 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:16:44,428 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:16:44,513 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:16:44,513 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:18:48,154 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:18:48,155 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:18:51,301 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:18:54,488 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:18:57,609 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:19:00,742 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:19:03,994 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:19:07,227 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:19:10,435 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:19:13,515 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:19:18,060 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:19:22,621 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:19:27,378 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:19:27,378 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:19:27,378 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:19:27,379 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:19:27,379 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:19:27,380 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:19:27,380 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:19:27,381 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:19:27,381 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:19:27,381 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:19:27,414 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:19:27,553 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:19:27,553 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:19:27,687 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:19:27,688 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:19:27,790 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:19:27,790 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:19:27,858 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:19:27,858 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:19:27,961 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:19:27,961 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:19:28,040 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:19:28,040 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:19:28,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:19:28,143 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:19:28,212 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:19:28,212 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:19:28,317 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:19:28,317 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,389 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:19:28,390 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:19:28,489 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,489 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:19:28,563 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:19:28,668 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,668 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,740 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:19:28,741 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:19:28,838 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:19:28,839 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:19:28,907 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:19:28,908 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:19:29,011 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:19:29,012 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:19:29,086 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:19:29,087 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:19:29,189 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:19:29,190 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:19:29,258 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:19:29,259 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:19:29,364 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:19:29,364 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:19:29,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:19:29,439 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:19:29,540 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:19:29,541 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:19:29,616 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:19:29,616 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:19:29,718 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:19:29,718 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:19:29,791 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:19:29,792 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:19:29,893 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:19:29,893 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:19:29,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:19:29,961 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:19:30,066 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:19:30,066 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:19:30,140 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:19:30,140 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:19:30,240 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:19:30,240 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:19:30,314 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:19:30,314 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:19:30,418 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:19:30,418 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:19:30,495 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:19:30,496 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:19:30,597 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:19:30,597 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:19:30,675 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:19:30,676 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:19:30,778 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:19:30,778 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:19:30,848 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:19:30,849 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:19:30,948 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:19:30,948 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:19:31,017 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:19:31,018 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:19:31,118 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:19:31,118 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:19:31,198 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:19:31,199 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:19:31,299 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:19:31,300 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:19:31,369 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:19:31,370 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:19:31,477 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:19:31,478 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:19:31,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:19:31,562 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:19:31,662 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:19:31,662 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:19:31,730 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:19:31,731 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:19:31,831 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:19:31,831 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:19:31,907 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:19:31,908 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:19:32,009 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:19:32,009 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:19:32,077 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:19:32,078 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:19:32,176 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:19:32,177 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:19:32,253 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:19:32,253 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:19:32,353 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:19:32,353 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:19:32,421 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:19:32,421 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:19:32,525 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:19:32,525 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:19:32,604 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:19:32,605 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:19:32,706 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:19:32,706 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:19:32,775 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:19:32,776 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:19:32,875 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:19:32,875 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:19:32,949 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:19:32,950 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:19:33,049 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:19:33,050 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:19:33,124 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:19:33,125 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:19:33,224 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:19:33,225 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:19:33,299 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:19:33,300 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:19:33,398 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:19:33,399 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:19:33,471 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:19:33,471 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:19:33,573 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:19:33,573 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:19:33,657 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:19:33,657 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:19:33,757 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:19:33,757 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:19:33,826 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:19:33,826 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:19:33,929 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:19:33,929 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:19:34,006 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:19:34,007 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:19:56,178 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:19:56,179 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:19:59,252 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:20:02,393 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:20:05,449 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:20:08,522 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:20:11,720 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:20:14,889 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:20:18,054 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:20:21,085 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:20:25,563 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:20:30,044 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:20:34,710 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:20:34,710 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:20:34,710 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:20:34,711 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:20:34,711 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:20:34,712 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:20:34,713 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:20:34,713 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:20:34,713 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:20:34,714 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:20:34,746 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:20:34,881 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:20:34,881 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,007 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:20:35,007 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:20:35,107 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,108 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:20:35,178 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:20:35,178 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:20:35,276 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:20:35,276 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,350 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:20:35,350 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:20:35,448 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,448 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,516 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:20:35,517 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:20:35,615 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:20:35,615 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:20:35,695 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:20:35,695 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:20:35,794 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:20:35,795 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:20:35,863 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:20:35,864 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:20:35,965 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:20:35,965 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:20:36,041 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:20:36,042 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:20:36,140 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:20:36,140 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:20:36,221 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:20:36,221 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:20:36,323 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:20:36,324 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:20:36,397 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:20:36,397 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:20:36,496 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:20:36,496 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:20:36,574 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:20:36,575 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:20:36,676 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:20:36,677 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:20:36,749 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:20:36,749 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:20:36,848 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:20:36,848 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:20:36,917 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:20:36,917 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:20:37,022 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:20:37,022 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:20:37,103 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:20:37,103 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:20:37,205 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:20:37,205 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:20:37,273 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:20:37,274 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:20:37,377 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:20:37,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:20:37,450 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:20:37,451 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:20:37,552 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:20:37,553 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:20:37,628 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:20:37,629 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:20:37,725 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:20:37,725 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:20:37,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:20:37,798 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:20:37,897 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:20:37,897 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:20:37,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:20:37,966 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:20:38,068 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:20:38,068 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:20:38,143 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:20:38,144 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:20:38,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:20:38,244 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:20:38,313 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:20:38,313 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:20:38,412 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:20:38,412 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:20:38,484 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:20:38,485 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:20:38,583 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:20:38,583 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:20:38,665 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:20:38,666 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:20:38,770 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:20:38,770 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:20:38,841 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:20:38,842 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:20:38,938 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:20:38,938 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:20:39,006 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:20:39,006 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:20:39,108 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:20:39,108 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:20:39,183 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:20:39,184 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:20:39,283 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:20:39,283 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:20:39,351 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:20:39,352 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:20:39,455 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:20:39,455 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:20:39,529 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:20:39,530 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:20:39,627 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:20:39,627 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:20:39,698 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:20:39,699 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:20:39,803 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:20:39,803 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:20:39,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:20:39,880 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:20:39,977 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:20:39,977 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,050 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:20:40,051 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:20:40,153 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,154 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,236 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:20:40,237 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:20:40,337 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,338 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,409 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:20:40,410 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:20:40,512 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:20:40,512 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:20:40,594 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:20:40,594 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:20:40,696 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:20:40,696 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:20:40,769 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:20:40,770 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:20:40,871 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:20:40,871 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:20:40,947 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:20:40,948 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:20:41,048 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:20:41,048 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:20:41,116 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:20:41,116 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:20:41,215 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:20:41,215 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:20:41,295 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:20:41,296 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:22:37,012 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:22:37,013 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:22:38,576 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:22:40,075 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:22:41,624 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:22:43,109 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:22:44,721 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:22:46,312 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:22:47,904 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:22:49,357 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:22:51,681 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:22:53,995 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:22:56,516 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:22:56,517 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:22:56,517 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:22:56,517 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:22:56,517 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:22:56,517 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:22:56,518 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:22:56,518 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:22:56,518 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:22:56,519 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:22:56,550 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:22:56,620 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:22:56,620 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:22:56,725 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:22:56,726 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:22:56,785 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:22:56,785 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:22:56,853 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:22:56,853 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:22:56,907 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:22:56,907 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:22:56,975 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:22:56,975 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:22:57,035 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:22:57,035 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:22:57,097 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:22:57,097 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:22:57,152 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:22:57,152 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:22:57,215 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:22:57,270 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,270 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,331 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:22:57,331 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:22:57,384 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,384 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,446 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:22:57,446 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:22:57,503 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:22:57,503 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:22:57,571 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:22:57,571 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:22:57,625 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:22:57,625 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:22:57,691 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:22:57,692 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:22:57,751 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:22:57,751 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:22:57,812 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:22:57,813 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:22:57,867 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:22:57,867 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:22:57,930 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:22:57,931 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:22:57,987 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:22:57,987 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:22:58,050 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:22:58,050 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:22:58,104 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:22:58,104 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:22:58,166 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:22:58,166 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:22:58,226 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:22:58,226 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:22:58,301 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:22:58,301 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:22:58,356 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:22:58,356 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:22:58,424 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:22:58,424 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:22:58,479 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:22:58,480 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:22:58,541 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:22:58,542 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:22:58,595 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:22:58,595 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:22:58,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:22:58,664 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:22:58,719 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:22:58,719 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:22:58,783 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:22:58,784 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:22:58,838 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:22:58,838 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:22:58,902 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:22:58,903 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:22:58,957 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:22:58,957 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:22:59,025 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:22:59,026 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:22:59,079 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:22:59,079 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:22:59,145 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:22:59,146 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:22:59,201 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:22:59,201 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:22:59,271 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:22:59,271 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:22:59,323 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:22:59,323 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:22:59,387 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:22:59,388 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:22:59,444 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:22:59,444 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:22:59,510 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:22:59,511 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:22:59,566 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:22:59,566 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:22:59,640 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:22:59,640 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:22:59,699 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:22:59,699 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:22:59,761 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:22:59,762 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:22:59,817 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:22:59,817 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:22:59,883 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:22:59,883 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:22:59,937 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:22:59,937 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:22:59,998 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:22:59,999 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:23:00,054 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:23:00,055 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:23:00,124 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:23:00,124 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:23:00,179 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:23:00,180 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,243 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:23:00,243 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:23:00,298 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,298 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,359 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:23:00,359 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:23:00,413 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,413 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,474 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:23:00,474 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:23:00,528 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:00,528 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:23:00,597 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:23:00,597 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:23:00,650 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:23:00,650 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:23:00,718 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:23:00,719 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:23:00,772 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:23:00,773 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:23:00,848 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:23:00,848 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:23:00,904 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:23:00,904 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:23:00,966 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:23:00,966 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:23:01,020 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:23:01,021 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:23:01,083 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:23:01,084 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:23:33,094 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:23:33,094 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:23:34,598 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:23:36,039 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:23:37,537 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:23:38,977 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:23:40,547 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:23:42,093 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:23:43,617 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:23:45,024 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:23:47,265 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:23:49,520 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:23:51,973 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:23:51,973 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:23:51,974 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:23:51,974 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:23:51,974 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:23:51,974 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:23:51,975 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:23:51,975 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:23:51,975 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:23:51,975 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:23:52,007 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:23:52,077 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:23:52,077 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,182 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:23:52,182 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:23:52,240 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,241 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:23:52,306 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:23:52,306 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:23:52,361 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:23:52,361 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,422 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:23:52,423 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:23:52,478 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,478 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,541 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:23:52,542 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:23:52,596 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:23:52,596 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,660 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:23:52,660 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:23:52,715 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,715 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,778 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:23:52,779 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:23:52,833 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,833 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:23:52,895 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:23:52,950 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:23:52,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:23:53,011 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:23:53,012 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:23:53,065 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:23:53,065 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:23:53,127 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:23:53,127 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:23:53,184 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:23:53,184 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:23:53,246 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:23:53,247 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:23:53,301 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:23:53,301 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:23:53,389 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:23:53,390 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:23:53,463 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:23:53,463 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:23:53,539 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:23:53,539 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:23:53,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:23:53,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:23:53,726 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:23:53,727 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:23:53,781 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:23:53,781 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:23:53,839 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:23:53,840 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:23:53,893 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:23:53,893 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:23:53,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:23:53,961 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:23:54,016 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:23:54,016 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:23:54,089 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:23:54,090 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:23:54,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:23:54,142 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:23:54,206 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:23:54,207 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:23:54,261 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:23:54,261 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:23:54,324 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:23:54,325 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:23:54,378 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:23:54,378 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:23:54,445 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:23:54,446 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:23:54,501 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:23:54,502 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:23:54,575 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:23:54,576 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:23:54,631 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:23:54,631 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:23:54,698 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:23:54,698 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:23:54,752 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:23:54,753 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:23:54,812 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:23:54,812 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:23:54,869 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:23:54,869 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:23:54,937 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:23:54,937 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:23:54,990 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:23:54,990 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:23:55,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:23:55,063 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:23:55,117 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:23:55,118 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:23:55,184 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:23:55,184 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:23:55,240 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:23:55,240 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:23:55,301 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:23:55,302 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:23:55,355 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:23:55,355 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:23:55,423 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:23:55,424 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:23:55,477 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:23:55,478 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:23:55,549 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:23:55,549 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:23:55,602 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:23:55,603 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:23:55,677 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:23:55,678 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:23:55,731 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:23:55,732 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:55,790 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:23:55,791 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:23:55,845 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:55,845 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:55,917 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:23:55,917 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:23:55,971 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:55,971 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:23:56,044 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:23:56,044 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:23:56,097 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:23:56,097 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:23:56,161 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:23:56,161 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:23:56,216 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:23:56,217 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:23:56,277 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:23:56,278 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:23:56,330 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:23:56,330 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:23:56,402 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:23:56,402 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:23:56,458 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:23:56,458 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:23:56,520 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:23:56,520 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:23:56,574 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:23:56,575 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:23:56,644 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:23:56,644 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:25:36,249 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:25:36,250 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:25:39,361 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:25:42,519 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:25:45,624 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:25:48,722 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:25:51,947 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:25:55,159 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:25:58,356 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:26:01,418 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:26:05,955 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:26:10,489 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:26:15,226 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:26:15,226 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:26:15,227 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:26:15,227 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:26:15,228 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:26:15,228 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:26:15,229 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:26:15,230 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:26:15,230 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:26:15,230 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:26:15,262 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:26:15,398 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:26:15,398 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:26:15,523 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:26:15,524 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:26:15,623 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:26:15,623 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:26:15,693 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:26:15,693 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:26:15,794 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:26:15,795 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:26:15,870 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:26:15,870 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:26:15,971 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:26:15,972 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:26:16,040 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:26:16,041 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:26:16,144 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:26:16,144 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,217 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:26:16,218 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:26:16,317 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,317 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,385 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:26:16,386 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:26:16,488 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,488 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,567 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:26:16,567 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:26:16,668 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:26:16,668 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:26:16,736 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:26:16,737 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:26:16,837 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:26:16,837 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:26:16,912 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:26:16,913 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:26:17,012 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:26:17,012 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:26:17,080 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:26:17,081 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:26:17,183 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:26:17,184 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:26:17,258 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:26:17,259 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:26:17,360 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:26:17,360 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:26:17,428 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:26:17,429 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:26:17,526 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:26:17,526 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:26:17,608 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:26:17,608 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:26:17,709 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:26:17,710 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:26:17,777 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:26:17,777 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:26:17,879 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:26:17,879 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:26:17,954 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:26:17,955 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:26:18,057 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:26:18,058 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:26:18,125 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:26:18,126 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:26:18,229 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:26:18,229 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:26:18,304 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:26:18,305 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:26:18,406 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:26:18,406 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:26:18,475 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:26:18,475 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:26:18,577 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:26:18,578 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:26:18,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:26:18,663 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:26:18,762 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:26:18,762 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:26:18,830 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:26:18,831 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:26:18,931 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:26:18,931 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:26:19,012 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:26:19,012 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:26:19,111 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:26:19,111 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:26:19,179 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:26:19,180 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:26:19,282 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:26:19,283 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:26:19,358 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:26:19,359 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:26:19,458 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:26:19,459 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:26:19,526 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:26:19,527 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:26:19,631 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:26:19,631 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:26:19,711 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:26:19,711 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:26:19,812 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:26:19,812 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:26:19,878 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:26:19,879 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:26:19,975 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:26:19,975 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:26:20,052 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:26:20,053 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:26:20,153 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:26:20,153 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:26:20,225 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:26:20,226 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:26:20,328 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:26:20,328 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:26:20,402 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:26:20,403 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:26:20,503 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:26:20,503 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:26:20,577 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:26:20,577 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:26:20,678 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:26:20,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:26:20,753 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:26:20,754 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:26:20,852 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:26:20,852 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:26:20,921 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:26:20,922 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:26:21,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:26:21,024 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:26:21,097 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:26:21,098 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:26:21,196 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:26:21,196 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:26:21,266 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:26:21,267 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:26:21,368 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:26:21,368 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:26:21,444 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:26:21,445 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:26:21,541 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:26:21,541 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:26:21,618 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:26:21,619 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:26:21,724 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:26:21,724 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:26:21,800 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:26:21,801 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:51:46,115 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:51:46,116 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:51:47,627 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:51:49,063 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:51:50,559 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:51:52,024 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:51:53,606 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:51:55,158 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:51:56,689 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:51:58,107 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:52:00,396 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:52:02,777 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:52:05,217 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:52:05,218 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:52:05,218 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:52:05,218 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:52:05,218 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:52:05,219 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:52:05,219 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:52:05,219 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:52:05,219 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:52:05,220 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:52:05,251 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:52:05,320 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:52:05,320 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,419 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:52:05,419 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:52:05,477 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,477 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:52:05,548 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:52:05,548 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:52:05,605 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:52:05,605 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,675 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:52:05,675 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:52:05,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,728 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,789 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:52:05,790 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:52:05,845 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:52:05,845 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:52:05,906 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:52:05,906 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:52:05,963 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:52:05,963 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:52:06,029 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:52:06,030 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:52:06,085 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:52:06,085 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:52:06,149 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:52:06,149 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:52:06,208 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:52:06,208 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:52:06,268 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:52:06,269 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:52:06,323 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:52:06,324 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:52:06,385 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:52:06,385 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:52:06,437 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:52:06,437 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:52:06,503 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:52:06,504 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:52:06,566 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:52:06,566 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:52:06,637 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:52:06,638 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:52:06,691 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:52:06,691 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:52:06,758 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:52:06,758 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:52:06,825 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:52:06,825 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:52:06,947 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:52:06,947 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:52:07,005 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:52:07,005 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:52:07,066 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:52:07,067 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:52:07,124 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:52:07,124 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:52:07,194 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:52:07,194 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:52:07,256 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:52:07,256 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:52:07,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:52:07,321 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:52:07,376 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:52:07,376 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:52:07,438 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:52:07,439 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:52:07,497 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:52:07,497 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:52:07,584 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:52:07,585 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:52:07,642 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:52:07,642 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:52:07,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:52:07,739 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:52:07,793 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:52:07,793 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:52:07,865 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:52:07,865 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:52:07,920 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:52:07,920 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:52:07,985 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:52:07,986 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:52:08,039 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:52:08,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:52:08,110 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:52:08,111 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:52:08,167 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:52:08,167 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:52:08,241 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:52:08,242 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:52:08,297 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:52:08,297 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:52:08,356 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:52:08,357 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:52:08,411 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:52:08,412 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:52:08,474 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:52:08,474 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:52:08,532 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:52:08,532 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:52:08,600 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:52:08,600 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:52:08,655 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:52:08,656 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:52:08,730 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:52:08,731 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:52:08,783 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:52:08,783 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:52:08,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:52:08,848 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:52:08,904 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:52:08,904 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:52:08,975 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:52:08,976 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:52:09,029 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:52:09,029 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,169 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:52:09,170 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:52:09,223 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,223 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,297 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:52:09,298 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:52:09,351 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,351 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,417 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:52:09,418 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:52:09,470 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:52:09,470 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:52:09,693 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:52:09,693 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:52:09,754 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:52:09,755 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:52:09,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:52:09,848 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:52:09,915 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:52:09,915 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:52:10,006 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:52:10,006 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:52:10,064 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:52:10,064 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:52:10,136 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:52:10,137 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:52:10,191 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:52:10,192 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:52:10,271 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:52:10,272 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:53:23,566 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:53:23,566 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:53:26,682 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:53:29,851 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:53:32,960 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:53:36,059 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:53:39,292 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:53:42,513 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:53:45,708 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:53:48,782 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:53:53,305 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:53:57,836 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:54:02,560 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:54:02,561 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:54:02,561 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:54:02,562 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:54:02,562 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:54:02,563 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:54:02,564 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:54:02,564 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:54:02,564 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:54:02,565 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:54:02,598 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:54:02,734 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:54:02,734 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:54:02,860 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:54:02,861 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:54:02,962 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:54:02,962 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:54:03,040 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:54:03,041 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:54:03,144 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:54:03,144 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:54:03,223 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:54:03,223 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:54:03,323 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:54:03,323 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:54:03,394 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:54:03,395 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:54:03,493 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:54:03,493 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:54:03,573 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:54:03,574 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:54:03,676 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:54:03,676 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:54:03,753 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:54:03,753 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:54:03,854 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:54:03,854 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:54:03,931 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:54:03,931 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:54:04,034 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:54:04,034 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:54:04,107 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:54:04,108 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:54:04,210 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:54:04,211 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:54:04,297 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:54:04,298 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:54:04,398 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:54:04,406 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:54:04,478 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:54:04,479 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:54:04,584 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:54:04,584 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:54:04,667 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:54:04,668 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:54:04,769 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:54:04,769 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:54:04,846 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:54:04,846 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:54:04,948 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:54:04,948 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:54:05,026 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:54:05,027 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:54:05,128 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:54:05,128 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:54:05,201 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:54:05,201 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:54:05,305 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:54:05,305 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:54:05,382 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:54:05,383 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:54:05,485 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:54:05,485 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:54:05,564 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:54:05,565 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:54:05,668 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:54:05,668 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:54:05,746 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:54:05,746 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:54:05,848 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:54:05,848 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:54:05,928 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:54:05,929 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:54:06,028 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:54:06,028 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:54:06,102 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:54:06,102 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:54:06,205 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:54:06,205 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:54:06,275 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:54:06,276 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:54:06,373 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:54:06,374 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:54:06,451 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:54:06,451 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:54:06,551 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:54:06,551 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:54:06,629 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:54:06,629 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:54:06,725 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:54:06,725 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:54:06,801 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:54:06,802 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:54:06,898 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:54:06,898 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:54:06,967 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:54:06,967 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:54:07,070 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:54:07,070 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:54:07,145 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:54:07,145 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:54:07,241 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:54:07,242 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:54:07,315 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:54:07,316 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:54:07,416 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:54:07,416 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:54:07,491 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:54:07,492 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:54:07,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:54:07,595 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:54:07,670 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:54:07,670 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:54:07,773 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:54:07,773 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:54:07,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:54:07,848 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:54:07,945 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:54:07,946 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,017 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:54:08,018 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:54:08,120 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,120 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,196 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:54:08,197 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:54:08,298 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,298 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,369 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:54:08,369 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:54:08,473 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:54:08,473 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:54:08,550 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:54:08,551 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:54:08,652 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:54:08,652 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:54:08,724 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:54:08,724 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:54:08,820 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:54:08,820 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:54:08,896 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:54:08,897 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:54:08,995 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:54:08,995 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:54:09,068 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:54:09,069 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:54:09,171 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:54:09,172 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:54:09,247 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:54:09,248 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:54:39,476 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:54:39,477 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:54:42,557 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:54:45,688 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:54:48,761 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:54:51,832 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:54:55,028 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:54:58,222 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:55:01,391 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:55:04,436 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:55:08,925 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:55:13,425 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:55:18,139 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:55:18,140 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:55:18,140 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:55:18,141 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:55:18,141 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:55:18,142 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:55:18,142 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:55:18,143 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:55:18,143 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:55:18,143 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:55:18,176 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:55:18,312 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:55:18,312 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:55:18,440 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:55:18,441 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:55:18,541 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:55:18,541 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:55:18,621 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:55:18,622 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:55:18,722 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:55:18,723 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:55:18,799 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:55:18,799 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:55:18,898 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:55:18,899 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:55:18,968 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:55:18,969 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:55:19,070 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:55:19,070 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,153 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:55:19,154 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:55:19,255 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,255 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,333 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:55:19,333 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:55:19,432 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,433 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,509 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:55:19,510 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:55:19,612 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:55:19,613 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:55:19,687 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:55:19,688 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:55:19,788 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:55:19,789 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:55:19,867 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:55:19,867 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:55:19,970 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:55:19,970 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:55:20,047 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:55:20,047 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:55:20,153 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:55:20,154 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:55:20,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:55:20,232 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:55:20,334 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:55:20,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:55:20,405 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:55:20,405 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:55:20,506 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:55:20,506 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:55:20,586 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:55:20,587 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:55:20,688 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:55:20,689 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:55:20,761 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:55:20,765 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:55:20,867 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:55:20,867 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:55:20,940 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:55:20,941 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:55:21,039 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:55:21,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:55:21,116 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:55:21,117 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:55:21,224 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:55:21,224 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:55:21,298 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:55:21,299 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:55:21,400 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:55:21,401 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:55:21,475 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:55:21,476 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:55:21,577 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:55:21,577 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:55:21,661 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:55:21,661 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:55:21,767 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:55:21,767 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:55:21,852 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:55:21,853 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:55:21,953 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:55:21,953 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:55:22,035 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:55:22,035 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:55:22,133 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:55:22,133 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:55:22,205 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:55:22,205 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:55:22,305 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:55:22,306 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:55:22,382 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:55:22,382 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:55:22,484 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:55:22,484 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:55:22,566 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:55:22,567 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:55:22,670 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:55:22,671 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:55:22,744 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:55:22,744 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:55:22,843 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:55:22,843 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:55:22,914 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:55:22,915 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:55:23,018 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:55:23,019 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:55:23,098 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:55:23,098 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:55:23,197 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:55:23,198 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:55:23,268 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:55:23,269 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:55:23,371 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:55:23,373 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:55:23,449 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:55:23,450 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:55:23,550 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:55:23,550 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:55:23,628 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:55:23,629 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:55:23,731 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:55:23,731 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:55:23,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:55:23,806 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:55:23,905 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:55:23,906 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:55:23,975 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:55:23,976 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:55:24,074 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:55:24,074 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:55:24,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:55:24,151 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:55:24,251 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:55:24,251 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:55:24,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:55:24,322 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:55:24,425 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:55:24,425 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:55:24,509 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:55:24,509 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:55:24,610 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:55:24,610 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:55:24,688 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:55:24,689 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:55:24,794 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:55:24,794 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:55:24,873 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:55:24,873 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 21:56:23,248 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 21:56:23,248 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 21:56:26,390 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 21:56:29,569 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 21:56:32,703 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 21:56:35,834 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 21:56:39,113 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 21:56:42,364 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 21:56:45,599 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 21:56:48,700 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 21:56:53,269 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 21:56:57,852 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 21:57:02,625 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 21:57:02,625 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 21:57:02,625 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 21:57:02,626 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 21:57:02,626 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 21:57:02,627 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 21:57:02,627 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 21:57:02,628 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 21:57:02,628 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 21:57:02,628 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 21:57:02,661 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 21:57:02,796 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 21:57:02,796 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:57:02,920 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 21:57:02,921 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 21:57:03,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:57:03,023 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 21:57:03,094 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 21:57:03,095 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 21:57:03,199 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 21:57:03,200 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:57:03,279 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 21:57:03,280 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 21:57:03,380 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:57:03,380 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 21:57:03,452 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 21:57:03,452 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 21:57:03,550 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 21:57:03,550 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:57:03,632 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 21:57:03,633 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 21:57:03,734 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:57:03,734 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:57:03,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 21:57:03,806 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 21:57:03,910 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:57:03,910 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 21:57:03,983 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 21:57:03,984 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 21:57:04,082 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 21:57:04,083 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:57:04,153 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 21:57:04,153 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 21:57:04,258 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:57:04,258 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 21:57:04,333 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 21:57:04,333 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 21:57:04,430 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 21:57:04,430 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 21:57:04,501 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 21:57:04,502 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 21:57:04,603 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 21:57:04,603 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 21:57:04,681 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 21:57:04,682 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 21:57:04,784 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 21:57:04,784 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 21:57:04,859 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 21:57:04,860 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 21:57:04,961 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 21:57:04,961 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 21:57:05,038 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 21:57:05,039 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 21:57:05,146 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 21:57:05,146 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 21:57:05,227 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 21:57:05,228 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 21:57:05,334 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 21:57:05,334 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 21:57:05,412 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 21:57:05,413 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 21:57:05,512 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 21:57:05,512 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 21:57:05,590 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 21:57:05,591 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 21:57:05,690 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 21:57:05,690 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 21:57:05,766 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 21:57:05,766 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 21:57:05,867 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 21:57:05,867 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:57:05,942 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 21:57:05,943 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 21:57:06,042 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:57:06,042 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 21:57:06,121 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 21:57:06,122 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 21:57:06,223 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 21:57:06,223 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 21:57:06,296 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 21:57:06,297 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 21:57:06,395 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 21:57:06,396 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 21:57:06,466 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 21:57:06,467 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 21:57:06,568 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 21:57:06,569 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 21:57:06,653 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 21:57:06,653 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 21:57:06,756 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 21:57:06,756 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 21:57:06,833 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 21:57:06,834 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 21:57:06,933 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 21:57:06,934 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 21:57:07,010 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 21:57:07,011 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 21:57:07,111 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 21:57:07,111 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 21:57:07,193 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 21:57:07,194 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 21:57:07,295 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 21:57:07,295 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 21:57:07,370 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 21:57:07,371 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 21:57:07,470 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 21:57:07,470 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:57:07,549 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 21:57:07,550 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 21:57:07,651 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:57:07,651 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 21:57:07,722 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 21:57:07,723 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 21:57:07,825 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 21:57:07,825 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 21:57:07,893 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 21:57:07,894 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 21:57:07,993 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 21:57:07,993 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,075 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 21:57:08,076 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 21:57:08,177 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,177 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,249 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 21:57:08,250 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 21:57:08,350 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,350 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,430 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 21:57:08,430 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 21:57:08,532 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 21:57:08,532 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 21:57:08,610 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 21:57:08,610 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 21:57:08,711 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 21:57:08,711 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:57:08,789 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 21:57:08,789 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 21:57:08,891 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:57:08,892 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 21:57:08,962 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 21:57:08,963 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 21:57:09,063 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 21:57:09,064 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 21:57:09,135 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 21:57:09,136 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 21:57:09,239 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 21:57:09,239 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 21:57:09,310 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 21:57:09,311 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 22:36:27,616 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 22:36:27,616 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 22:36:29,132 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 22:36:30,566 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 22:36:32,056 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 22:36:33,495 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 22:36:35,083 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 22:36:36,629 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 22:36:38,169 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 22:36:39,577 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 22:36:41,829 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 22:36:44,098 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 22:36:46,544 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 22:36:46,544 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 22:36:46,545 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 22:36:46,545 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 22:36:46,545 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 22:36:46,545 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 22:36:46,546 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 22:36:46,546 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 22:36:46,546 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 22:36:46,546 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 22:36:46,578 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 22:36:46,646 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 22:36:46,646 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:36:46,754 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 22:36:46,755 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 22:36:46,814 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:36:46,815 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 22:36:46,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 22:36:46,879 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 22:36:46,934 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 22:36:46,934 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:36:46,997 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 22:36:46,997 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 22:36:47,056 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:36:47,056 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:36:47,117 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 22:36:47,117 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 22:36:47,173 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:36:47,173 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,235 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 22:36:47,236 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 22:36:47,288 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,288 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,350 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 22:36:47,350 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 22:36:47,405 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,405 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,466 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 22:36:47,466 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 22:36:47,524 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:36:47,524 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:36:47,593 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 22:36:47,594 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 22:36:47,646 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:36:47,646 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:36:47,708 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 22:36:47,709 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 22:36:47,766 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:36:47,766 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 22:36:47,827 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 22:36:47,827 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 22:36:47,882 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 22:36:47,882 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 22:36:47,944 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 22:36:47,944 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 22:36:48,002 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 22:36:48,002 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 22:36:48,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 22:36:48,064 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 22:36:48,116 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 22:36:48,116 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 22:36:48,178 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 22:36:48,178 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 22:36:48,237 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 22:36:48,238 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 22:36:48,297 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 22:36:48,297 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 22:36:48,350 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 22:36:48,350 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 22:36:48,422 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 22:36:48,422 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 22:36:48,483 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 22:36:48,484 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 22:36:48,556 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 22:36:48,557 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 22:36:48,610 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 22:36:48,610 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 22:36:48,677 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 22:36:48,678 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 22:36:48,736 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 22:36:48,736 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:36:48,794 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 22:36:48,795 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 22:36:48,850 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:36:48,850 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:36:48,915 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 22:36:48,915 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 22:36:48,971 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:36:48,971 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 22:36:49,035 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 22:36:49,035 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 22:36:49,088 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 22:36:49,088 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 22:36:49,148 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 22:36:49,148 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 22:36:49,202 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 22:36:49,202 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 22:36:49,264 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 22:36:49,264 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 22:36:49,319 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 22:36:49,319 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 22:36:49,380 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 22:36:49,380 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 22:36:49,437 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 22:36:49,437 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 22:36:49,498 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 22:36:49,498 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 22:36:49,552 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 22:36:49,552 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 22:36:49,620 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 22:36:49,621 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 22:36:49,672 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 22:36:49,673 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 22:36:49,732 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 22:36:49,732 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 22:36:49,783 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 22:36:49,784 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:36:49,843 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 22:36:49,844 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 22:36:49,900 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:36:49,900 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:36:49,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 22:36:49,965 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 22:36:50,019 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:36:50,020 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 22:36:50,083 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 22:36:50,084 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 22:36:50,143 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 22:36:50,143 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,204 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 22:36:50,205 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 22:36:50,259 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,259 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,322 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 22:36:50,322 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 22:36:50,379 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,379 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,441 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 22:36:50,441 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 22:36:50,494 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:36:50,494 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 22:36:50,561 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 22:36:50,562 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 22:36:50,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 22:36:50,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:36:50,681 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 22:36:50,682 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 22:36:50,735 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:36:50,735 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:36:50,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 22:36:50,798 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 22:36:50,849 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:36:50,850 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 22:36:50,911 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 22:36:50,912 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 22:36:50,964 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 22:36:50,965 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 22:36:51,028 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 22:36:51,028 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 22:55:31,731 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 22:55:31,731 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 22:55:33,222 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 22:55:34,667 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 22:55:36,149 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 22:55:37,584 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 22:55:39,144 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 22:55:40,695 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 22:55:42,228 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 22:55:43,639 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 22:55:45,890 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 22:55:48,140 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 22:55:50,667 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 22:55:50,667 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 22:55:50,668 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 22:55:50,668 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 22:55:50,668 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 22:55:50,668 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 22:55:50,669 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 22:55:50,669 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 22:55:50,669 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 22:55:50,669 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 22:55:50,701 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 22:55:50,770 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 22:55:50,770 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:55:50,873 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 22:55:50,874 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 22:55:50,927 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:55:50,928 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 22:55:50,994 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 22:55:50,994 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 22:55:51,049 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 22:55:51,049 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:55:51,114 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 22:55:51,114 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 22:55:51,172 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:55:51,172 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:55:51,233 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 22:55:51,233 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 22:55:51,286 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:55:51,286 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,358 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 22:55:51,358 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 22:55:51,415 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,415 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,476 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 22:55:51,477 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 22:55:51,530 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,530 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,591 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 22:55:51,591 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 22:55:51,644 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:55:51,645 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:55:51,719 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 22:55:51,720 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 22:55:51,772 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:55:51,773 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:55:51,841 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 22:55:51,842 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 22:55:51,898 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:55:51,898 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 22:55:51,957 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 22:55:51,957 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 22:55:52,012 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 22:55:52,012 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 22:55:52,073 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 22:55:52,074 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 22:55:52,126 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 22:55:52,126 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 22:55:52,189 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 22:55:52,189 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 22:55:52,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 22:55:52,244 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 22:55:52,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 22:55:52,306 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 22:55:52,358 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 22:55:52,359 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 22:55:52,420 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 22:55:52,421 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 22:55:52,475 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 22:55:52,475 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 22:55:52,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 22:55:52,537 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 22:55:52,592 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 22:55:52,592 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 22:55:52,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 22:55:52,664 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 22:55:52,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 22:55:52,728 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 22:55:52,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 22:55:52,806 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 22:55:52,858 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 22:55:52,858 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:55:52,917 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 22:55:52,918 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 22:55:52,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:55:52,973 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:55:53,039 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 22:55:53,039 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 22:55:53,092 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:55:53,092 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 22:55:53,156 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 22:55:53,156 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 22:55:53,210 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 22:55:53,210 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 22:55:53,272 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 22:55:53,272 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 22:55:53,329 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 22:55:53,330 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 22:55:53,397 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 22:55:53,397 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 22:55:53,452 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 22:55:53,452 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 22:55:53,516 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 22:55:53,516 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 22:55:53,571 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 22:55:53,571 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 22:55:53,634 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 22:55:53,635 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 22:55:53,687 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 22:55:53,688 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 22:55:53,752 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 22:55:53,752 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 22:55:53,809 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 22:55:53,810 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 22:55:53,880 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 22:55:53,881 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 22:55:53,934 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 22:55:53,934 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:55:53,997 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 22:55:53,997 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 22:55:54,053 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:55:54,053 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:55:54,114 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 22:55:54,114 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 22:55:54,166 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:55:54,167 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 22:55:54,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 22:55:54,231 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 22:55:54,298 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 22:55:54,298 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,362 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 22:55:54,363 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 22:55:54,416 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,416 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,480 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 22:55:54,480 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 22:55:54,537 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,538 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,596 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 22:55:54,597 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 22:55:54,650 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:55:54,650 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 22:55:54,711 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 22:55:54,712 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 22:55:54,765 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 22:55:54,765 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:55:54,837 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 22:55:54,837 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 22:55:54,891 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:55:54,891 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:55:54,954 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 22:55:54,954 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 22:55:55,011 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:55:55,011 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 22:55:55,071 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 22:55:55,072 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 22:55:55,123 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 22:55:55,123 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 22:55:55,185 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 22:55:55,185 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 22:57:38,781 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 22:57:38,781 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 22:57:41,911 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 22:57:45,088 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 22:57:48,214 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 22:57:51,344 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 22:57:54,592 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 22:57:57,823 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 22:58:01,050 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 22:58:04,145 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 22:58:08,699 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 22:58:13,416 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 22:58:18,201 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 22:58:18,201 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 22:58:18,202 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 22:58:18,202 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 22:58:18,203 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 22:58:18,204 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 22:58:18,204 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 22:58:18,205 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 22:58:18,205 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 22:58:18,205 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 22:58:18,238 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 22:58:18,373 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 22:58:18,374 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:58:18,500 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 22:58:18,501 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 22:58:18,604 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:58:18,604 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 22:58:18,680 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 22:58:18,681 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 22:58:18,784 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 22:58:18,784 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:58:18,861 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 22:58:18,862 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 22:58:18,963 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:58:18,963 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 22:58:19,036 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 22:58:19,037 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 22:58:19,141 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 22:58:19,141 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,234 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 22:58:19,235 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 22:58:19,335 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,403 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 22:58:19,404 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 22:58:19,505 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,505 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,580 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 22:58:19,580 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 22:58:19,679 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 22:58:19,679 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:58:19,748 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 22:58:19,748 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 22:58:19,858 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:58:19,859 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 22:58:19,931 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 22:58:19,932 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 22:58:20,031 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 22:58:20,031 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 22:58:20,098 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 22:58:20,098 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 22:58:20,197 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 22:58:20,197 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 22:58:20,272 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 22:58:20,273 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 22:58:20,374 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 22:58:20,375 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 22:58:20,443 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 22:58:20,443 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 22:58:20,545 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 22:58:20,545 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 22:58:20,622 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 22:58:20,622 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 22:58:20,723 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 22:58:20,723 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 22:58:20,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 22:58:20,797 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 22:58:20,895 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 22:58:20,896 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 22:58:20,971 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 22:58:20,971 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 22:58:21,072 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 22:58:21,073 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 22:58:21,141 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 22:58:21,141 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 22:58:21,245 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 22:58:21,245 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 22:58:21,319 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 22:58:21,319 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 22:58:21,417 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 22:58:21,418 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:58:21,497 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 22:58:21,498 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 22:58:21,603 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:58:21,603 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 22:58:21,676 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 22:58:21,677 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 22:58:21,775 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 22:58:21,775 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 22:58:21,853 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 22:58:21,853 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 22:58:21,954 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 22:58:21,954 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 22:58:22,032 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 22:58:22,033 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 22:58:22,132 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 22:58:22,132 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 22:58:22,201 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 22:58:22,201 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 22:58:22,303 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 22:58:22,303 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 22:58:22,378 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 22:58:22,379 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 22:58:22,475 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 22:58:22,475 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 22:58:22,544 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 22:58:22,545 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 22:58:22,649 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 22:58:22,649 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 22:58:22,724 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 22:58:22,725 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 22:58:22,821 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 22:58:22,822 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 22:58:22,890 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 22:58:22,891 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 22:58:22,991 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 22:58:22,992 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:58:23,073 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 22:58:23,074 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 22:58:23,174 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:58:23,174 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 22:58:23,245 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 22:58:23,246 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 22:58:23,349 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 22:58:23,349 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 22:58:23,425 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 22:58:23,425 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 22:58:23,523 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 22:58:23,524 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:58:23,592 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 22:58:23,592 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 22:58:23,690 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:58:23,691 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:58:23,767 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 22:58:23,768 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 22:58:23,866 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:58:23,867 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 22:58:23,935 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 22:58:23,935 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 22:58:24,038 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 22:58:24,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 22:58:24,113 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 22:58:24,113 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 22:58:24,212 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 22:58:24,212 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:58:24,283 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 22:58:24,283 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 22:58:24,386 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:58:24,386 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 22:58:24,464 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 22:58:24,464 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 22:58:24,564 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 22:58:24,565 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 22:58:24,634 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 22:58:24,635 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 22:58:24,733 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 22:58:24,733 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 22:58:24,815 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 22:58:24,815 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:01:07,658 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:01:07,659 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:01:10,758 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:01:13,911 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:01:17,016 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:01:20,126 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:01:23,359 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:01:26,566 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:01:29,762 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:01:32,828 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:01:37,359 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:01:41,893 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:01:46,645 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:01:46,645 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:01:46,645 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:01:46,646 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:01:46,647 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:01:46,647 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:01:46,648 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:01:46,649 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:01:46,649 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:01:46,649 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:01:46,683 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:01:46,818 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:01:46,819 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:01:46,952 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:01:46,953 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:01:47,054 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:01:47,055 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:01:47,126 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:01:47,127 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:01:47,230 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:01:47,231 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:01:47,306 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:01:47,306 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:01:47,407 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:01:47,407 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:01:47,476 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:01:47,476 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:01:47,581 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:01:47,581 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:01:47,662 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:01:47,663 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:01:47,760 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:01:47,761 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:01:47,833 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:01:47,833 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:01:47,935 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:01:47,935 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:01:48,010 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:01:48,010 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:01:48,108 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:01:48,108 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:01:48,177 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:01:48,178 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:01:48,278 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:01:48,278 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:01:48,354 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:01:48,355 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:01:48,454 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:01:48,455 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:01:48,529 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:01:48,530 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:01:48,633 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:01:48,633 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:01:48,709 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:01:48,710 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:01:48,807 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:01:48,808 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:01:48,885 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:01:48,885 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:01:48,983 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:01:48,984 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:01:49,059 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:01:49,060 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:01:49,158 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:01:49,158 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:01:49,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:01:49,232 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:01:49,334 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:01:49,334 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:01:49,411 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:01:49,411 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:01:49,509 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:01:49,509 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:01:49,580 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:01:49,581 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:01:49,678 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:01:49,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:01:49,754 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:01:49,755 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:01:49,854 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:01:49,854 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:01:49,931 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:01:49,932 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:01:50,033 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:01:50,033 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:01:50,110 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:01:50,111 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:01:50,211 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:01:50,211 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:01:50,280 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:01:50,281 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:01:50,384 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:01:50,385 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:01:50,461 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:01:50,461 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:01:50,558 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:01:50,558 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:01:50,629 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:01:50,630 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:01:50,727 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:01:50,727 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:01:50,803 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:01:50,804 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:01:50,900 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:01:50,900 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:01:50,970 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:01:50,971 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:01:51,071 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:01:51,071 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:01:51,148 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:01:51,149 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:01:51,250 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:01:51,250 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:01:51,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:01:51,322 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:01:51,425 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:01:51,425 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:01:51,501 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:01:51,502 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:01:51,599 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:01:51,599 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:01:51,671 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:01:51,672 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:01:51,771 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:01:51,771 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:01:51,849 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:01:51,849 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:01:51,948 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:01:51,948 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,018 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:01:52,019 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:01:52,119 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,119 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,194 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:01:52,194 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:01:52,291 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,291 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,363 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:01:52,364 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:01:52,462 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:01:52,462 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:01:52,539 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:01:52,540 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:01:52,640 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:01:52,641 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:01:52,712 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:01:52,712 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:01:52,812 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:01:52,813 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:01:52,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:01:52,896 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:01:52,995 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:01:52,995 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:01:53,071 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:01:53,071 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:01:53,175 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:01:53,175 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:01:53,250 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:01:53,251 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:20:45,371 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:20:45,372 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:20:48,446 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:20:51,576 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:20:54,669 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:20:57,764 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:21:00,973 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:21:04,172 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:21:07,346 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:21:10,398 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:21:14,896 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:21:19,491 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:21:24,214 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:21:24,214 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:21:24,214 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:21:24,215 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:21:24,216 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:21:24,216 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:21:24,217 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:21:24,217 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:21:24,217 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:21:24,218 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:21:24,250 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:21:24,385 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:21:24,385 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:21:24,510 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:21:24,511 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:21:24,609 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:21:24,610 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:21:24,679 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:21:24,679 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:21:24,785 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:21:24,785 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:21:24,861 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:21:24,862 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:21:24,962 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:21:24,962 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:21:25,033 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:21:25,033 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:21:25,137 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:21:25,137 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,211 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:21:25,212 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:21:25,312 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,312 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,380 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:21:25,381 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:21:25,484 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,484 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,564 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:21:25,565 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:21:25,664 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:21:25,664 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:21:25,733 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:21:25,733 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:21:25,833 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:21:25,833 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:21:25,912 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:21:25,912 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:21:26,010 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:21:26,010 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:21:26,085 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:21:26,085 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:21:26,190 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:21:26,190 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:21:26,263 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:21:26,263 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:21:26,363 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:21:26,363 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:21:26,430 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:21:26,431 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:21:26,533 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:21:26,533 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:21:26,608 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:21:26,609 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:21:26,709 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:21:26,710 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:21:26,788 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:21:26,789 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:21:26,890 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:21:26,890 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:21:26,973 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:21:26,973 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:21:27,073 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:21:27,073 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:21:27,148 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:21:27,148 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:21:27,249 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:21:27,250 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:21:27,326 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:21:27,327 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:21:27,426 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:21:27,426 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:21:27,494 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:21:27,494 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:21:27,595 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:21:27,595 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:21:27,672 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:21:27,672 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:21:27,771 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:21:27,772 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:21:27,841 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:21:27,841 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:21:27,943 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:21:27,944 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:21:28,019 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:21:28,020 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:21:28,118 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:21:28,119 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:21:28,192 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:21:28,193 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:21:28,290 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:21:28,291 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:21:28,366 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:21:28,366 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:21:28,467 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:21:28,468 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:21:28,536 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:21:28,536 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:21:28,638 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:21:28,638 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:21:28,713 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:21:28,714 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:21:28,812 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:21:28,812 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:21:28,880 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:21:28,881 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:21:28,980 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:21:28,980 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:21:29,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:21:29,064 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:21:29,162 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:21:29,162 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:21:29,230 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:21:29,231 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:21:29,330 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:21:29,330 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:21:29,405 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:21:29,405 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:21:29,503 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:21:29,503 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:21:29,572 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:21:29,573 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:21:29,672 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:21:29,672 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:21:29,748 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:21:29,748 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:21:29,847 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:21:29,847 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:21:29,915 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:21:29,915 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:21:30,016 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:21:30,017 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:21:30,095 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:21:30,095 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:21:30,193 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:21:30,193 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:21:30,261 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:21:30,262 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:21:30,361 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:21:30,361 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:21:30,437 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:21:30,437 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:21:30,535 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:21:30,536 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:21:30,604 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:21:30,604 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:21:30,703 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:21:30,703 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:21:30,779 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:21:30,779 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:29:13,219 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:29:13,219 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:29:16,405 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:29:19,625 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:29:22,783 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:29:25,953 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:29:29,247 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:29:32,513 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:29:35,761 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:29:38,878 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:29:43,500 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:29:48,131 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:29:52,967 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:29:52,967 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:29:52,967 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:29:52,968 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:29:52,969 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:29:52,969 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:29:52,970 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:29:52,970 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:29:52,971 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:29:52,971 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:29:53,003 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:29:53,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:29:53,142 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,276 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:29:53,277 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:29:53,376 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:29:53,444 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:29:53,445 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:29:53,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:29:53,548 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,623 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:29:53,623 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:29:53,724 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,725 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,793 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:29:53,794 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:29:53,893 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:29:53,893 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:29:53,970 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:29:53,970 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:29:54,070 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:29:54,071 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:29:54,139 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:29:54,140 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:29:54,241 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:29:54,242 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:29:54,318 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:29:54,318 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:29:54,418 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:29:54,418 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:29:54,485 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:29:54,486 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:29:54,585 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:29:54,586 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:29:54,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:29:54,663 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:29:54,762 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:29:54,763 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:29:54,830 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:29:54,830 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:29:54,930 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:29:54,930 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:29:55,007 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:29:55,008 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:29:55,109 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:29:55,110 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:29:55,176 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:29:55,177 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:29:55,279 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:29:55,280 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:29:55,355 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:29:55,355 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:29:55,457 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:29:55,457 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:29:55,525 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:29:55,526 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:29:55,631 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:29:55,631 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:29:55,706 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:29:55,707 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:29:55,804 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:29:55,805 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:29:55,873 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:29:55,873 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:29:55,977 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:29:55,977 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:29:56,053 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:29:56,053 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:29:56,151 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:29:56,151 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:29:56,225 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:29:56,225 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:29:56,324 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:29:56,324 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:29:56,399 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:29:56,399 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:29:56,499 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:29:56,500 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:29:56,567 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:29:56,567 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:29:56,665 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:29:56,665 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:29:56,740 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:29:56,741 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:29:56,838 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:29:56,839 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:29:56,907 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:29:56,907 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:29:57,008 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:29:57,008 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:29:57,085 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:29:57,085 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:29:57,185 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:29:57,185 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:29:57,263 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:29:57,264 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:29:57,368 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:29:57,368 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:29:57,441 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:29:57,441 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:29:57,540 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:29:57,541 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:29:57,607 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:29:57,608 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:29:57,706 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:29:57,706 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:29:57,783 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:29:57,784 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:29:57,883 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:29:57,884 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:29:57,953 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:29:57,954 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:29:58,057 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:29:58,058 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:29:58,132 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:29:58,133 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:29:58,231 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:29:58,231 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:29:58,306 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:29:58,407 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,407 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,482 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:29:58,482 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:29:58,583 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,583 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,653 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:29:58,653 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:29:58,758 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:29:58,758 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:29:58,832 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:29:58,833 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:29:58,931 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:29:58,931 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:29:58,999 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:29:59,000 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:29:59,103 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:29:59,103 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:29:59,179 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:29:59,180 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:29:59,281 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:29:59,281 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:29:59,349 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:29:59,350 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:29:59,448 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:29:59,448 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:29:59,524 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:29:59,525 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:35:56,840 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:35:56,841 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:35:58,339 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:35:59,780 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:36:01,272 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:36:02,708 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:36:04,285 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:36:05,824 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:36:07,346 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:36:08,762 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:36:11,010 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:36:13,272 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:36:15,721 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:36:15,721 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:36:15,721 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:36:15,721 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:36:15,722 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:36:15,722 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:36:15,722 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:36:15,722 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:36:15,723 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:36:15,723 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:36:15,754 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:36:15,824 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:36:15,824 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:36:15,926 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:36:15,926 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:36:15,984 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:36:15,985 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:36:16,052 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:36:16,052 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:36:16,105 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:36:16,105 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:36:16,165 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:36:16,166 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:36:16,220 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:36:16,220 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:36:16,285 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:36:16,286 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:36:16,338 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:36:16,339 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,398 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:36:16,399 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:36:16,452 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,452 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,521 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:36:16,522 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:36:16,576 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,577 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,642 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:36:16,642 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:36:16,695 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:36:16,695 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:36:16,767 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:36:16,767 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:36:16,821 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:36:16,822 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:36:16,884 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:36:16,885 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:36:16,942 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:36:16,942 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:36:17,003 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:36:17,003 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:36:17,058 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:36:17,058 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:36:17,119 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:36:17,120 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:36:17,173 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:36:17,173 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:36:17,235 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:36:17,236 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:36:17,289 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:36:17,289 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:36:17,358 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:36:17,359 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:36:17,416 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:36:17,416 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:36:17,480 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:36:17,481 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:36:17,535 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:36:17,535 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:36:17,598 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:36:17,599 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:36:17,656 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:36:17,657 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:36:17,719 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:36:17,719 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:36:17,772 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:36:17,772 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:36:17,832 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:36:17,833 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:36:17,885 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:36:17,885 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:36:17,947 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:36:17,949 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:36:18,003 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:36:18,003 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:36:18,066 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:36:18,067 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:36:18,125 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:36:18,125 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:36:18,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:36:18,187 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:36:18,242 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:36:18,242 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:36:18,312 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:36:18,313 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:36:18,369 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:36:18,369 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:36:18,428 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:36:18,428 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:36:18,482 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:36:18,482 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:36:18,543 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:36:18,543 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:36:18,600 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:36:18,600 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:36:18,663 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:36:18,664 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:36:18,716 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:36:18,717 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:36:18,776 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:36:18,776 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:36:18,828 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:36:18,828 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:36:18,891 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:36:18,892 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:36:18,944 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:36:18,944 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:36:19,006 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:36:19,006 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:36:19,062 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:36:19,063 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:36:19,126 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:36:19,127 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:36:19,180 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:36:19,181 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:36:19,244 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:36:19,244 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:36:19,296 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:36:19,297 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,368 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:36:19,368 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:36:19,422 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,422 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,487 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:36:19,487 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:36:19,540 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,540 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,601 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:36:19,602 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:36:19,655 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:36:19,656 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:36:19,717 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:36:19,718 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:36:19,774 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:36:19,774 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:36:19,836 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:36:19,836 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:36:19,890 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:36:19,890 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:36:19,952 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:36:19,953 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:36:20,008 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:36:20,008 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:36:20,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:36:20,069 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:36:20,122 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:36:20,122 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:36:20,183 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:36:20,184 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:40:11,414 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:40:11,415 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:40:12,895 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:40:14,352 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:40:15,804 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:40:17,255 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:40:18,827 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:40:20,390 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:40:21,930 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:40:23,348 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:40:25,604 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:40:27,869 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:40:30,325 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:40:30,326 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:40:30,327 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:40:30,328 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:40:30,330 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:40:30,331 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:40:30,333 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:40:30,334 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:40:30,336 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:40:30,337 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:40:30,370 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:40:30,451 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:40:30,451 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:40:30,554 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:40:30,556 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:40:30,614 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:40:30,614 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:40:30,676 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:40:30,677 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:40:30,738 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:40:30,739 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:40:59,964 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:40:59,965 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:41:01,420 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:41:26,888 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:41:26,889 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:41:28,340 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:41:29,782 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:41:31,220 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:41:32,662 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:41:34,229 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:41:35,780 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:41:37,336 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:41:38,742 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:41:40,977 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:41:43,234 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:41:45,669 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:41:45,670 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:41:45,671 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:41:45,672 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:41:45,674 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:41:45,675 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:41:45,677 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:41:45,678 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:41:45,678 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:41:45,680 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:41:45,713 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:41:45,792 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:41:45,792 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:41:45,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:41:45,896 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:41:45,956 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:41:45,956 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:41:46,015 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:41:46,017 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:41:46,075 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:41:46,076 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:41:46,134 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:41:46,136 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:41:46,194 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:41:46,194 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:41:46,255 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:41:46,257 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:41:46,314 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:41:46,315 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,380 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:41:46,381 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:41:46,440 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,440 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,499 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:41:46,500 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:41:46,563 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,563 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,622 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:41:46,624 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:41:46,682 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:41:46,682 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:41:46,740 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:41:46,741 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:41:46,802 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:41:46,802 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:41:46,863 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:41:46,865 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:41:46,922 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:41:46,923 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:41:46,982 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:41:46,983 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:41:47,044 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:41:47,044 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:41:47,103 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:41:47,104 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:41:47,161 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:41:47,162 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:41:47,222 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:41:47,223 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:41:47,282 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:41:47,282 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:41:47,342 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:41:47,343 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:41:47,401 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:41:47,401 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:41:47,462 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:41:47,464 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:41:47,522 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:41:47,522 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:41:47,581 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:41:47,584 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:41:47,642 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:41:47,642 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:41:47,700 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:41:47,702 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:41:47,759 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:41:47,759 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:41:47,816 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:41:47,818 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:41:47,876 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:41:47,876 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:41:47,935 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:41:47,937 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:41:47,996 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:41:47,996 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:41:48,057 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:41:48,059 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:41:48,117 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:41:48,117 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:41:48,177 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:41:48,178 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:41:48,237 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:41:48,237 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:41:48,295 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:41:48,297 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:41:48,354 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:41:48,354 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:41:48,421 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:41:48,423 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:41:48,481 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:41:48,481 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:41:48,539 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:41:48,540 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:41:48,597 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:41:48,597 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:41:48,658 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:41:48,660 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:41:48,717 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:41:48,717 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:41:48,777 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:41:48,778 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:41:48,836 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:41:48,836 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:41:48,894 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:41:48,896 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:41:48,953 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:41:48,953 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:41:49,011 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:41:49,013 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:41:49,069 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:41:49,069 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:41:49,127 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:41:49,129 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:41:49,187 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:41:49,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:41:49,247 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:41:49,249 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:41:49,305 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:41:49,305 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,366 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:41:49,367 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:41:49,424 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,424 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,497 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:41:49,498 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:41:49,555 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,556 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,615 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:41:49,617 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:41:49,674 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:41:49,674 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:41:49,732 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:41:49,733 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:41:49,792 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:41:49,792 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:41:49,858 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:41:49,859 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:41:49,917 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:41:49,917 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:41:49,980 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:41:49,981 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:41:50,039 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:41:50,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:41:50,098 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:41:50,099 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:41:50,161 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:41:50,161 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:41:50,229 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:41:50,231 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:43:09,865 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:43:09,866 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:43:11,325 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:43:12,780 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:43:14,225 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:43:15,685 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:43:17,264 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:43:18,829 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:43:20,365 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:43:21,779 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:43:24,036 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:43:26,287 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:43:28,766 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:43:28,767 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:43:28,768 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:43:28,769 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:43:28,770 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:43:28,772 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:43:28,773 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:43:28,775 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:43:28,775 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:43:28,777 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:43:28,820 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:43:28,901 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:43:28,901 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,004 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:43:29,006 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:43:29,066 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,067 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:43:29,126 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:43:29,128 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:43:29,188 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:43:29,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,245 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:43:29,247 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:43:29,305 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,305 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,366 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:43:29,368 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:43:29,430 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:43:29,431 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,489 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:43:29,491 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:43:29,550 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,550 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,609 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:43:29,611 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:43:29,669 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,669 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:43:29,730 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:43:29,788 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:43:29,788 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:43:29,846 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:43:29,848 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:43:29,906 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:43:29,906 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:43:29,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:43:29,967 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:43:30,028 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:43:30,028 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:43:30,087 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:43:30,089 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:43:30,149 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:43:30,149 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:43:30,209 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:43:30,211 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:43:30,268 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:43:30,269 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:43:30,326 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:43:30,328 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:43:30,388 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:43:30,388 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:43:30,455 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:43:30,457 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:43:30,515 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:43:30,516 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:43:30,575 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:43:30,576 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:43:30,636 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:43:30,636 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:43:30,694 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:43:30,696 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:43:30,755 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:43:30,755 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:43:30,812 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:43:30,814 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:43:30,872 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:43:30,872 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:43:30,930 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:43:30,932 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:43:30,990 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:43:30,990 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:43:31,052 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:43:31,053 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:43:31,111 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:43:31,111 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:43:31,170 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:43:31,171 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:43:31,231 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:43:31,231 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:43:31,289 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:43:31,291 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:43:31,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:43:31,348 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:43:31,414 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:43:31,416 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:43:31,473 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:43:31,473 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:43:31,533 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:43:31,535 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:43:31,592 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:43:31,592 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:43:31,652 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:43:31,654 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:43:31,712 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:43:31,712 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:43:31,771 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:43:31,773 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:43:31,831 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:43:31,831 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:43:31,889 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:43:31,891 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:43:31,949 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:43:31,949 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:43:32,007 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:43:32,008 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:43:32,067 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:43:32,067 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:43:32,129 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:43:32,131 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:43:32,189 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:43:32,189 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:43:32,250 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:43:32,252 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:43:32,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:43:32,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:43:32,372 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:43:32,373 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:43:32,432 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:43:32,432 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,505 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:43:32,507 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:43:32,563 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,563 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,628 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:43:32,630 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:43:32,688 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,688 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,748 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:43:32,750 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:43:32,807 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:43:32,808 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:43:32,868 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:43:32,869 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:43:32,926 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:43:32,927 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:43:32,989 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:43:32,990 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:43:33,050 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:43:33,051 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:43:33,115 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:43:33,117 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:43:33,178 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:43:33,178 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:43:33,246 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:43:33,248 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:43:33,308 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:43:33,308 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:43:33,375 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:43:33,376 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 23:48:03,417 font_manager.py[line:1334] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=15.0.
2021-01-09 23:48:03,418 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,418 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmsy10' (cmsy10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,418 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,418 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Italic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmex10' (cmex10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,419 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniIta.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFiveSym' (STIXSizFiveSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmtt10' (cmtt10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif Display' (DejaVuSerifDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Display' (DejaVuSansDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmss10' (cmss10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,420 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-BoldItalic.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmr10' (cmr10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmmi10' (cmmi10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUni.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,421 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmb10' (cmb10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralItalic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,422 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEO.TTF) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCB_____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harrington' (HARNGTON.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bradley Hand ITC' (BRADHITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comici.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothL.ttc) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,423 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimHei' (simhei.ttf) normal normal 400 normal>) = 0.05
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed Extra Bold' (TCCEB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesi.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHV.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Console' (lucon.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Broadway' (BROADW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,424 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Poor Richard' (POORICH.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailu.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Magneto' (MAGNETOB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Symbol' (symbol.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelawUI.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (Sitka.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:03,425 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_R.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibl.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (CENSCBK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgia.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaI.ttc) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,426 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Modern No. 20' (MOD20.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (Nirmala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibriz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXinwei' (STXINWEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'YouYuan' (SIMYOU.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi Cond' (FRADMCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariblk.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,427 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (BKANT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSI.TTF) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Deng.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Monotype Corsiva' (MTCORSVA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdana.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZShuTi' (FZSTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rage Italic' (RAGE.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,428 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Light' (COPRGTL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Viner Hand ITC' (VINERITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STKaiti' (STKAITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kristen ITC' (ITCKRIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tempus Sans ITC' (TEMPSITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'OCR A Extended' (OCRAEXT.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Javanese Text' (javatext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,429 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibri.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Medium ITC' (ERASMD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgun.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arial.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaB.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,430 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taile.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Extra Bold' (ROCKEB.TTF) normal normal 800 normal>) = 10.43
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (cour.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'French Script MT' (FRSCRIPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Blackadder ITC' (ITCBLKAD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Emoji' (seguiemj.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,431 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Onyx' (ONYX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Sans Serif' (micross.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontC' (jdiconfontC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun-ExtB' (simsunb.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailub.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Castellar' (CASTELAR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoeprb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,432 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Jokerman' (JOKERMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bauhaus 93' (BAUHS93.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MingLiU-ExtB' (mingliub.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Old English Text MT' (OLDENGL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_B.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelli.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mistral' (MISTRAL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palabi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,433 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbell.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kunstler Script' (KUNSTLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtextb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CR.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTIBD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,434 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisb.ttf) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Algerian' (ALGER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Solid' (NIAGSOL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sylfaen' (sylfaen.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Calligraphy' (LCALLIG.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Maiandra GD' (MAIAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontB' (jdiconfontB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelUIsl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,435 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framd.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Script MT Bold' (SCRIPTBL.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bahnschrift' (bahnschrift.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 2' (WINGDNG2.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaZ.ttc) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,436 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Brush Script MT' (BRUSHSCI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (times.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOS.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCM____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (couri.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,437 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Chiller' (CHILLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GIL_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'JdIonicons' (jdIonicons.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Freestyle Script' (FREESCPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,438 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings' (wingding.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Colonna MT' (COLONNA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXihei' (STXIHEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTILI.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Specialty' (REFSPCL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STCaiyun' (STCAIYUN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,439 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCB____.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Matura MT Script Capitals' (MATURASC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothR.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taileb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Stencil' (STENCIL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palace Script MT' (PALSCRI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe MDL2 Assets' (segmdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,440 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STHupo' (STHUPO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Felix Titling' (FELIXTI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ink Free' (Inkfree.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Handwriting' (LHANDW.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Unicode MS' (ARIALUNI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Light ITC' (ERASLGHT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vivaldi' (VIVALDII.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,441 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Haettenschweiler' (HATTEN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Yi Baiti' (msyi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harlow Solid Italic' (HARLOWSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuil.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,442 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZLanTingHeiS-UL-GB' (FZLTCXHJW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspa.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Britannic Bold' (BRITANIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCCB___.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABKIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Condensed' (GILC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,443 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADMIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibli.ttf) italic normal 900 normal>) = 11.525
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeui.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontA' (jdiconfontA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium Cond' (FRAMDCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontAwesome' (jdFontAwesome.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Edwardian Script ITC' (ITCEDSCR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Playbill' (PLAYBILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,444 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Showcard Gothic' (SHOWG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrimabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCBI____.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constan.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mongolian Baiti' (monbaiti.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbeli.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,445 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'LiSu' (SIMLI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookshelf Symbol 7' (BSSYM7.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framdit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Engraved' (NIAGENG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MV Boli' (mvboli.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,446 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STZhongsong' (STZHONGS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahoma.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Imprint MT Shadow' (IMPRISHA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wide Latin' (LATINWD.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Rounded MT Bold' (ARLRDBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'KaiTi' (simkai.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Webdings' (webdings.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,447 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunsl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Forte' (FORTE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Gothic' (msgothic.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbel.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,448 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAI.TTF) italic normal 900 normal>) = 11.525
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'TeamViewer15' (teamviewer15.otf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelaUIb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Symbol' (seguisym.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STSong' (STSONG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,449 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (pala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_PSTC.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Footlight MT Light' (FTLTLT.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCM_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUABI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Engravers MT' (ENGR.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,450 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Impact' (impact.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITED.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Outlook' (OUTLOOK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHVIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoepr.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoFont' (jdIcoFont.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaS.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,451 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSBI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constani.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrima.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century' (CENTURY.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,452 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAR.TTF) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candara.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bernard MT Condensed' (BERNHC.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 3' (WINGDNG3.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,453 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MT Extra' (MTEXTRA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCMI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Juice ITC' (JUICE___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoMoonFree' (jdIcoMoonFree.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold Condensed' (GILLUBCD.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STLiti' (STLITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,454 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Informal Roman' (INFROMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuiz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Unicode' (l_10646.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXingkai' (STXINGKA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaraz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,455 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Centaur' (CENTAUR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Demi ITC' (ERASDEMI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold' (GILSANUB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Curlz MT' (CURLZ___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Himalaya' (himalaya.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Parchment' (PARCHM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,456 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Baskerville Old Face' (BASKVILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STFangsong' (STFANGSO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FangSong' (simfang.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambria.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoesc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun' (simsun.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Bold' (COPRGTB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARAIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,457 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'HoloLens MDL2 Assets' (holomdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Marlett' (marlett.ttf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuisl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Pristina' (PRISTINA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_I.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,458 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebuc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoescb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Sans Serif' (REFSAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ravie' (RAVIE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEBO.TTF) oblique normal 600 normal>) = 11.24
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Historic' (seguihis.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,459 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB Demi' (BRLNSDB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Papyrus' (PAPYRUS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Ext Condensed Bold' (GLSNECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gabriola' (Gabriola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahomabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,460 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vladimir Script' (VLADIMIR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibril.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cooper Black' (COOPBL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gigi' (GIGI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PER_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comic.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontD' (jdiconfontD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Stout' (GOUDYSTO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,461 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothM.ttc) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Snap ITC' (SNAP____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZYaoTi' (FZYTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisli.ttf) italic normal 350 normal>) = 11.0975
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontCustom' (jdFontCustom.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,462 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Bold ITC' (ERASBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARABD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisbi.ttf) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,463 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gloucester MT Extra Condensed' (GLECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,464 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,464 font_manager.py[line:1366] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=15.0 to SimHei ('C:\\Windows\\Fonts\\simhei.ttf') with score of 0.050000.
2021-01-09 23:48:03,508 font_manager.py[line:1334] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.
2021-01-09 23:48:03,508 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,508 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmsy10' (cmsy10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,508 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,508 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Italic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmex10' (cmex10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,509 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniIta.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFiveSym' (STIXSizFiveSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmtt10' (cmtt10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif Display' (DejaVuSerifDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Display' (DejaVuSansDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmss10' (cmss10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,510 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-BoldItalic.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmr10' (cmr10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmmi10' (cmmi10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUni.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmb10' (cmb10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,511 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralItalic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,512 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEO.TTF) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCB_____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harrington' (HARNGTON.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bradley Hand ITC' (BRADHITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comici.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothL.ttc) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimHei' (simhei.ttf) normal normal 400 normal>) = 0.05
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,513 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed Extra Bold' (TCCEB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesi.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHV.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Console' (lucon.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Broadway' (BROADW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Poor Richard' (POORICH.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailu.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Magneto' (MAGNETOB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,514 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Symbol' (symbol.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelawUI.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (Sitka.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_R.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,515 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibl.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (CENSCBK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgia.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaI.ttc) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Modern No. 20' (MOD20.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (Nirmala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,516 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibriz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXinwei' (STXINWEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'YouYuan' (SIMYOU.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi Cond' (FRADMCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariblk.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (BKANT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,517 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSI.TTF) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Deng.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Monotype Corsiva' (MTCORSVA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdana.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZShuTi' (FZSTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rage Italic' (RAGE.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Light' (COPRGTL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Viner Hand ITC' (VINERITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,518 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STKaiti' (STKAITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kristen ITC' (ITCKRIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tempus Sans ITC' (TEMPSITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'OCR A Extended' (OCRAEXT.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Javanese Text' (javatext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibri.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,519 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Medium ITC' (ERASMD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgun.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arial.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaB.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taile.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Extra Bold' (ROCKEB.TTF) normal normal 800 normal>) = 10.43
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (cour.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'French Script MT' (FRSCRIPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,520 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Blackadder ITC' (ITCBLKAD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Emoji' (seguiemj.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Onyx' (ONYX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Sans Serif' (micross.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontC' (jdiconfontC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun-ExtB' (simsunb.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailub.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Castellar' (CASTELAR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,521 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoeprb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Jokerman' (JOKERMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bauhaus 93' (BAUHS93.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MingLiU-ExtB' (mingliub.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Old English Text MT' (OLDENGL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_B.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelli.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mistral' (MISTRAL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palabi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,522 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbell.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kunstler Script' (KUNSTLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtextb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CR.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTIBD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,523 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisb.ttf) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Algerian' (ALGER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Solid' (NIAGSOL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sylfaen' (sylfaen.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Calligraphy' (LCALLIG.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Maiandra GD' (MAIAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontB' (jdiconfontB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,524 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelUIsl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framd.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Script MT Bold' (SCRIPTBL.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bahnschrift' (bahnschrift.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 2' (WINGDNG2.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,525 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaZ.ttc) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Brush Script MT' (BRUSHSCI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (times.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOS.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCM____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,526 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (couri.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Chiller' (CHILLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GIL_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'JdIonicons' (jdIonicons.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Freestyle Script' (FREESCPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,527 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings' (wingding.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Colonna MT' (COLONNA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXihei' (STXIHEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTILI.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Specialty' (REFSPCL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STCaiyun' (STCAIYUN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCB____.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,528 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Matura MT Script Capitals' (MATURASC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothR.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taileb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Stencil' (STENCIL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palace Script MT' (PALSCRI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe MDL2 Assets' (segmdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STHupo' (STHUPO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,529 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Felix Titling' (FELIXTI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ink Free' (Inkfree.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Handwriting' (LHANDW.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Unicode MS' (ARIALUNI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Light ITC' (ERASLGHT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vivaldi' (VIVALDII.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,530 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Haettenschweiler' (HATTEN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Yi Baiti' (msyi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harlow Solid Italic' (HARLOWSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuil.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZLanTingHeiS-UL-GB' (FZLTCXHJW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspa.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,531 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Britannic Bold' (BRITANIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCCB___.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABKIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Condensed' (GILC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADMIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibli.ttf) italic normal 900 normal>) = 11.525
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeui.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,532 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontA' (jdiconfontA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium Cond' (FRAMDCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontAwesome' (jdFontAwesome.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Edwardian Script ITC' (ITCEDSCR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Playbill' (PLAYBILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Showcard Gothic' (SHOWG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,533 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrimabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCBI____.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constan.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mongolian Baiti' (monbaiti.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbeli.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'LiSu' (SIMLI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,534 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookshelf Symbol 7' (BSSYM7.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framdit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Engraved' (NIAGENG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MV Boli' (mvboli.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STZhongsong' (STZHONGS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahoma.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,535 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Imprint MT Shadow' (IMPRISHA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wide Latin' (LATINWD.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Rounded MT Bold' (ARLRDBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'KaiTi' (simkai.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Webdings' (webdings.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunsl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Forte' (FORTE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,536 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Gothic' (msgothic.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbel.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAI.TTF) italic normal 900 normal>) = 11.525
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'TeamViewer15' (teamviewer15.otf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelaUIb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,537 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Symbol' (seguisym.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STSong' (STSONG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (pala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_PSTC.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Footlight MT Light' (FTLTLT.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCM_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,538 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUABI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Engravers MT' (ENGR.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Impact' (impact.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITED.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Outlook' (OUTLOOK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHVIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoepr.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,539 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoFont' (jdIcoFont.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaS.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSBI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constani.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrima.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,540 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century' (CENTURY.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAR.TTF) normal normal 900 normal>) = 10.525
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candara.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bernard MT Condensed' (BERNHC.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,541 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 3' (WINGDNG3.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MT Extra' (MTEXTRA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCMI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Juice ITC' (JUICE___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,542 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoMoonFree' (jdIcoMoonFree.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold Condensed' (GILLUBCD.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STLiti' (STLITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Informal Roman' (INFROMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuiz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,543 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Unicode' (l_10646.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXingkai' (STXINGKA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaraz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Centaur' (CENTAUR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Demi ITC' (ERASDEMI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold' (GILSANUB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Curlz MT' (CURLZ___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,544 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Himalaya' (himalaya.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Parchment' (PARCHM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Baskerville Old Face' (BASKVILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STFangsong' (STFANGSO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FangSong' (simfang.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambria.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,545 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoesc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun' (simsun.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Bold' (COPRGTB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARAIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'HoloLens MDL2 Assets' (holomdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Marlett' (marlett.ttf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuisl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:03,546 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Pristina' (PRISTINA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_I.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebuc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoescb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Sans Serif' (REFSAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ravie' (RAVIE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,547 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEBO.TTF) oblique normal 600 normal>) = 11.24
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Historic' (seguihis.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB Demi' (BRLNSDB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Papyrus' (PAPYRUS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Ext Condensed Bold' (GLSNECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,548 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gabriola' (Gabriola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahomabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vladimir Script' (VLADIMIR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibril.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cooper Black' (COOPBL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gigi' (GIGI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,549 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PER_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comic.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontD' (jdiconfontD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Stout' (GOUDYSTO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothM.ttc) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Snap ITC' (SNAP____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZYaoTi' (FZYTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,550 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisli.ttf) italic normal 350 normal>) = 11.0975
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontCustom' (jdFontCustom.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Bold ITC' (ERASBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARABD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:03,551 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisbi.ttf) italic normal 600 normal>) = 11.24
2021-01-09 23:48:03,552 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,552 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gloucester MT Extra Condensed' (GLECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:03,552 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:03,552 font_manager.py[line:1366] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to SimHei ('C:\\Windows\\Fonts\\simhei.ttf') with score of 0.050000.
2021-01-09 23:48:18,921 font_manager.py[line:1334] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=25.0.
2021-01-09 23:48:18,921 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,921 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmsy10' (cmsy10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,921 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,921 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Italic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,921 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmex10' (cmex10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniIta.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,922 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeFiveSym' (STIXSizFiveSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmtt10' (cmtt10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif Display' (DejaVuSerifDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Display' (DejaVuSansDisplay.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmss10' (cmss10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBolIta.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-BoldItalic.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,923 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmr10' (cmr10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmmi10' (cmmi10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Bold.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXNonUnicode' (STIXNonUni.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Serif' (DejaVuSerif.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'cmb10' (cmb10.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXGeneral' (STIXGeneralItalic.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,924 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-BoldOblique.ttf) oblique normal 700 normal>) = 11.335
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Oblique.ttf) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymReg.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymBol.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEO.TTF) oblique normal 400 normal>) = 11.05
2021-01-09 23:48:18,925 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCB_____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harrington' (HARNGTON.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bradley Hand ITC' (BRADHITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comici.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothL.ttc) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimHei' (simhei.ttf) normal normal 400 normal>) = 0.05
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed Extra Bold' (TCCEB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesi.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,926 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHV.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Console' (lucon.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Broadway' (BROADW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Poor Richard' (POORICH.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailu.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Magneto' (MAGNETOB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Symbol' (symbol.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,927 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelawUI.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (Sitka.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_R.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILB____.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,928 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibl.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (CENSCBK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgia.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaI.ttc) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Modern No. 20' (MOD20.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (Nirmala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibriz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXinwei' (STXINWEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,929 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'YouYuan' (SIMYOU.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi Cond' (FRADMCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariblk.ttf) normal normal 900 normal>) = 10.525
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (BKANT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSI.TTF) italic normal 300 normal>) = 11.145
2021-01-09 23:48:18,930 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Deng.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Monotype Corsiva' (MTCORSVA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdana.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZShuTi' (FZSTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rage Italic' (RAGE.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjh.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Light' (COPRGTL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Viner Hand ITC' (VINERITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STKaiti' (STKAITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,931 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kristen ITC' (ITCKRIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tempus Sans ITC' (TEMPSITC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'OCR A Extended' (OCRAEXT.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Javanese Text' (javatext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALIST.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNBI.TTF) italic normal 700 condensed>) = 11.535
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibri.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Medium ITC' (ERASMD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgun.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,932 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arial.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,933 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaB.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,933 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taile.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,933 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Extra Bold' (ROCKEB.TTF) normal normal 800 normal>) = 10.43
2021-01-09 23:48:18,933 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (cour.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,933 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'French Script MT' (FRSCRIPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ariali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Blackadder ITC' (ITCBLKAD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Emoji' (seguiemj.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Onyx' (ONYX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Sans Serif' (micross.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontC' (jdiconfontC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun-ExtB' (simsunb.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,934 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft New Tai Lue' (ntailub.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Castellar' (CASTELAR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoeprb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Jokerman' (JOKERMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bauhaus 93' (BAUHS93.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MingLiU-ExtB' (mingliub.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Old English Text MT' (OLDENGL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_B.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelli.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:18,935 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mistral' (MISTRAL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palabi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbell.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Kunstler Script' (KUNSTLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtextb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CR.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,936 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTIBD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisb.ttf) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Algerian' (ALGER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Solid' (NIAGSOL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sylfaen' (sylfaen.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Calligraphy' (LCALLIG.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,937 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Maiandra GD' (MAIAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontB' (jdiconfontB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelUIsl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framd.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Script MT Bold' (SCRIPTBL.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bahnschrift' (bahnschrift.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,938 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 2' (WINGDNG2.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaral.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaZ.ttc) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Brush Script MT' (BRUSHSCI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (times.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOS.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,939 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCM____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (couri.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Chiller' (CHILLER.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Sitka Small' (SitkaB.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrili.ttf) italic normal 300 normal>) = 11.145
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GIL_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'JdIonicons' (jdIonicons.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,940 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Freestyle Script' (FREESCPT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings' (wingding.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Colonna MT' (COLONNA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXihei' (STXIHEI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua Titling MT' (PERTILI.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Specialty' (REFSPCL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STCaiyun' (STCAIYUN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,941 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT Condensed' (TCCB____.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Matura MT Script Capitals' (MATURASC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothR.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Tai Le' (taileb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Stencil' (STENCIL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palace Script MT' (PALSCRI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe MDL2 Assets' (segmdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,942 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STHupo' (STHUPO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Felix Titling' (FELIXTI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ink Free' (Inkfree.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Handwriting' (LHANDW.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Unicode MS' (ARIALUNI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Light ITC' (ERASLGHT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vivaldi' (VIVALDII.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,943 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambriai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Haettenschweiler' (HATTEN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Yi Baiti' (msyi.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Harlow Solid Italic' (HARLOWSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuil.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZLanTingHeiS-UL-GB' (FZLTCXHJW.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PERBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,944 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspa.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Britannic Bold' (BRITANIC.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell Condensed' (ROCCB___.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABKIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Condensed' (GILC____.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADMIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,945 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguibli.ttf) italic normal 900 normal>) = 11.525
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeui.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELLI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontA' (jdiconfontA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITEDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium Cond' (FRAMDCN.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontAwesome' (jdFontAwesome.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Edwardian Script ITC' (ITCEDSCR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Playbill' (PLAYBILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Showcard Gothic' (SHOWG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,946 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gadugi' (gadugib.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrimabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCBI____.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarali.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constan.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Mongolian Baiti' (monbaiti.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbeli.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Myanmar Text' (mmrtext.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,947 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'LiSu' (SIMLI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOSB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookshelf Symbol 7' (BSSYM7.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Times New Roman' (timesbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Medium' (framdit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Book' (FRABK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Niagara Engraved' (NIAGENG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,948 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MV Boli' (mvboli.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STZhongsong' (STZHONGS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahoma.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Imprint MT Shadow' (IMPRISHA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wide Latin' (LATINWD.TTF) normal normal 400 expanded>) = 10.25
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial Rounded MT Bold' (ARLRDBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'KaiTi' (simkai.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Webdings' (webdings.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,949 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunsl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Forte' (FORTE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Gothic' (msgothic.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbel.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft YaHei' (msyhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB' (BRLNSR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Courier New' (courbi.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAI.TTF) italic normal 900 normal>) = 11.525
2021-01-09 23:48:18,950 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'TeamViewer15' (teamviewer15.otf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Leelawadee UI' (LeelaUIb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Symbol' (seguisym.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Elephant' (ELEPHNTI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STSong' (STSONG.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXDI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (pala.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_PSTC.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Footlight MT Light' (FTLTLT.TTF) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,951 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCM_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUABI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Engravers MT' (ENGR.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Impact' (impact.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITED.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Old Style' (GOUDOS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Outlook' (OUTLOOK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Heavy' (FRAHVIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,952 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Print' (segoepr.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoFont' (jdIcoFont.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebucit.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Nirmala UI' (NirmalaS.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhbd.ttc) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bookman Old Style' (BOOKOSBI.TTF) italic normal 600 normal>) = 11.24
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEB.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constani.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ebrima' (ebrima.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,953 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT' (GILBI___.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century' (CENTURY.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft PhagsPa' (phagspab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_BLAR.TTF) normal normal 900 normal>) = 10.525
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candara.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAX.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bernard MT Condensed' (BERNHC.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,954 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bell MT' (BELL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Bright' (LBRITE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Consolas' (consolaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Wingdings 3' (WINGDNG3.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MT Extra' (MTEXTRA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tw Cen MT' (TCMI____.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Juice ITC' (JUICE___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengl.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Palatino Linotype' (palai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,955 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdIcoMoonFree' (jdIcoMoonFree.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold Condensed' (GILLUBCD.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STLiti' (STLITI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanaz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Informal Roman' (INFROMAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuiz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Georgia' (georgiab.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,956 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Unicode' (l_10646.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STXingkai' (STXINGKA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (arialbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candaraz.ttf) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Centaur' (CENTAUR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans' (LSANSI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Demi ITC' (ERASDEMI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans Ultra Bold' (GILSANUB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Curlz MT' (CURLZ___.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Malgun Gothic' (malgunbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft Himalaya' (himalaya.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,957 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNI.TTF) italic normal 400 condensed>) = 11.25
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Parchment' (PARCHM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Baskerville Old Face' (BASKVILL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'STFangsong' (STFANGSO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FangSong' (simfang.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cambria' (cambria.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoesc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,958 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'SimSun' (simsun.ttc) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Copperplate Gothic Bold' (COPRGTB.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARAIT.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calisto MT' (CALISTB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'HoloLens MDL2 Assets' (holomdl2.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Marlett' (marlett.ttf) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Book Antiqua' (ANTQUAI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comicbd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuisl.ttf) normal normal 350 normal>) = 10.0975
2021-01-09 23:48:18,959 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Gothic' (GOTHICB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Candara' (Candarai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Pristina' (PRISTINA.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_I.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Trebuchet MS' (trebuc.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Franklin Gothic Demi' (FRADM.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe Script' (segoescb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'MS Reference Sans Serif' (REFSAN.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Ravie' (RAVIE.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,960 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Sans Typewriter' (LTYPEBO.TTF) oblique normal 600 normal>) = 11.24
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI Historic' (seguihis.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Bodoni MT' (BOD_CB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Berlin Sans FB Demi' (BRLNSDB.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Papyrus' (PAPYRUS.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gill Sans MT Ext Condensed Bold' (GLSNECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Agency FB' (AGENCYR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gabriola' (Gabriola.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,961 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Century Schoolbook' (SCHLBKBI.TTF) italic normal 700 normal>) = 11.335
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Arial' (ARIALNB.TTF) normal normal 700 condensed>) = 10.535
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Tahoma' (tahomabd.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Verdana' (verdanai.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Vladimir Script' (VLADIMIR.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibril.ttf) normal normal 300 normal>) = 10.145
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Cooper Black' (COOPBL.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gigi' (GIGI.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Californian FB' (CALIFI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,962 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Perpetua' (PER_____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Comic Sans MS' (comic.ttf) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdiconfontD' (jdiconfontD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Goudy Stout' (GOUDYSTO.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Yu Gothic' (YuGothM.ttc) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'DengXian' (Dengb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Snap ITC' (SNAP____.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Lucida Fax' (LFAXD.TTF) normal normal 600 normal>) = 10.24
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'FZYaoTi' (FZYTK.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisli.ttf) italic normal 350 normal>) = 11.0975
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'jdFontCustom' (jdFontCustom.TTF) normal normal 500 normal>) = 10.145
2021-01-09 23:48:18,963 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'High Tower Text' (HTOWERT.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Rockwell' (ROCKI.TTF) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Corbel' (corbelb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Eras Bold ITC' (ERASBD.TTF) normal normal 400 normal>) = 10.05
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Garamond' (GARABD.TTF) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Constantia' (constanb.ttf) normal normal 700 normal>) = 10.335
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Microsoft JhengHei' (msjhl.ttc) normal normal 290 normal>) = 10.1545
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (seguisbi.ttf) italic normal 600 normal>) = 11.24
2021-01-09 23:48:18,964 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Segoe UI' (segoeuii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,965 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Gloucester MT Extra Condensed' (GLECB.TTF) normal normal 400 condensed>) = 10.25
2021-01-09 23:48:18,965 font_manager.py[line:1346] - DEBUG - matplotlib.font_manager : findfont: score(<Font 'Calibri' (calibrii.ttf) italic normal 400 normal>) = 11.05
2021-01-09 23:48:18,965 font_manager.py[line:1366] - DEBUG - matplotlib.font_manager : findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=25.0 to SimHei ('C:\\Windows\\Fonts\\simhei.ttf') with score of 0.050000.
2021-01-09 23:52:43,427 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 23:52:43,428 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 23:52:46,527 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 23:52:49,665 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 23:52:52,742 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 23:52:55,836 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 23:52:59,061 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 23:53:02,262 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 23:53:05,433 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 23:53:08,481 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 23:53:12,980 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 23:53:17,499 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 23:53:22,232 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 23:53:22,232 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 23:53:22,232 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 23:53:22,233 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 23:53:22,233 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 23:53:22,234 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 23:53:22,235 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 23:53:22,235 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 23:53:22,235 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 23:53:22,235 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 23:53:22,268 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 23:53:22,402 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.75% (3748 / 4176)
2021-01-09 23:53:22,402 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:53:22,532 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 23:53:22,533 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 23:53:22,631 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:53:22,631 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.82% (3751 / 4176)
2021-01-09 23:53:22,705 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 23:53:22,706 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 23:53:22,806 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.82% (3751 / 4176)
2021-01-09 23:53:22,806 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:53:22,890 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 23:53:22,891 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 23:53:22,989 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:53:22,989 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3750 / 4176)
2021-01-09 23:53:23,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 23:53:23,064 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 23:53:23,161 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3750 / 4176)
2021-01-09 23:53:23,162 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,237 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 23:53:23,238 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 23:53:23,336 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,337 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,405 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 23:53:23,406 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 23:53:23,509 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,509 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,586 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 23:53:23,586 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 23:53:23,685 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3753 / 4176)
2021-01-09 23:53:23,685 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:53:23,753 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 23:53:23,754 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 23:53:23,855 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:53:23,855 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.89% (3754 / 4176)
2021-01-09 23:53:23,930 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 23:53:23,930 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 23:53:24,032 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.89% (3754 / 4176)
2021-01-09 23:53:24,032 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.92% (3755 / 4176)
2021-01-09 23:53:24,103 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 23:53:24,104 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 23:53:24,205 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.92% (3755 / 4176)
2021-01-09 23:53:24,205 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3757 / 4176)
2021-01-09 23:53:24,279 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 23:53:24,280 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 23:53:24,379 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3757 / 4176)
2021-01-09 23:53:24,379 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3758 / 4176)
2021-01-09 23:53:24,446 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 23:53:24,446 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 23:53:24,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3758 / 4176)
2021-01-09 23:53:24,548 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.01% (3759 / 4176)
2021-01-09 23:53:24,622 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 23:53:24,623 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 23:53:24,723 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.01% (3759 / 4176)
2021-01-09 23:53:24,723 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3760 / 4176)
2021-01-09 23:53:24,791 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 23:53:24,791 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 23:53:24,891 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3760 / 4176)
2021-01-09 23:53:24,891 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3761 / 4176)
2021-01-09 23:53:24,970 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 23:53:24,971 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 23:53:25,070 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3761 / 4176)
2021-01-09 23:53:25,070 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3762 / 4176)
2021-01-09 23:53:25,137 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 23:53:25,138 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 23:53:25,239 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3762 / 4176)
2021-01-09 23:53:25,239 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.11% (3763 / 4176)
2021-01-09 23:53:25,315 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 23:53:25,316 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 23:53:25,415 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.11% (3763 / 4176)
2021-01-09 23:53:25,415 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:53:25,486 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 23:53:25,486 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 23:53:25,585 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:53:25,585 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3765 / 4176)
2021-01-09 23:53:25,661 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 23:53:25,662 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 23:53:25,760 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3765 / 4176)
2021-01-09 23:53:25,760 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.18% (3766 / 4176)
2021-01-09 23:53:25,828 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 23:53:25,828 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 23:53:25,926 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.18% (3766 / 4176)
2021-01-09 23:53:25,926 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3768 / 4176)
2021-01-09 23:53:26,003 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 23:53:26,004 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 23:53:26,102 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3768 / 4176)
2021-01-09 23:53:26,103 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.25% (3769 / 4176)
2021-01-09 23:53:26,171 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 23:53:26,171 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 23:53:26,277 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.25% (3769 / 4176)
2021-01-09 23:53:26,277 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3770 / 4176)
2021-01-09 23:53:26,362 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 23:53:26,363 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 23:53:26,461 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3770 / 4176)
2021-01-09 23:53:26,461 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.30% (3771 / 4176)
2021-01-09 23:53:26,538 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 23:53:26,539 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 23:53:26,643 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.30% (3771 / 4176)
2021-01-09 23:53:26,643 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.35% (3773 / 4176)
2021-01-09 23:53:26,717 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 23:53:26,717 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 23:53:26,816 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.35% (3773 / 4176)
2021-01-09 23:53:26,817 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.40% (3775 / 4176)
2021-01-09 23:53:26,884 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 23:53:26,885 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 23:53:26,984 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.40% (3775 / 4176)
2021-01-09 23:53:26,985 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:53:27,060 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 23:53:27,061 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 23:53:27,160 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:53:27,160 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3777 / 4176)
2021-01-09 23:53:27,230 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 23:53:27,231 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 23:53:27,335 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3777 / 4176)
2021-01-09 23:53:27,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.47% (3778 / 4176)
2021-01-09 23:53:27,409 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 23:53:27,409 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 23:53:27,507 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.47% (3778 / 4176)
2021-01-09 23:53:27,507 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:53:27,577 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 23:53:27,578 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 23:53:27,677 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:53:27,677 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:53:27,752 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 23:53:27,753 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 23:53:27,850 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:53:27,850 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.52% (3780 / 4176)
2021-01-09 23:53:27,918 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 23:53:27,919 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 23:53:28,020 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.52% (3780 / 4176)
2021-01-09 23:53:28,020 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3783 / 4176)
2021-01-09 23:53:28,098 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 23:53:28,098 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 23:53:28,196 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3783 / 4176)
2021-01-09 23:53:28,196 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:53:28,264 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 23:53:28,265 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 23:53:28,364 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:53:28,364 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.61% (3784 / 4176)
2021-01-09 23:53:28,441 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 23:53:28,442 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 23:53:28,539 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.61% (3784 / 4176)
2021-01-09 23:53:28,539 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3786 / 4176)
2021-01-09 23:53:28,608 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 23:53:28,609 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 23:53:28,707 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.66% (3786 / 4176)
2021-01-09 23:53:28,707 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.68% (3787 / 4176)
2021-01-09 23:53:28,783 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 23:53:28,784 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
