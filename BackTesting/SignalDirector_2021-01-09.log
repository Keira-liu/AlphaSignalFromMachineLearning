2021-01-09 18:59:49,432 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 18:59:49,432 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 18:59:50,884 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 18:59:52,319 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 18:59:53,741 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 18:59:55,158 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 18:59:56,584 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 18:59:58,126 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 18:59:59,655 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:00:01,167 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:00:02,561 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:00:04,790 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:00:07,031 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:00:09,474 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:00:09,476 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:00:09,476 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:00:44,744 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:00:44,745 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:00:46,234 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:00:47,695 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:00:49,168 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:00:50,627 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:00:52,084 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:00:53,673 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:00:55,274 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:00:56,858 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:00:58,316 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:01:00,638 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:01:02,957 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:01:05,464 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:04:26,882 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:04:26,882 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:04:28,372 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:04:29,794 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:04:31,207 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:04:32,631 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:04:34,045 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:04:35,598 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:04:37,119 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:04:38,625 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:04:39,998 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:04:42,229 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:04:44,475 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:04:46,905 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:04:46,906 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:13:16,343 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:13:16,343 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:13:17,763 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:13:19,168 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:13:20,578 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:13:21,995 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:13:23,406 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:13:24,955 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:13:26,477 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:13:28,014 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:13:29,393 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:13:31,612 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:13:33,879 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:13:36,309 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:13:36,309 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:13:36,310 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:13:36,311 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:13:36,352 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:17:08,813 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:17:08,813 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:17:10,681 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:17:12,455 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:17:14,061 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:17:15,487 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:17:16,929 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:17:18,564 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:17:20,128 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:17:21,685 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:17:23,073 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:17:25,289 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:17:27,524 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:17:29,950 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:17:29,951 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:17:29,990 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:20:44,178 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:20:44,178 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:20:47,247 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:20:50,310 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:20:53,373 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:20:56,427 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:20:59,482 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:21:02,668 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:21:05,891 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:21:09,045 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:21:12,069 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:21:16,543 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:21:21,040 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:21:25,725 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:21:25,725 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:21:25,726 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:21:25,726 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:21:25,727 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:21:25,728 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:21:25,769 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:25:01,891 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:25:01,891 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:25:04,963 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:25:08,023 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:25:11,084 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:25:14,144 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:25:17,216 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:25:20,418 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:25:23,637 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:25:26,797 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:25:29,826 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:25:34,314 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:25:38,815 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:25:43,507 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:25:43,508 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:25:43,509 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:25:43,510 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:25:43,551 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:28:25,668 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:28:25,668 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:28:28,752 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:28:31,816 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:28:34,889 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:28:37,947 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:28:41,009 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:28:44,200 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:28:47,425 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:28:50,595 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:28:53,645 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:28:58,140 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:29:02,636 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:29:07,332 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:29:07,333 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:29:07,334 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:29:07,376 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:30:55,068 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:30:55,068 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:30:56,495 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:30:57,899 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:30:59,301 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:31:00,713 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:31:02,115 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:31:03,661 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:31:05,165 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:31:06,707 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:31:08,088 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:31:10,299 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:31:12,519 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:31:14,931 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:31:14,932 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:31:14,972 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:32:06,611 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:32:06,611 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:32:09,678 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:32:12,718 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:32:15,750 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:32:18,783 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:32:21,832 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:32:25,009 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:32:28,211 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:32:31,346 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:32:34,352 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:32:38,805 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:32:43,281 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:32:47,939 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:32:47,940 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:32:47,941 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : factor free_circulating_market_cap is loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:32:47,942 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:32:47,986 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:37:21,610 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:37:21,610 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:37:23,040 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:37:24,455 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:37:25,872 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:37:27,289 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:37:28,708 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:37:30,268 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:37:31,797 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:37:33,364 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:37:34,745 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:37:36,998 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:37:39,241 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:37:41,663 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:37:41,664 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:39:53,558 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:39:53,558 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:39:55,029 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:39:56,487 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:39:57,935 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:39:59,390 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:40:00,842 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:40:02,421 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:40:03,977 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:40:05,573 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:40:06,992 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:40:09,276 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:40:11,576 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:40:14,065 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:40:14,066 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:40:14,067 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:40:14,107 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:40:14,175 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:40:14,176 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,404 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:40:14,404 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:40:14,460 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,460 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:40:14,552 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:40:14,552 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:40:14,608 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:40:14,609 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:40:14,686 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:40:14,743 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,743 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:40:14,806 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:40:14,862 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:14,863 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:14,937 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:40:14,937 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:40:14,993 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:14,993 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,057 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:40:15,058 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:40:15,113 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,113 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,174 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:40:15,175 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:40:15,233 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:40:15,233 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,294 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:40:15,294 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:40:15,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,348 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,410 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:40:15,410 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:40:15,464 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:40:15,464 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:40:15,528 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:40:15,529 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:40:15,584 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:40:15,584 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:40:15,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:40:15,650 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:40:15,705 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:40:15,705 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:40:15,771 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:40:15,771 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:40:15,826 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:40:15,826 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:40:15,895 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:40:15,896 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:40:15,950 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:40:15,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:40:16,016 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:40:16,017 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:40:16,071 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:40:16,071 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:40:16,132 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:40:16,133 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:40:16,188 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:40:16,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:40:16,252 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:40:16,252 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:40:16,306 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:40:16,306 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:40:16,368 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:40:16,369 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:40:16,425 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:40:16,425 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,488 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:40:16,488 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:40:16,542 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,542 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,605 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:40:16,605 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:40:16,658 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:40:16,659 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:40:16,722 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:40:16,723 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:40:16,776 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:40:16,776 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:40:16,844 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:40:16,845 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:40:16,901 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:40:16,901 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:40:16,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:40:16,962 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:40:17,016 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:40:17,017 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:40:17,079 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:40:17,080 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:40:17,136 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:40:17,136 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:40:17,201 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:40:17,202 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:40:17,256 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:40:17,256 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:40:17,320 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:40:17,321 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:40:17,377 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:40:17,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:40:17,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:40:17,440 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:40:17,494 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:40:17,494 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,556 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:40:17,557 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:40:17,613 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,613 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,674 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:40:17,675 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:40:17,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:40:17,728 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:40:17,791 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:40:17,791 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:40:17,847 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:40:17,847 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:17,919 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:40:17,920 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:40:17,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:17,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,040 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:40:18,040 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:40:18,097 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,097 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,159 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:40:18,160 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:40:18,214 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:40:18,214 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:40:18,278 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:40:18,278 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:40:18,334 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:40:18,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,397 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:40:18,397 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:40:18,450 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,450 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,513 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:40:18,513 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:40:18,567 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:40:18,567 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:40:18,633 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:40:18,633 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:40:18,687 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:40:18,687 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:40:18,750 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:40:18,750 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:40:39,417 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:40:39,417 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:40:40,840 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:40:42,253 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:40:43,662 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:40:45,077 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:40:46,495 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:40:48,037 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:40:49,559 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:40:51,122 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:40:52,499 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:40:54,732 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:40:56,971 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:40:59,396 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:40:59,397 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:40:59,398 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:40:59,439 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:40:59,509 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:40:59,510 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,619 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:40:59,619 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:40:59,673 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,673 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:40:59,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:40:59,739 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:40:59,794 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:40:59,795 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,862 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:40:59,862 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:40:59,915 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,915 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:40:59,974 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:40:59,975 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:41:00,033 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:41:00,033 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,107 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:41:00,108 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:41:00,169 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,169 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,231 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:41:00,232 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:41:00,286 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,286 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,351 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:41:00,352 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:41:00,410 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:41:00,411 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,473 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:41:00,473 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:41:00,528 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,528 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,596 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:41:00,596 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:41:00,654 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:41:00,654 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:41:00,714 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:41:00,715 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:41:00,769 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:41:00,769 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:41:00,835 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:41:00,836 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:41:00,893 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:41:00,893 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:41:00,956 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:41:00,956 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:41:01,009 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:41:01,009 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:41:01,070 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:41:01,071 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:41:01,127 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:41:01,127 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:41:01,189 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:41:01,189 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:41:01,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:41:01,243 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:41:01,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:41:01,305 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:41:01,359 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:41:01,359 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:41:01,420 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:41:01,421 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:41:01,474 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:41:01,474 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:41:01,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:41:01,538 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:41:01,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:41:01,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,657 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:41:01,657 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:41:01,710 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,711 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,773 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:41:01,774 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:41:01,828 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:41:01,828 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:41:01,897 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:41:01,898 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:41:01,950 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:41:01,950 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:41:02,016 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:41:02,016 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:41:02,074 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:41:02,074 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:41:02,138 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:41:02,138 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:41:02,193 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:41:02,193 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:41:02,256 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:41:02,257 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:41:02,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:41:02,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:41:02,372 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:41:02,372 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:41:02,424 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:41:02,424 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:41:02,486 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:41:02,486 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:41:02,543 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:41:02,544 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:41:02,608 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:41:02,608 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:41:02,664 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:41:02,664 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:41:02,729 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:41:02,786 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,786 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,857 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:41:02,858 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:41:02,912 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:41:02,913 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:41:02,982 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:41:02,983 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:41:03,038 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:41:03,039 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,099 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:41:03,100 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:41:03,154 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,154 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,218 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:41:03,219 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:41:03,276 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,276 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,348 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:41:03,349 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:41:03,402 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:41:03,402 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:41:03,473 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:41:03,474 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:41:03,528 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:41:03,528 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,599 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:41:03,600 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:41:03,658 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,658 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,732 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:41:03,733 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:41:03,790 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:41:03,790 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:41:03,861 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:41:03,862 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:41:03,915 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:41:03,916 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:41:03,992 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:41:03,993 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:42:16,860 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:42:16,860 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:42:18,288 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:42:19,699 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:42:21,114 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:42:22,534 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:42:24,002 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:42:25,542 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:42:27,060 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:42:28,612 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:42:29,984 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:42:32,218 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:42:34,440 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:42:36,866 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:42:36,867 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:42:36,868 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:42:36,900 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:42:36,969 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:42:36,969 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,071 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:42:37,072 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:42:37,126 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,126 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:42:37,192 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:42:37,193 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:42:37,246 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:42:37,246 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,308 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:42:37,308 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:42:37,368 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,368 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,440 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:42:37,441 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:42:37,496 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:42:37,497 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,560 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:42:37,561 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:42:37,614 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,614 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,674 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:42:37,674 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:42:37,728 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,729 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,788 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:42:37,789 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:42:37,848 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:42:37,848 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:42:37,915 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:42:37,916 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:42:37,969 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:42:37,969 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:42:38,034 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:42:38,034 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:42:38,091 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:42:38,091 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:42:38,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:42:38,151 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:42:38,203 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:42:38,204 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:42:38,265 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:42:38,265 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:42:38,322 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:42:38,322 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:42:38,384 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:42:38,385 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:42:38,440 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:42:38,440 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:42:38,505 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:42:38,505 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:42:38,561 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:42:38,561 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:42:38,624 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:42:38,625 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:42:38,677 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:42:38,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:42:38,739 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:42:38,739 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:42:38,798 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:42:38,798 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:42:38,866 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:42:38,867 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:42:38,921 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:42:38,921 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:42:38,983 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:42:38,984 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:42:39,037 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:42:39,037 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,096 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:42:39,097 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:42:39,150 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,150 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,212 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:42:39,212 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:42:39,269 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:42:39,269 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:42:39,331 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:42:39,331 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:42:39,385 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:42:39,385 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:42:39,449 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:42:39,450 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:42:39,507 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:42:39,507 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:42:39,568 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:42:39,569 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:42:39,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:42:39,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:42:39,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:42:39,683 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:42:39,735 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:42:39,735 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:42:39,797 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:42:39,798 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:42:39,852 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:42:39,853 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:42:39,922 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:42:39,922 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:42:39,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:42:39,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:42:40,036 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:42:40,036 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:42:40,088 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:42:40,088 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,150 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:42:40,150 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:42:40,208 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,208 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,270 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:42:40,271 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:42:40,325 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:42:40,325 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:42:40,386 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:42:40,387 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:42:40,439 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:42:40,439 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,501 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:42:40,502 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:42:40,557 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,557 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,621 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:42:40,621 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:42:40,673 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,674 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,740 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:42:40,741 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:42:40,792 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:42:40,793 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:42:40,860 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:42:40,860 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:42:40,912 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:42:40,912 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:42:40,972 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:42:40,973 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:42:41,025 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,026 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,085 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:42:41,086 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:42:41,142 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:42:41,142 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:42:41,207 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:42:41,208 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:42:41,261 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:42:41,261 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:42:41,324 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:42:41,324 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:43:09,064 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:43:09,064 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:43:10,722 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:43:12,379 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:43:14,026 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:43:15,680 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:43:17,331 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:43:19,106 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:43:20,867 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:43:22,663 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:43:24,278 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:43:26,822 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:43:29,385 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:43:32,151 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:43:32,152 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:43:32,153 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:43:32,153 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:43:32,185 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:43:32,263 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:43:32,263 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,367 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:43:32,367 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:43:32,427 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,427 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:43:32,492 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:43:32,492 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:43:32,551 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:43:32,552 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,621 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:43:32,622 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:43:32,683 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,683 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,744 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:43:32,745 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:43:32,808 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:43:32,808 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:32,879 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:43:32,880 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:43:32,942 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:32,942 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,003 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:43:33,004 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:43:33,066 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,066 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,133 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:43:33,133 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:43:33,197 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:43:33,198 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,258 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:43:33,258 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:43:33,318 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,319 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,384 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:43:33,384 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:43:33,447 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:43:33,448 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:43:33,507 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:43:33,507 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:43:33,566 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:43:33,566 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:43:33,633 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:43:33,633 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:43:33,697 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:43:33,697 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:43:33,758 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:43:33,759 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:43:33,822 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:43:33,822 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:43:33,893 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:43:33,894 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:43:33,957 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:43:33,957 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:43:34,020 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:43:34,021 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:43:34,079 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:43:34,079 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:43:34,145 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:43:34,145 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:43:34,210 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:43:34,210 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:43:34,270 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:43:34,270 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:43:34,331 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:43:34,332 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:43:34,399 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:43:34,399 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:43:34,462 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:43:34,462 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,523 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:43:34,524 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:43:34,584 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,585 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,651 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:43:34,651 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:43:34,714 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:43:34,714 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:43:34,774 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:43:34,775 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:43:34,838 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:43:34,838 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:43:34,909 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:43:34,910 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:43:34,972 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:43:34,973 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:43:35,039 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:43:35,039 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:43:35,100 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:43:35,100 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:43:35,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:43:35,167 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:43:35,232 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:43:35,232 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:43:35,293 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:43:35,293 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:43:35,354 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:43:35,354 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:43:35,421 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:43:35,421 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:43:35,485 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:43:35,485 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:43:35,544 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:43:35,544 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:43:35,605 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:43:35,606 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,670 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:43:35,670 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:43:35,733 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,733 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,795 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:43:35,795 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:43:35,858 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:43:35,859 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:43:35,930 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:43:35,931 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:43:35,993 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:43:35,993 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,057 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:43:36,058 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:43:36,118 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,118 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,184 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:43:36,184 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:43:36,243 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,244 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,303 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:43:36,304 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:43:36,363 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:43:36,363 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:43:36,427 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:43:36,427 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:43:36,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:43:36,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,551 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:43:36,552 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:43:36,612 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,617 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:43:36,683 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:43:36,741 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:43:36,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:43:36,800 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:43:36,801 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:43:36,863 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:43:36,863 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:43:36,931 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:43:36,931 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:43:54,373 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:43:54,397 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:43:56,053 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:43:57,697 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:43:59,348 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:44:00,999 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:44:02,661 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:44:04,446 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:44:06,206 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:44:07,946 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:44:09,628 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:44:12,171 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:44:14,727 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:44:17,474 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:44:17,475 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:44:17,489 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:44:17,490 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:44:17,490 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:44:17,491 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:44:17,492 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:44:17,528 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:44:17,607 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:44:17,607 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,678 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:44:17,679 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:44:17,743 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,743 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:44:17,809 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:44:17,810 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:44:17,868 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:44:17,868 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:17,938 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:44:17,939 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:44:18,001 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,001 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,062 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:44:18,062 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:44:18,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:44:18,121 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:44:18,188 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:44:18,249 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,249 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,311 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:44:18,311 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:44:18,369 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,369 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,433 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:44:18,434 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:44:18,493 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:44:18,493 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,553 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:44:18,553 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:44:18,614 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,614 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,681 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:44:18,681 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:44:18,745 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:44:18,745 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:44:18,805 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:44:18,805 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:44:18,867 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:44:18,868 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:44:18,938 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:44:18,939 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:44:19,001 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:44:19,001 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:44:19,063 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:44:19,063 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:44:19,123 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:44:19,124 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:44:19,188 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:44:19,189 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:44:19,247 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:44:19,247 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:44:19,305 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:44:19,305 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:44:19,363 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:44:19,364 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:44:19,428 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:44:19,428 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:44:19,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:44:19,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:44:19,551 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:44:19,552 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:44:19,612 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:44:19,612 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:44:19,676 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:44:19,676 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:44:19,740 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:44:19,740 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,801 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:44:19,802 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:44:19,865 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,865 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,936 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:44:19,936 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:44:19,999 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:44:19,999 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:44:20,061 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:44:20,061 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:44:20,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:44:20,121 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:44:20,187 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:44:20,187 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:44:20,251 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:44:20,251 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:44:20,311 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:44:20,312 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:44:20,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:44:20,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:44:20,437 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:44:20,438 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:44:20,498 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:44:20,498 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:44:20,558 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:44:20,559 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:44:20,619 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:44:20,619 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:44:20,684 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:44:20,684 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:44:20,747 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:44:20,747 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:44:20,808 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:44:20,808 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:44:20,871 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:44:20,871 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:44:20,944 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:44:20,944 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:44:21,008 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,008 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:44:21,069 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:44:21,131 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:44:21,131 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:44:21,196 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:44:21,197 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:44:21,255 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:44:21,255 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,314 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:44:21,314 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:44:21,372 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,372 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,436 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:44:21,437 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:44:21,500 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,500 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:44:21,563 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:44:21,621 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:44:21,621 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:44:21,685 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:44:21,686 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:44:21,746 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:44:21,746 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,806 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:44:21,806 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:44:21,869 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,869 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,939 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:44:21,940 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:44:21,999 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:44:21,999 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:44:22,061 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:44:22,062 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:44:22,120 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:44:22,120 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:44:22,185 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:44:22,185 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:44:44,037 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:44:44,037 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:44:45,476 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:44:46,901 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:44:48,320 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:44:49,745 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:44:51,161 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:44:52,725 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:44:54,255 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:44:55,818 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:44:57,212 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:44:59,444 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:45:01,690 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:45:04,126 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:45:04,127 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:45:04,128 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:45:04,160 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:45:04,229 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:45:04,229 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,334 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:45:04,334 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:45:04,390 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,390 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:45:04,462 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:45:04,463 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:45:04,518 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:45:04,519 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,584 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:45:04,585 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:45:04,643 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,643 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:45:04,703 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:45:04,758 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:45:04,758 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,822 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:45:04,822 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:45:04,877 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,877 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:04,946 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:45:04,947 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:45:05,002 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,002 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,067 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:45:05,067 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:45:05,121 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:45:05,122 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,184 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:45:05,184 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:45:05,238 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,238 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,301 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:45:05,301 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:45:05,358 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:45:05,358 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:45:05,419 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:45:05,419 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:45:05,474 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:45:05,474 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:45:05,537 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:45:05,538 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:45:05,594 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:45:05,594 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:45:05,654 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:45:05,654 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:45:05,709 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:45:05,709 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:45:05,773 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:45:05,774 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:45:05,827 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:45:05,827 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:45:05,897 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:45:05,898 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:45:05,953 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:45:05,954 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:45:06,019 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:45:06,019 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:45:06,076 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:45:06,076 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:45:06,137 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:45:06,138 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:45:06,193 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:45:06,193 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:45:06,255 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:45:06,256 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:45:06,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:45:06,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,371 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:45:06,372 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:45:06,426 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,426 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,489 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:45:06,489 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:45:06,544 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:45:06,544 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:45:06,612 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:45:06,612 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:45:06,666 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:45:06,666 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:45:06,729 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:45:06,730 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:45:06,785 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:45:06,786 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:45:06,846 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:45:06,846 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:45:06,901 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:45:06,902 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:45:06,972 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:45:06,972 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:45:07,030 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:45:07,031 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:45:07,090 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:45:07,091 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:45:07,144 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:45:07,144 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:45:07,207 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:45:07,208 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:45:07,261 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:45:07,261 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:45:07,324 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:45:07,324 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:45:07,377 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:45:07,377 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:45:07,440 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:45:07,494 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,494 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,557 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:45:07,557 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:45:07,610 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:45:07,611 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:45:07,671 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:45:07,672 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:45:07,727 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:45:07,727 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,790 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:45:07,790 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:45:07,844 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,844 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,914 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:45:07,914 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:45:07,973 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:07,974 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:45:08,042 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:45:08,042 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:45:08,096 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:45:08,096 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:45:08,160 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:45:08,161 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:45:08,218 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:45:08,218 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,281 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:45:08,281 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:45:08,335 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,335 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,396 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:45:08,397 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:45:08,453 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:45:08,453 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:45:08,517 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:45:08,517 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:45:08,570 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:45:08,570 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:45:08,634 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:45:08,635 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 19:59:03,092 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 19:59:03,093 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 19:59:04,535 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 19:59:06,185 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 19:59:07,615 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 19:59:09,045 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 19:59:10,764 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 19:59:12,457 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 19:59:14,240 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 19:59:15,752 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 19:59:17,145 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 19:59:19,382 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 19:59:21,632 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 19:59:24,061 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 19:59:24,062 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 19:59:24,063 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 19:59:24,064 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 19:59:24,065 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 19:59:24,067 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 19:59:24,069 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 19:59:24,070 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 19:59:24,070 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 19:59:24,071 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 19:59:24,104 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 19:59:24,188 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 19:59:24,188 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,287 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 19:59:24,289 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 19:59:24,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,348 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 19:59:24,407 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 19:59:24,409 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 19:59:24,467 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 19:59:24,467 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,525 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 19:59:24,526 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 19:59:24,585 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,585 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,646 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 19:59:24,648 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 19:59:24,705 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 19:59:24,705 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,764 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 19:59:24,765 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 19:59:24,823 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,823 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,883 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 19:59:24,884 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 19:59:24,942 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:24,942 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 19:59:25,002 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 19:59:25,004 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 19:59:25,063 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 19:59:25,064 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,129 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 19:59:25,131 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 19:59:25,190 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,190 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,250 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 19:59:25,251 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 19:59:25,310 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 19:59:25,310 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 19:59:25,369 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 19:59:25,371 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 19:59:25,428 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 19:59:25,428 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 19:59:25,488 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 19:59:25,490 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 19:59:25,547 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 19:59:25,547 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 19:59:25,610 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 19:59:25,612 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 19:59:25,669 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 19:59:25,670 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 19:59:25,728 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 19:59:25,730 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 19:59:25,788 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 19:59:25,788 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 19:59:25,847 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 19:59:25,848 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 19:59:25,906 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 19:59:25,907 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 19:59:25,965 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 19:59:25,966 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 19:59:26,025 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 19:59:26,025 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 19:59:26,092 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 19:59:26,093 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 19:59:26,152 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 19:59:26,152 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 19:59:26,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 19:59:26,216 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 19:59:26,272 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 19:59:26,273 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,330 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 19:59:26,331 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 19:59:26,389 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,389 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,448 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 19:59:26,449 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 19:59:26,506 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 19:59:26,507 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 19:59:26,573 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 19:59:26,575 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 19:59:26,637 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 19:59:26,637 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 19:59:26,696 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 19:59:26,697 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 19:59:26,754 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 19:59:26,755 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 19:59:26,822 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 19:59:26,823 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 19:59:26,881 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 19:59:26,881 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 19:59:26,946 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 19:59:26,947 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 19:59:27,006 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 19:59:27,006 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 19:59:27,074 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 19:59:27,076 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 19:59:27,134 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 19:59:27,134 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 19:59:27,198 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 19:59:27,200 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 19:59:27,257 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 19:59:27,258 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 19:59:27,317 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 19:59:27,318 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 19:59:27,375 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 19:59:27,375 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,433 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 19:59:27,434 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 19:59:27,491 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,491 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,552 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 19:59:27,554 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 19:59:27,613 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 19:59:27,613 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 19:59:27,671 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 19:59:27,673 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 19:59:27,730 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 19:59:27,730 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,790 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 19:59:27,791 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 19:59:27,849 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,849 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,909 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 19:59:27,910 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 19:59:27,967 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:27,967 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 19:59:28,031 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 19:59:28,033 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 19:59:28,091 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 19:59:28,091 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 19:59:28,153 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 19:59:28,154 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 19:59:28,214 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 19:59:28,214 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,274 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 19:59:28,275 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 19:59:28,333 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,333 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,392 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 19:59:28,393 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 19:59:28,451 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 19:59:28,451 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 19:59:28,510 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 19:59:28,511 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 19:59:28,568 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 19:59:28,568 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 19:59:28,632 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 19:59:28,633 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
2021-01-09 20:00:45,607 logger.py[line:40] - INFO - root : globalVars is initialized
2021-01-09 20:00:45,608 logger.py[line:40] - INFO - root : materialData:{} is now in global
2021-01-09 20:00:47,142 logger.py[line:40] - INFO - root : close is now in globalVars.materialData
2021-01-09 20:00:48,588 logger.py[line:40] - INFO - root : high is now in globalVars.materialData
2021-01-09 20:00:50,036 logger.py[line:40] - INFO - root : low is now in globalVars.materialData
2021-01-09 20:00:51,479 logger.py[line:40] - INFO - root : open is now in globalVars.materialData
2021-01-09 20:00:52,941 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData
2021-01-09 20:00:54,517 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData
2021-01-09 20:00:56,068 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData
2021-01-09 20:00:57,618 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData
2021-01-09 20:00:59,028 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData
2021-01-09 20:01:01,288 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData
2021-01-09 20:01:03,553 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData
2021-01-09 20:01:06,021 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData
2021-01-09 20:01:06,022 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded
2021-01-09 20:01:06,023 logger.py[line:40] - INFO - root : factors:{} is now in global
2021-01-09 20:01:06,025 logger.py[line:40] - INFO - root : factor close is loaded
2021-01-09 20:01:06,026 logger.py[line:40] - INFO - root : factor amount is loaded
2021-01-09 20:01:06,028 logger.py[line:40] - INFO - root : factor open is loaded
2021-01-09 20:01:06,029 logger.py[line:40] - INFO - root : factor high is loaded
2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : factor low is loaded
2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : all factors are loaded
2021-01-09 20:01:06,032 logger.py[line:40] - INFO - root : start to generate signalGenerator
2021-01-09 20:01:06,066 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00
2021-01-09 20:01:06,149 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.73% (3748 / 4177)
2021-01-09 20:01:06,149 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,254 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668
2021-01-09 20:01:06,256 logger.py[line:37] - DEBUG - root : 2020-01-02 00:00:00 finished
2021-01-09 20:01:06,315 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,315 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.80% (3751 / 4177)
2021-01-09 20:01:06,374 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303
2021-01-09 20:01:06,375 logger.py[line:37] - DEBUG - root : 2020-01-03 00:00:00 finished
2021-01-09 20:01:06,436 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.80% (3751 / 4177)
2021-01-09 20:01:06,436 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,495 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877
2021-01-09 20:01:06,497 logger.py[line:37] - DEBUG - root : 2020-01-06 00:00:00 finished
2021-01-09 20:01:06,556 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,556 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,617 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675
2021-01-09 20:01:06,619 logger.py[line:37] - DEBUG - root : 2020-01-07 00:00:00 finished
2021-01-09 20:01:06,678 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.78% (3750 / 4177)
2021-01-09 20:01:06,678 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402
2021-01-09 20:01:06,739 logger.py[line:37] - DEBUG - root : 2020-01-08 00:00:00 finished
2021-01-09 20:01:06,797 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,797 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,858 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835
2021-01-09 20:01:06,860 logger.py[line:37] - DEBUG - root : 2020-01-09 00:00:00 finished
2021-01-09 20:01:06,919 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,920 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.85% (3753 / 4177)
2021-01-09 20:01:06,980 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645
2021-01-09 20:01:06,982 logger.py[line:37] - DEBUG - root : 2020-01-10 00:00:00 finished
2021-01-09 20:01:07,042 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.85% (3753 / 4177)
2021-01-09 20:01:07,042 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,111 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821
2021-01-09 20:01:07,113 logger.py[line:37] - DEBUG - root : 2020-01-13 00:00:00 finished
2021-01-09 20:01:07,171 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,171 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,239 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985
2021-01-09 20:01:07,241 logger.py[line:37] - DEBUG - root : 2020-01-14 00:00:00 finished
2021-01-09 20:01:07,298 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.87% (3754 / 4177)
2021-01-09 20:01:07,298 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.90% (3755 / 4177)
2021-01-09 20:01:07,357 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576
2021-01-09 20:01:07,359 logger.py[line:37] - DEBUG - root : 2020-01-15 00:00:00 finished
2021-01-09 20:01:07,419 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.90% (3755 / 4177)
2021-01-09 20:01:07,420 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.94% (3757 / 4177)
2021-01-09 20:01:07,480 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195
2021-01-09 20:01:07,482 logger.py[line:37] - DEBUG - root : 2020-01-16 00:00:00 finished
2021-01-09 20:01:07,540 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.94% (3757 / 4177)
2021-01-09 20:01:07,540 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.97% (3758 / 4177)
2021-01-09 20:01:07,602 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185
2021-01-09 20:01:07,603 logger.py[line:37] - DEBUG - root : 2020-01-17 00:00:00 finished
2021-01-09 20:01:07,661 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.97% (3758 / 4177)
2021-01-09 20:01:07,661 logger.py[line:37] - DEBUG - root : Actual available testing data account for 89.99% (3759 / 4177)
2021-01-09 20:01:07,720 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785
2021-01-09 20:01:07,722 logger.py[line:37] - DEBUG - root : 2020-01-20 00:00:00 finished
2021-01-09 20:01:07,779 logger.py[line:37] - DEBUG - root : Actual available training data account for 89.99% (3759 / 4177)
2021-01-09 20:01:07,779 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.02% (3760 / 4177)
2021-01-09 20:01:07,838 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582
2021-01-09 20:01:07,839 logger.py[line:37] - DEBUG - root : 2020-01-21 00:00:00 finished
2021-01-09 20:01:07,897 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.02% (3760 / 4177)
2021-01-09 20:01:07,898 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.04% (3761 / 4177)
2021-01-09 20:01:07,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474
2021-01-09 20:01:07,963 logger.py[line:37] - DEBUG - root : 2020-01-22 00:00:00 finished
2021-01-09 20:01:08,023 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.04% (3761 / 4177)
2021-01-09 20:01:08,023 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.06% (3762 / 4177)
2021-01-09 20:01:08,091 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504
2021-01-09 20:01:08,093 logger.py[line:37] - DEBUG - root : 2020-01-23 00:00:00 finished
2021-01-09 20:01:08,151 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.06% (3762 / 4177)
2021-01-09 20:01:08,151 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.09% (3763 / 4177)
2021-01-09 20:01:08,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248
2021-01-09 20:01:08,217 logger.py[line:37] - DEBUG - root : 2020-02-03 00:00:00 finished
2021-01-09 20:01:08,275 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.09% (3763 / 4177)
2021-01-09 20:01:08,275 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,341 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775
2021-01-09 20:01:08,342 logger.py[line:37] - DEBUG - root : 2020-02-04 00:00:00 finished
2021-01-09 20:01:08,401 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,401 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,459 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524
2021-01-09 20:01:08,460 logger.py[line:37] - DEBUG - root : 2020-02-05 00:00:00 finished
2021-01-09 20:01:08,519 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.14% (3765 / 4177)
2021-01-09 20:01:08,520 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.16% (3766 / 4177)
2021-01-09 20:01:08,580 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614
2021-01-09 20:01:08,581 logger.py[line:37] - DEBUG - root : 2020-02-06 00:00:00 finished
2021-01-09 20:01:08,640 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.16% (3766 / 4177)
2021-01-09 20:01:08,640 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.21% (3768 / 4177)
2021-01-09 20:01:08,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407
2021-01-09 20:01:08,705 logger.py[line:37] - DEBUG - root : 2020-02-07 00:00:00 finished
2021-01-09 20:01:08,763 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.21% (3768 / 4177)
2021-01-09 20:01:08,763 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.23% (3769 / 4177)
2021-01-09 20:01:08,824 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887
2021-01-09 20:01:08,825 logger.py[line:37] - DEBUG - root : 2020-02-10 00:00:00 finished
2021-01-09 20:01:08,882 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.23% (3769 / 4177)
2021-01-09 20:01:08,882 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.26% (3770 / 4177)
2021-01-09 20:01:08,939 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593
2021-01-09 20:01:08,941 logger.py[line:37] - DEBUG - root : 2020-02-11 00:00:00 finished
2021-01-09 20:01:08,998 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.26% (3770 / 4177)
2021-01-09 20:01:08,998 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.28% (3771 / 4177)
2021-01-09 20:01:09,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201
2021-01-09 20:01:09,071 logger.py[line:37] - DEBUG - root : 2020-02-12 00:00:00 finished
2021-01-09 20:01:09,129 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.28% (3771 / 4177)
2021-01-09 20:01:09,129 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.33% (3773 / 4177)
2021-01-09 20:01:09,202 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761
2021-01-09 20:01:09,203 logger.py[line:37] - DEBUG - root : 2020-02-13 00:00:00 finished
2021-01-09 20:01:09,260 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.33% (3773 / 4177)
2021-01-09 20:01:09,260 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.38% (3775 / 4177)
2021-01-09 20:01:09,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661
2021-01-09 20:01:09,322 logger.py[line:37] - DEBUG - root : 2020-02-14 00:00:00 finished
2021-01-09 20:01:09,379 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.38% (3775 / 4177)
2021-01-09 20:01:09,379 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587
2021-01-09 20:01:09,441 logger.py[line:37] - DEBUG - root : 2020-02-17 00:00:00 finished
2021-01-09 20:01:09,498 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,499 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176
2021-01-09 20:01:09,563 logger.py[line:37] - DEBUG - root : 2020-02-18 00:00:00 finished
2021-01-09 20:01:09,622 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.42% (3777 / 4177)
2021-01-09 20:01:09,622 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.45% (3778 / 4177)
2021-01-09 20:01:09,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687
2021-01-09 20:01:09,684 logger.py[line:37] - DEBUG - root : 2020-02-19 00:00:00 finished
2021-01-09 20:01:09,741 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.45% (3778 / 4177)
2021-01-09 20:01:09,741 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,799 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759
2021-01-09 20:01:09,801 logger.py[line:37] - DEBUG - root : 2020-02-20 00:00:00 finished
2021-01-09 20:01:09,859 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,860 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128
2021-01-09 20:01:09,925 logger.py[line:37] - DEBUG - root : 2020-02-21 00:00:00 finished
2021-01-09 20:01:09,982 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:09,982 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.50% (3780 / 4177)
2021-01-09 20:01:10,049 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367
2021-01-09 20:01:10,051 logger.py[line:37] - DEBUG - root : 2020-02-24 00:00:00 finished
2021-01-09 20:01:10,108 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.50% (3780 / 4177)
2021-01-09 20:01:10,108 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.57% (3783 / 4177)
2021-01-09 20:01:10,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522
2021-01-09 20:01:10,169 logger.py[line:37] - DEBUG - root : 2020-02-25 00:00:00 finished
2021-01-09 20:01:10,228 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.57% (3783 / 4177)
2021-01-09 20:01:10,228 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,287 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598
2021-01-09 20:01:10,288 logger.py[line:37] - DEBUG - root : 2020-02-26 00:00:00 finished
2021-01-09 20:01:10,348 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,349 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,412 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447
2021-01-09 20:01:10,413 logger.py[line:37] - DEBUG - root : 2020-02-27 00:00:00 finished
2021-01-09 20:01:10,471 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.59% (3784 / 4177)
2021-01-09 20:01:10,471 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.64% (3786 / 4177)
2021-01-09 20:01:10,531 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307
2021-01-09 20:01:10,532 logger.py[line:37] - DEBUG - root : 2020-02-28 00:00:00 finished
2021-01-09 20:01:10,588 logger.py[line:37] - DEBUG - root : Actual available training data account for 90.64% (3786 / 4177)
2021-01-09 20:01:10,589 logger.py[line:37] - DEBUG - root : Actual available testing data account for 90.66% (3787 / 4177)
2021-01-09 20:01:10,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439
2021-01-09 20:01:10,651 logger.py[line:37] - DEBUG - root : 2020-03-02 00:00:00 finished
