{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align:center; margin: 20px 0;\">\n",
       "<p style=\"color:#FA5882;text-align:center; margin: 10px 0 20px 0;\">\n",
       "\n",
       "</p>\n",
       "<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">PE-TTM PB-TTM因子双因子分析</button>\n",
       "<hr/>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"\"\"\n",
    "<div style=\"text-align:center; margin: 20px 0;\">\n",
    "<p style=\"color:#FA5882;text-align:center; margin: 10px 0 20px 0;\">\n",
    "\n",
    "</p>\n",
    "<button onclick=\"$('.input, .prompt, .output_stderr, .output_error').toggle();\">PE-TTM PB-TTM因子双因子分析</button>\n",
    "<hr/>\n",
    "</div>\n",
    "\"\"\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\soft\\Anaconda3\\envs\\qta_alpha_signal\\lib\\site-packages\\pyfolio\\pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyfolio as pf\n",
    "from pyfolio import plotting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "project_path = os.path.abspath(os.path.dirname(os.getcwd())+os.path.sep+\".\")\n",
    "sys.path.append(project_path)\n",
    "from FactorAnalyse import FactorAnalyserBase\n",
    "from DataPreProcessing import *\n",
    "\n",
    "figsize=(20, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from Tool.logger import Logger\n",
    "from Tool.DataPreProcessing import DeExtremeMethod, ImputeMethod, StandardizeMethod\n",
    "from BackTesting.Signal.SignalSynthesis import SignalSynthesis\n",
    "from BackTesting.systhesisDirector import SignalDirector\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "logger = Logger(\"SignalDirector\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "params = {\n",
    "    \"startDate\": pd.to_datetime('20200101'),\n",
    "    \"endDate\": pd.to_datetime('20200301'),\n",
    "    \"panelSize\": 3,\n",
    "    \"trainTestGap\": 1,\n",
    "    \"maskList\": None,\n",
    "    \"deExtremeMethod\": DeExtremeMethod.MeanStd(),\n",
    "    \"imputeMethod\": ImputeMethod.JustMask(),\n",
    "    \"standardizeMethod\": StandardizeMethod.StandardScaler(),\n",
    "    \"pipeline\": None,\n",
    "    \"factorNameList\": ['close', 'amount', 'open', 'high', 'low'],\n",
    "    # params for XGBoost\n",
    "    \"modelParams\": {\n",
    "        \"jsonPath\": None,\n",
    "        \"paraDict\": {\n",
    "            \"n_estimators\": 50,\n",
    "            \"random_state\": 42,\n",
    "            \"max_depth\": 2}\n",
    "    },\n",
    "    # metric function for machine learning models\n",
    "    \"metric_func\": mean_squared_error,\n",
    "    # smoothing params\n",
    "    \"periods\": 10,\n",
    "    # smoothing的时候用的方式\n",
    "    \"method\": \"linear\"\n",
    "}\n",
    "\n",
    "director = SignalDirector(SignalSynthesis, params=params, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个简单的demo策略。\n",
    "#### 去掉以下条件的股票：\n",
    "1. PE-TTM<=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleStrategy(FactorAnalyserBase):\n",
    "    def __init__(self, start_date, end_date, benchmark_id, universe, props):\n",
    "        super(SampleStrategy, self).__init__(director, start_date, end_date, benchmark_id,\n",
    "                                             universe, props)\n",
    "\n",
    "    def filter(self):\n",
    "        for date, df in self.raw_universe_df_dict.items():\n",
    "            df['上市天数'] = (date-df['listed_date']).dt.days + 1  # 自然日\n",
    "            self.set_filter((~df['is_st']) & (df['上市天数'] > 180) & df['is_exist'], date)\n",
    "\n",
    "    def rate_stock(self):\n",
    "        \"\"\"\n",
    "        选股逻辑，去极值、中性化、标准化等。需要用户自己定义\n",
    "        \"\"\"\n",
    "        for date, df in self.processed_universe_df_dict.items():\n",
    "            score = self.signals.loc[date, df['code'].to_list()]\n",
    "            self.set_score(score, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PETTM_PBTTM_Strategy类benchmark参数\n",
    "1. 可选'000300.SH', '000905.SH' \n",
    "\n",
    "#### PETTM_PBTTM_Strategy类universe参数\n",
    "1. 可选'全A'、'沪深300'、'中证500'\n",
    "\n",
    "#### PETTM_PBTTM_Strategy类props参数：\n",
    "1. 不另行设置时默认值为：props = {'单边交易费率': 0.0015, '换仓日期模式': '月初换', '自定义换仓日期': None, '自定义篮子': None}  \n",
    "2. 换仓日期模式可选 月初换，月末换和自定义\n",
    "3. 当换仓日期模式选择了自定义，需要同时向自定义换仓日期传入换仓日期list\n",
    "4. 自定义篮子尚不支持，占坑。  \n",
    "\n",
    "#### grouping_test方法介绍\n",
    "\n",
    "分组收益分析，基于Score分组，由大到小分别是Q1、Q2...Q group_num，快速回测  \n",
    "        :param group_num: 分组数量  \n",
    "        :param control_dict: 需要控制的因子，若不为空必须是OrderDict，需要控制的因子名称为key，分组数量为value，\n",
    "                              非数字型因子，如行业分类，则value为空字符串 ： \"\"。将按顺序依次控制分组。\n",
    "                              例如：OrderedDict([('申万一级行业', ''), ('流通市值', 3)])  \n",
    "        :param group_by_benchmark: 是否按基准的因子去划分  \n",
    "        :param weight_method:配权方法，目前只支持EW、LVW和VW，分别是等权、流通市值平方根加权，总市值平方根加权  \n",
    "        :param max_stock_num 每个分组网格内最大的持股数  \n",
    "        :return: result_dict:包含4个部分。分组回测的净值、分组回测每个持有周期的IC、分组回测每次的换手率、\n",
    "                                            控制变量下每组的股票数量。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 20:00:45,607 logger.py[line:40] - INFO - root : globalVars is initialized\n",
      "2021-01-09 20:00:45,608 logger.py[line:40] - INFO - root : materialData:{} is now in global\n",
      "2021-01-09 20:00:47,142 logger.py[line:40] - INFO - root : close is now in globalVars.materialData\n",
      "2021-01-09 20:00:48,588 logger.py[line:40] - INFO - root : high is now in globalVars.materialData\n",
      "2021-01-09 20:00:50,036 logger.py[line:40] - INFO - root : low is now in globalVars.materialData\n",
      "2021-01-09 20:00:51,479 logger.py[line:40] - INFO - root : open is now in globalVars.materialData\n",
      "2021-01-09 20:00:52,941 logger.py[line:40] - INFO - root : preclose is now in globalVars.materialData\n",
      "2021-01-09 20:00:54,517 logger.py[line:40] - INFO - root : amount is now in globalVars.materialData\n",
      "2021-01-09 20:00:56,068 logger.py[line:40] - INFO - root : volume is now in globalVars.materialData\n",
      "2021-01-09 20:00:57,618 logger.py[line:40] - INFO - root : pctChange is now in globalVars.materialData\n",
      "2021-01-09 20:00:59,028 logger.py[line:40] - INFO - root : is_trading is now in globalVars.materialData\n",
      "2021-01-09 20:01:01,288 logger.py[line:40] - INFO - root : market_cap is now in globalVars.materialData\n",
      "2021-01-09 20:01:03,553 logger.py[line:40] - INFO - root : circulating_market_cap is now in globalVars.materialData\n",
      "2021-01-09 20:01:06,021 logger.py[line:40] - INFO - root : free_circulating_market_cap is now in globalVars.materialData\n",
      "2021-01-09 20:01:06,022 logger.py[line:40] - INFO - root : material data ['close', 'high', 'low', 'open', 'preclose', 'amount', 'volume', 'pctChange', 'is_trading', 'market_cap', 'circulating_market_cap', 'free_circulating_market_cap'] is loaded\n",
      "2021-01-09 20:01:06,023 logger.py[line:40] - INFO - root : factors:{} is now in global\n",
      "2021-01-09 20:01:06,025 logger.py[line:40] - INFO - root : factor close is loaded\n",
      "2021-01-09 20:01:06,026 logger.py[line:40] - INFO - root : factor amount is loaded\n",
      "2021-01-09 20:01:06,028 logger.py[line:40] - INFO - root : factor open is loaded\n",
      "2021-01-09 20:01:06,029 logger.py[line:40] - INFO - root : factor high is loaded\n",
      "2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : factor low is loaded\n",
      "2021-01-09 20:01:06,031 logger.py[line:40] - INFO - root : all factors are loaded\n",
      "2021-01-09 20:01:06,032 logger.py[line:40] - INFO - root : start to generate signalGenerator\n",
      "2021-01-09 20:01:06,066 logger.py[line:40] - INFO - root : start to generate signals from 2020-01-01 00:00:00 ot 2020-03-02 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorProfileBase __init__\n",
      "close is now in globalVars.factors\n",
      "FactorProfileBase __init__\n",
      "amount is now in globalVars.factors\n",
      "FactorProfileBase __init__\n",
      "open is now in globalVars.factors\n",
      "FactorProfileBase __init__\n",
      "high is now in globalVars.factors\n",
      "FactorProfileBase __init__\n",
      "low is now in globalVars.factors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e97147357e84d4fb2c7bbb583e03b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 20:01:06,254 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.344184638015829, testing loss: 6.751261404403668\n",
      "2021-01-09 20:01:06,374 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.298597817373551, testing loss: 6.299000032860303\n",
      "2021-01-09 20:01:06,495 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.965392827232416, testing loss: 5.980822136293877\n",
      "2021-01-09 20:01:06,617 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.253683477794484, testing loss: 14.316033910667675\n",
      "2021-01-09 20:01:06,738 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.405289051724763, testing loss: 14.667731681818402\n",
      "2021-01-09 20:01:06,858 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9436693262928086, testing loss: 8.491550311744835\n",
      "2021-01-09 20:01:06,980 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.037649747487158, testing loss: 6.5824335953881645\n",
      "2021-01-09 20:01:07,111 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.9523961753504397, testing loss: 6.799186709349821\n",
      "2021-01-09 20:01:07,239 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.365125992644804, testing loss: 5.4473693570945985\n",
      "2021-01-09 20:01:07,357 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 3.930411090031863, testing loss: 5.099598101442576\n",
      "2021-01-09 20:01:07,480 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.2114189770312835, testing loss: 5.864109861620195\n",
      "2021-01-09 20:01:07,602 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.718409232703778, testing loss: 8.592880584569185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 20:01:07,720 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.161248368140615, testing loss: 12.609092469760785\n",
      "2021-01-09 20:01:07,838 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.422946773153474, testing loss: 11.015002087179582\n",
      "2021-01-09 20:01:07,961 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.882923511894422, testing loss: 22.089059984396474\n",
      "2021-01-09 20:01:08,091 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.500552898090847, testing loss: 46.69022983348504\n",
      "2021-01-09 20:01:08,215 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 12.1431673895238, testing loss: 81.7034618978248\n",
      "2021-01-09 20:01:08,341 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.206406588901967, testing loss: 18.666477725000775\n",
      "2021-01-09 20:01:08,459 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.492906400761451, testing loss: 9.9541504826524\n",
      "2021-01-09 20:01:08,580 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.604906429943358, testing loss: 13.746280231194614\n",
      "2021-01-09 20:01:08,703 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.994324164389637, testing loss: 11.534633823177407\n",
      "2021-01-09 20:01:08,824 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.574437090748065, testing loss: 13.292941641495887\n",
      "2021-01-09 20:01:08,939 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.267574687404167, testing loss: 11.898029476286593\n",
      "2021-01-09 20:01:09,069 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.705209075427795, testing loss: 13.686328230014201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 20:01:09,202 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.715403864769744, testing loss: 7.302932730663761\n",
      "2021-01-09 20:01:09,321 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.982530203124146, testing loss: 16.21113373221661\n",
      "2021-01-09 20:01:09,439 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 4.941299600718445, testing loss: 10.509253413352587\n",
      "2021-01-09 20:01:09,562 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 6.217091215352629, testing loss: 15.106486605378176\n",
      "2021-01-09 20:01:09,682 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 7.751265241684819, testing loss: 12.773517509259687\n",
      "2021-01-09 20:01:09,799 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.368411072205198, testing loss: 7.938640366811759\n",
      "2021-01-09 20:01:09,924 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.528056669413347, testing loss: 10.200062660456128\n",
      "2021-01-09 20:01:10,049 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.27786125755347, testing loss: 12.969578670585367\n",
      "2021-01-09 20:01:10,167 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.126702640647462, testing loss: 21.869883665924522\n",
      "2021-01-09 20:01:10,287 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 9.99673802952975, testing loss: 14.72255188807598\n",
      "2021-01-09 20:01:10,412 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 8.119855103661493, testing loss: 44.96261638766447\n",
      "2021-01-09 20:01:10,531 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 10.382117728925317, testing loss: 95.23307948633307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 20:01:10,649 logger.py[line:40] - INFO - root : Model XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=50, n_jobs=16, num_parallel_tree=1, random_state=42,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None) training loss: 5.985344152867381, testing loss: 14.544009387587439\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0ddb11031385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m fab = SampleStrategy(params['startDate'], params['endDate'], '000300.SH', '沪深300',\n\u001b[0;32m      2\u001b[0m                            {\"换仓日期模式\":\"每日换\"})\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrate_stock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\QTA\\AlphaSignalFromMachineLearning\\BackTesting\\FactorAnalyse.py\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;31m# 按日期记录因子数据到dict里\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_universe_df_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWXDBReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_universe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[1;31m# 按日期记录指数权重数据到dict里\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\QTA\\AlphaSignalFromMachineLearning\\GetData\\backtestDataApi.py\u001b[0m in \u001b[0;36mget_universe\u001b[1;34m(cls, bk, date, factor_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# 获取最近交易日的日期，自动过滤非交易日\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nearest_trading_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatabaseReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stock_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# 根据 stock_code_list 获取指定日期的因子数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"沪深300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hs300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"000300.SH\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\QTA\\AlphaSignalFromMachineLearning\\GetData\\backtestDatabase.py\u001b[0m in \u001b[0;36mget_stock_info\u001b[1;34m(cls, code_list, start_date, end_date, field_list)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mfield_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfixed_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_daily_factor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mtemp_fileds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sec_name,ipo_date,delist_date,industry_sw,industry_citic\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\QTA\\AlphaSignalFromMachineLearning\\GetData\\backtestDatabase.py\u001b[0m in \u001b[0;36mget_daily_factor\u001b[1;34m(cls, code_list, factor_list, start_date, end_date)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mdb_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mdb_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mdb_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\soft\\Anaconda3\\envs\\qta_alpha_signal\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3486\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3487\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\soft\\Anaconda3\\envs\\qta_alpha_signal\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3564\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3565\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\soft\\Anaconda3\\envs\\qta_alpha_signal\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3748\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3749\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3751\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\soft\\Anaconda3\\envs\\qta_alpha_signal\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of values does not match length of index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "fab = SampleStrategy(params['startDate'], params['endDate'], '000300.SH', '沪深300',\n",
    "                           {\"换仓日期模式\":\"每日换\"})\n",
    "fab.prepare_data()\n",
    "fab.filter()\n",
    "fab.rate_stock()\n",
    "\n",
    "start_ = datetime.datetime.now()\n",
    "result = fab.grouping_test(5, OrderedDict([('industry_zx1_name', ''), ('circulating_market_cap', 5)]),\n",
    "                               group_by_benchmark=True, weight_method='LVW')\n",
    "print(datetime.datetime.now()-start_)\n",
    "result = fab.grouping_test(10, {}, weight_method='VW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分组净值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_df = (1+result.all_group_ret_df).cumprod()\n",
    "nav_df[result.group_list+[result.benchmark_id]].plot(figsize=figsize,fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分组累计超额"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个回测区间的累积超额收益率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_df = nav_df.iloc[-1,:-1] - nav_df.iloc[-1,-1]\n",
    "excess_df[result.group_list].plot(kind='bar', figsize=figsize,fontsize=25, color='dodgerblue',grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多空组合净值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav = nav_df[result.group_list[0]] - nav_df[result.group_list[-1]] + 1\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plotting.plot_drawdown_periods(\n",
    "        nav.pct_change(), top=1, fontsize=15)\n",
    "plt.title('多空组合净值',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指数减空头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav = nav_df[result.benchmark_id] - nav_df[result.group_list[-1]] + 1\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plotting.plot_drawdown_periods(\n",
    "        nav.pct_change(), top=1, fontsize=15)\n",
    "plt.title('指数减空头',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头减指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav = nav_df[result.group_list[0]] - nav_df[result.benchmark_id] + 1\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plotting.plot_drawdown_periods(\n",
    "        nav.pct_change(), top=1, fontsize=15)\n",
    "plt.title('多头减指数',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 年度统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分析都是针对Q1组合的。  \n",
    "注1：年化因子为365天，与wind一致。  \n",
    "注2：超额收益即 Rp - Rb  \n",
    "注3：alpha 和 beta 是根据CAMP公式将Rp-Rf与Rb-Rf进行回归得出的系数。因此alpha相比超额收益来说多了风险调整这一步。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_ra_df = result.get_annual_return_statistic()\n",
    "styles = [\n",
    "            dict(selector=\"caption\", props=[(\"font-size\", \"150%\"),\n",
    "                                            (\"text-align\", \"center\")]),\n",
    "            dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")]),\n",
    "            dict(selector=\"th\", props=[(\"max-width\", \"1200px\"),\n",
    "                                       (\"text-align\", \"center\")])]\n",
    "pct_fields = []\n",
    "float_fields = []\n",
    "for col in annual_ra_df:\n",
    "    if '%' in col:\n",
    "        pct_fields.append(col)\n",
    "    else:\n",
    "        float_fields.append(col)\n",
    "annual_ra_df.style.format(\"{:.2%}\", subset=pct_fields) \\\n",
    "            .format(\"{:.2}\", subset=float_fields) \\\n",
    "            .set_properties(**{'max-width': '1000px', 'font-size': '11pt'}) \\\n",
    "            .set_caption(\"净值评价\") \\\n",
    "            .set_table_styles(styles) \\\n",
    "            .bar(align='mid', color=['#5fba7d', '#d65f5f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 纯多月收益统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分析都是针对Q1组合的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ax_monthly_heatmap = plt.subplot(121)\n",
    "ax_monthly_dist = plt.subplot(122)\n",
    "plotting.plot_monthly_returns_heatmap(result.ret_df['p'], ax=ax_monthly_heatmap)\n",
    "plotting.plot_monthly_returns_dist(result.ret_df['p'], ax=ax_monthly_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超额月收益统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分析都是针对Q1组合的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ax_monthly_heatmap = plt.subplot(121)\n",
    "ax_monthly_dist = plt.subplot(122)\n",
    "plotting.plot_monthly_returns_heatmap(result.ret_df['excess'], ax=ax_monthly_heatmap)\n",
    "plotting.plot_monthly_returns_dist(result.ret_df['excess'], ax=ax_monthly_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持有期IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IC分析不仅针对Q1，下图中每个柱子是直接对universe里面的所有股票的score与该持有期内的累计收益率作秩相关系数。  \n",
    "如果回测中设定了“换仓日期模式”为 “月初换”或“月末换”，那么持有期就是一个自然月。而如果“换仓日期模式”是“自定义”，那持有期就是自定义日期序列里两个相邻日期之间的时间段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "ic_plus = result.ic_series.copy()\n",
    "ic_plus[result.ic_series<0] = np.nan\n",
    "ic_minus = result.ic_series.copy()\n",
    "ic_minus[result.ic_series>=0] = np.nan\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax1 = fig.add_subplot(311)\n",
    "ic_plus.plot(kind='bar', color='r',ax=ax1, fontsize=15)\n",
    "ic_minus.plot(kind='bar', color='g',ax=ax1, fontsize=15)\n",
    "\n",
    "ax2 = fig.add_subplot(312)\n",
    "result.ic_series.plot(kind='kde', ax=ax2)\n",
    "\n",
    "fig.add_subplot(313)\n",
    "a=stats.probplot(result.ic_series, dist=\"norm\", plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC随持有天数变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.daily_ic_df.plot(kind='box', figsize=figsize, fontsize=15)\n",
    "plt.xlabel('持有天数', fontsize=15)\n",
    "plt.ylabel('IC', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC 半衰期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "半衰期通过下式回归得出\n",
    "$$ y = a \\times e^{-bx} $$\n",
    "其中x是持续天数，y是T+x天后的当日收益率。\n",
    "$$半衰期 = \\frac{\\ln2}{a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.half_time_period_series.plot(kind='bar', figsize=figsize, fontsize=15, color='dodgerblue')\n",
    "plt.xlabel('日期', fontsize=15)\n",
    "plt.ylabel('IC半衰期', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个持有期内，对全universe的期初score与当期收益率作回归，但这里还考虑了行业影响，加入了行业哑变量，如下：\n",
    "$$ R = \\alpha + \\beta_1 score + \\sum_{i=1}^{n}{\\beta_{2,i}\\times SWInd_i} + \\epsilon$$\n",
    "其中SWInd_i是行业哑变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.regression_result_df.astype(float).hist(bins=30, figsize=figsize)\n",
    "result.regression_result_df.astype(float).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行业超额收益率（中位数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各行业在每个持有期内超额收益的中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.industry_alpha_df.median(axis=1).sort_values(ascending=False).plot(kind='bar',color='dodgerblue', figsize=figsize)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持仓换手率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.turnover_rate_df[result.group_list].plot(figsize=figsize, fontsize=15)\n",
    "plt.ylabel('换手率', fontsize=15)\n",
    "result.turnover_rate_df[result.group_list].astype(float).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各分组内的可选股票数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.result_count_series_dict[max(result.result_count_series_dict.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=figsize)\n",
    "# 第一期\n",
    "fig.add_subplot(121)\n",
    "fab.processed_universe_df_dict[min(fab.processed_universe_df_dict.keys())]['score'].hist(bins=100)\n",
    "# 最后一期\n",
    "fig.add_subplot(122)\n",
    "fab.processed_universe_df_dict[max(fab.processed_universe_df_dict.keys())]['score'].hist(bins=100)\n",
    "\n",
    "l = []\n",
    "for date,df in fab.processed_universe_df_dict.items():\n",
    "    l.append(df['score'])\n",
    "concat_df = pd.concat(l, axis=1)\n",
    "\n",
    "fig1=plt.figure(figsize=figsize)\n",
    "ax = fig1.add_subplot(111)\n",
    "concat_df.plot(kind='box', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数字型因子分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把factor_name换成流通市值以外的任何数字型因子，从而看到持仓组合和基准组合的因子分布差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_name = 'free_circulating_market_cap'\n",
    "\n",
    "fig=plt.figure(figsize=figsize)\n",
    "# 第一期\n",
    "ax1 = fig.add_subplot(221)\n",
    "df = fab.raw_universe_df_dict[min(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "first_period_df = pd.concat([df[factor_name],\n",
    "                              fab.benchmark_weight_df_dict[min(fab.benchmark_weight_df_dict.keys())][factor_name]],\n",
    "                            axis=1)\n",
    "first_period_df.columns=('持仓'+factor_name, '基准'+factor_name)\n",
    "first_period_df['持仓'+factor_name].plot(kind='kde', ax=ax1, fontsize=15) \n",
    "first_period_df['基准'+factor_name].plot(kind='kde', ax=ax1, fontsize=15) \n",
    "plt.legend()\n",
    "plt.title('第一期{0}分布(普通坐标轴)'.format(factor_name))\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "df = fab.raw_universe_df_dict[min(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "first_period_df = pd.concat([df[factor_name],\n",
    "                              fab.benchmark_weight_df_dict[min(fab.benchmark_weight_df_dict.keys())][factor_name]],\n",
    "                            axis=1)\n",
    "first_period_df.columns=('持仓'+factor_name, '基准'+factor_name)\n",
    "first_period_df['持仓'+factor_name].plot(kind='kde', ax=ax2, logx=True, fontsize=15) \n",
    "first_period_df['基准'+factor_name].plot(kind='kde', ax=ax2, logx=True, fontsize=15) \n",
    "plt.legend()\n",
    "plt.title('第一期{0}分布(对数坐标轴)'.format(factor_name))\n",
    "# 最后一期\n",
    "ax3 = fig.add_subplot(223)\n",
    "df = fab.raw_universe_df_dict[max(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "last_period_df = pd.concat([df[factor_name],\n",
    "                              fab.benchmark_weight_df_dict[max(fab.benchmark_weight_df_dict.keys())][factor_name]],\n",
    "                            axis=1)\n",
    "last_period_df.columns=('持仓'+factor_name, '基准'+factor_name)\n",
    "last_period_df['持仓'+factor_name].plot(kind='kde', ax=ax3, fontsize=15) \n",
    "last_period_df['基准'+factor_name].plot(kind='kde', ax=ax3, fontsize=15) \n",
    "plt.legend()\n",
    "plt.title('最后一期{0}分布(普通坐标轴)'.format(factor_name))\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "df = fab.raw_universe_df_dict[max(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "last_period_df = pd.concat([df[factor_name],\n",
    "                              fab.benchmark_weight_df_dict[max(fab.benchmark_weight_df_dict.keys())][factor_name]],\n",
    "                            axis=1)\n",
    "last_period_df.columns=('持仓'+factor_name, '基准'+factor_name)\n",
    "last_period_df['持仓'+factor_name].plot(kind='kde', ax=ax4, logx=True, fontsize=15) \n",
    "last_period_df['基准'+factor_name].plot(kind='kde', ax=ax4, logx=True, fontsize=15) \n",
    "plt.legend()\n",
    "plt.title('最后一期{0}分布(对数坐标轴)'.format(factor_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行业权重分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_factor_name = 'industry_zx1_name'\n",
    "\n",
    "fig=plt.figure(figsize=(20,25))\n",
    "# 第一期\n",
    "ax1 = fig.add_subplot(211)\n",
    "df = fab.raw_universe_df_dict[min(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "bench_df = fab.benchmark_weight_df_dict[min(fab.benchmark_weight_df_dict.keys())]\n",
    "first_period_df = pd.concat([df.groupby(industry_factor_name)[result.group_list[0]].sum(),\n",
    "                             bench_df.groupby(industry_factor_name)['权重'].sum()],\n",
    "                            axis=1)\n",
    "first_period_df.columns = ('持仓'+industry_factor_name+'权重', '基准'+industry_factor_name+'权重')\n",
    "first_period_df.sort_values('持仓'+industry_factor_name+'权重',inplace=True, ascending=False)\n",
    "first_period_df.plot(kind='bar', ax=ax1, fontsize=15)\n",
    "plt.title('第一期')\n",
    "# 最后一期\n",
    "ax1 = fig.add_subplot(212)\n",
    "df = fab.raw_universe_df_dict[max(fab.raw_universe_df_dict.keys())]\n",
    "df = df[df[result.group_list[0]] > 0]\n",
    "bench_df = fab.benchmark_weight_df_dict[max(fab.benchmark_weight_df_dict.keys())]\n",
    "last_period_df = pd.concat([df.groupby(industry_factor_name)[result.group_list[0]].sum(),\n",
    "                             bench_df.groupby(industry_factor_name)['权重'].sum()],\n",
    "                            axis=1)\n",
    "last_period_df.columns = ('持仓'+industry_factor_name+'权重', '基准'+industry_factor_name+'权重')\n",
    "last_period_df.sort_values('持仓'+industry_factor_name+'权重',inplace=True, ascending=False)\n",
    "last_period_df.plot(kind='bar', ax=ax1, fontsize=15)\n",
    "plt.title('最后一期')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
